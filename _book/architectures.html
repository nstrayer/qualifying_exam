<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="neuralnetworks.html">
<link rel="next" href="future.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Continuous Classification using Deep Neural Networks</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#continuous-classification"><i class="fa fa-check"></i><b>1.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>1.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#activity-prediction"><i class="fa fa-check"></i><b>1.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#ehr-monitoring"><i class="fa fa-check"></i><b>1.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#hospital-automation"><i class="fa fa-check"></i><b>1.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#history-of-methods"><i class="fa fa-check"></i><b>1.3</b> History of methods</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#windowed-regression"><i class="fa fa-check"></i><b>1.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#transformation-methods"><i class="fa fa-check"></i><b>1.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#hidden-markov-models"><i class="fa fa-check"></i><b>1.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>1.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>2</b> Neural Networks</a><ul>
<li class="chapter" data-level="2.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>2.1</b> History</a><ul>
<li class="chapter" data-level="2.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>2.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="2.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>2.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="2.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>2.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="2.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>2.4</b> Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>2.5</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>2.5.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="2.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propagation"><i class="fa fa-check"></i><b>2.5.2</b> Back Propagation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>2.6</b> Training</a><ul>
<li class="chapter" data-level="2.6.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>2.6.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.6.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>2.6.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>2.7</b> Activation Functions</a></li>
<li class="chapter" data-level="2.8" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>2.8</b> Loss functions</a><ul>
<li class="chapter" data-level="2.8.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#likelihoods"><i class="fa fa-check"></i><b>2.8.1</b> Likelihoods</a></li>
<li class="chapter" data-level="2.8.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>2.8.2</b> Regression loss functions</a></li>
<li class="chapter" data-level="2.8.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>2.8.3</b> Classification loss functions</a></li>
<li class="chapter" data-level="2.8.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>2.8.4</b> Other loss functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>3</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="3.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>3.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="3.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>3.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="3.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>3.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="3.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>3.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="3.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>3.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="3.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>3.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="3.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>3.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="3.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>3.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="3.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>3.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>3.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="3.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>3.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>4</b> Opportunities for advancing field</a><ul>
<li class="chapter" data-level="4.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>4.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="4.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>4.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="4.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware"><i class="fa fa-check"></i><b>4.1.2</b> Runtime on mobile hardware</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>4.2</b> Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>4.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="4.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>4.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="4.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>4.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>4.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>4.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="4.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>4.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="architectures" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Architectures For Sequence Learning</h1>
<p>In the previous chapter we described the general neural network architecture. This is usually called a dense feed-forward network. ‘Dense’ refers to the fact that all neurons of a given layer are connected to all neurons of the successive layer. ‘Feed-forward’ refers to the fact that data flows into the network and straight to the output, traveling only forward through the layers. In this section we will expand upon this general model with different architectures: the recurrent neural network (RNN) (<span class="citation">Williams and Zipser (<a href="#ref-rnn_intro">1989</a>)</span>) and the convolutional neural net (CNN) (<span class="citation">LeCun and others (<a href="#ref-cnn_intro">1989</a>)</span>).</p>
<p>While they are often represented as very different models, these architectures are in fact sub-models of the dense feed-forward networks from the last chapter, just with restrictions placed on weights in the form of deleting connections (setting weight to 0) or sharing weights between multiple connections.</p>
<p>These restrictions applied to standard neural networks allow the models to more efficiently model tasks related to sequential data by reducing the number of parameters that need to be fit or, in some cases, helping with the propagation of the gradients for efficient training.</p>
<div id="terminology-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Terminology</h2>
<p>Throughout this chapter we will refer to an input <span class="math inline">\(\mathbf{x}\)</span> which is a vector of observations at <span class="math inline">\(t\)</span> time points. In addition, we have an outcome <span class="math inline">\(\mathbf{y}\)</span>, also of length <span class="math inline">\(t\)</span> that represents some state or property of the system generating <span class="math inline">\(x\)</span> at each time point. This could be the type of road a car is driving on, the sentiment of a speaker, or the next <span class="math inline">\(x_i\)</span> value (e.g. next word in a sentence).</p>
</div>
<div id="recurrent-neural-networks" class="section level2">
<h2><span class="header-section-number">3.2</span> Recurrent Neural Networks</h2>
<p>One way to efficiently deal with the fact that sequential data is often highly correlated between observations is to fit a model to each time-point and then pass it information on what was happening prior. The model can then combine the previous information with the newly observed input to come up with a prediction.</p>
<p>This can be accomplished in a neural network by adding recurrent links between layers. Typically, this is done by passing the hidden layer (or layers) of the network the values of itself at the previous time point. I.e. <span class="math inline">\(\mathbf{h}_{t} = g(\mathbf{x}_t, \mathbf{h}_{t - 1})\)</span>. The idea behind this is that the hidden layer learns to encode some ‘latent state’<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> of the system that is informative for its output, and so letting the model know what that latent state was previously will help it update the latent state and provide an accurate output.</p>
<p>Why not just pass the output at time <span class="math inline">\((t-1)\)</span> to the hidden state at <span class="math inline">\(t\)</span> instead? While this is possible, and indeed works much better than not communicating information between time points at all, it suffers from the squashing of the latent state information to out outcome of interest. This results in a loss of information about what is happening in the system since the hidden or latent state to the outcome is not necessarily a one-to-one function. In addition, there is convenience in the fact that the hidden state is already of the same dimension, allowing for a simple element-wise addition of the components from the previous hidden state and the new input information.</p>
<div class="figure" style="text-align: center"><span id="fig:cyclegraph"></span>
<img src="figures/rnn_compact.png" alt="A recurrent neural network with a single hidden layer. The hidden layer's values at time $t$ are passed to the hidden layer at time $(t + 1)$." width="40%" />
<p class="caption">
Figure 3.1: A recurrent neural network with a single hidden layer. The hidden layer’s values at time <span class="math inline">\(t\)</span> are passed to the hidden layer at time <span class="math inline">\((t + 1)\)</span>.
</p>
</div>
<div id="applications-to-sequential-data" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Applications to sequential data</h3>
<p>RNNs are fundamentally models for performing analysis on sequential data (although they have been applied to static inputs and used to generate sequential outputs (<span class="citation">Mao et al. (<a href="#ref-rnn_captions">2014</a>)</span>)). Some of the major success stories in recurrent neural networks come in the realm of machine translation. For instance, Google’s entire translation service is now powered by RNNs (<span class="citation">Wu et al. (<a href="#ref-google_translate">2016</a>)</span>).</p>
<p>Other domains in which RNNs have been successfully applied is in time-series regression (<span class="citation">Cai et al. (<a href="#ref-rnn_regression">2007</a>)</span>), speech recognition (<span class="citation">Graves, Mohamed, and Hinton (<a href="#ref-rnn_speach">2013</a>)</span>), and handwriting recognition (<span class="citation">Graves et al. (<a href="#ref-rnn_handwriting">2008</a>)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="figures/handwriting_rnn.png" alt="Example of an RNN's output for recognizing characters in handwritten characters. The model scans along one slice at a time of the data and the output is character likelihood. Still from [youtube video](https://www.youtube.com/watch?v=mLxsbWAYIpw) by Nikhil Buduma." width="90%" />
<p class="caption">
Figure 3.2: Example of an RNN’s output for recognizing characters in handwritten characters. The model scans along one slice at a time of the data and the output is character likelihood. Still from <a href="https://www.youtube.com/watch?v=mLxsbWAYIpw">youtube video</a> by Nikhil Buduma.
</p>
</div>
</div>
<div id="successes-in-natural-language-processing" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Successes in natural language processing</h3>
<p>One of the areas that has seen great results from the application of RNNs is natural language processing. Natural language processing (or NLP) refers broadly to the modeling of textual data in order to infer things like sentiment, predict next words, or even generate entirely new sentences.</p>
<p>Usage of neural networks for these tasks has greatly improved upon previous techniques that constrained were constrained by linear assumptions (e.g. word2vec (<span class="citation">Mikolov et al. (<a href="#ref-word2vec">2013</a>)</span>)) or limited ability to look backwards in time.</p>
<!-- #### Abstracts Example -->
</div>
<div id="cyclical-computation-graph" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Cyclical Computation Graph</h3>
<p>A natural question that arises from the cyclical computational graph shown in figure <a href="architectures.html#fig:cyclegraph">3.1</a> is how the gradient can be calculated via back propagation. In fact, the cycle as represented is just a visual simplification of the true computational graph. The ‘unrolled’ graph can be thought of as a long chain of neural networks that share connections between sequential hidden layers.</p>
<div class="figure" style="text-align: center"><span id="fig:unrolledgraph"></span>
<img src="figures/rnn_unrolled.png" alt="Unrolled view of the RNN shown in previous figure. Each timestep has two outputs, the timesteps predictions and its hidden state. The next time step subsequently has two inputs: the data at the timepoint and the previous timepoint's hidden state." width="65%" />
<p class="caption">
Figure 3.3: Unrolled view of the RNN shown in previous figure. Each timestep has two outputs, the timesteps predictions and its hidden state. The next time step subsequently has two inputs: the data at the timepoint and the previous timepoint’s hidden state.
</p>
</div>
<p>So in fact, we still satisfy the requirement of a directed acyclic computation graph, just it is convenient to represent the unique layers in the graph in a single cyclical diagram.</p>
</div>
<div id="weight-sharing" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Weight sharing</h3>
<p>The unrolled graph representation shows that a recurrent neural network is actually a very large neural network, so untuitively it should be hard to train. The secret to RNNs having maintainable parameter quantities is weight sharing. In the unrolled graph every layer has the same weights. This means that the hidden layer always parses the input data with the same affine function and combines it with the previous hidden state in the same way; likewise, with the same hidden state activation, the model output will always be the same<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>.</p>
<p>In order to calculate gradients, first an initial hidden state is defined (usually all zeros) and then forward propagation is carried out through all <span class="math inline">\(t\)</span> time-points of the input. Then, back propagation starts from the last output and proceeds all the way back through the computation graph. The resulting gradient descent updates are a function of the average of all of the gradients calculated. This procedure, although technically no different from plain back propagation, is known as the <em>back-propigation through time</em> (BPTT) algorithm. For a thorough overview of the form of the gradient equations for BPTT see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter 10.2.2.</p>
</div>
<div id="problems-with-exploding-and-vanishing-gradients" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Problems with exploding and vanishing gradients</h3>
<p>One way to think of an RNN is a series of function compositions over time. Much like a ‘deep’ neural network is a series of function compositions through it’s layers, the unrolled computation graph of the RNN is ‘deep’ in time. While a traditional feed forward network typically have somewhere between two and ten layers of this composition, RNNs can have hundreds of steps of this composition as sequences are commonly very long (e.g. time series data collected every second for a day: <span class="math inline">\(t= 86,400\)</span>). When this many compositions are performed negative side effects tend to accumulate. The largest being the problem of the exploding and vanishing gradients (<span class="citation">Hochreiter et al. (<a href="#ref-vanishing_gradient">2001</a>)</span>, <span class="citation">Bengio and Frasconi (<a href="#ref-bengio_gradient">1994</a>)</span>).</p>
<p>To illustrate this problem we can think of an extremely simple RNN. One that has no input, a single hidden layer <span class="math inline">\(\mathbf{h}^{(i)}, i \in \{1, ..., t\}\)</span>, and no non-linear activation functions. We will denote the weights for mapping <span class="math inline">\(\mathbf{h}^{(i)} \to \mathbf{h}^{(i + 1)}\)</span> with <span class="math inline">\(\mathbf{W}\)</span>. We also assume that <span class="math inline">\(\mathbf{W}\)</span> can be eigendecomposed to <span class="math inline">\(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}&#39;\)</span> where <span class="math inline">\(\mathbf{Q}\)</span> is orthogonal and <span class="math inline">\(\mathbf{\Lambda}\)</span> is the eigen matrix. This is obviously a functionally useless model, but it serves to illustrate our problem.</p>
<p>We can think of each step of our RNN through time in terms of matrix multiplication.</p>
<span class="math display" id="eq:simplernn1">\[\begin{equation} 
  \mathbf{h}^{(t)} = \mathbf{W}&#39; \mathbf{h}^{(t - 1)}
  \tag{3.1}
\end{equation}\]</span>
<p>This equation can be simplified to a function of only the weight matrix and the first hidden state using the power method.</p>
<span class="math display" id="eq:simplernn2">\[\begin{equation} 
  \mathbf{h}^{(t)} = \left[\mathbf{W}^t\right]&#39; \mathbf{h}^{(0)}
  \tag{3.2}
\end{equation}\]</span>
<p>Next, we substitute the eigendecomposition of <span class="math inline">\(\mathbf{W}\)</span>.</p>
<span class="math display" id="eq:simplernn3">\[\begin{align} 
  \mathbf{h}^{(t)} &amp;= \left[(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q})^t\right]&#39; \mathbf{h}^{(0)} \\
                    &amp;= \mathbf{Q}&#39;\mathbf{\Lambda}^t \mathbf{Q} \mathbf{h}^{(0)} \\
  \tag{3.3}
\end{align}\]</span>
<p>The form of the function composition in <a href="architectures.html#eq:simplernn3">(3.3)</a> allows us to see that while the eigen matrix is continuously being raised to the <span class="math inline">\(t\)</span> power, it will cause its eigenvalues with magnitude greater than one to diverge to infinity and those with magnitude less than one to converge to zero.</p>
<p>This observation leads to the problem that, over any long term sequence, our model will have a very hard time keeping track of dependencies. When applied to back propagation this is essentially means that the gradients associated with parameters linked to longer term dependencies will either vanish (go to zero) or explode (go to infinity).</p>
<p>Due to the problems caused by this repeated function composition plain RNNs have generally proved unable to learn dependencies longer than a few time-steps<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>, and when they do, they are outweighed by those closer in time, simply due to mathematical inconveniences and not necessarily information importance (<span class="citation">Bengio and Frasconi (<a href="#ref-bengio_gradient">1994</a>)</span>, <span class="citation">Graves (<a href="#ref-graves_rnn">2012</a>)</span>).</p>
</div>
<div id="modern-extensions" class="section level3">
<h3><span class="header-section-number">3.2.6</span> Modern Extensions</h3>
<p>There have been many proposed solutions to solving the problems associated with plain RNNs inability to learn long term dependencies. Some of them focus on forcing gradient’s into good behavior by constraining them to be close to one (<span class="citation">Sussillo (<a href="#ref-bounded_gradient_rnn">2014</a>)</span>) and others on adding ‘skip connections’ between times further apart than a single step (<span class="citation">Lin et al. (<a href="#ref-skip_connections">1998</a>)</span>).</p>
<p>While these methods do successfully allow RNNs to learn longer time dependencies, they are rather restrictive (forcing weights to have gradients near one can slow down training and inflate importance of some features and skip connections eliminate the benefit of the network learning the dependency lengths on its own). Next, we will introduce two methods to deal with the problems of learning long-term dependencies that have gained wide-spread adoption and generally dominate the techniques used.</p>
<div id="long-short-term-memory-networks" class="section level4">
<h4><span class="header-section-number">3.2.6.1</span> Long short term memory networks</h4>
<p>A way of controlling how an RNN keeps track of different dependencies over time is to make that control flow part of the model itself and allow it to be learned. This is the concept that long short term memory (LSTM) networks (<span class="citation">Hochreiter and Schmidhuber (<a href="#ref-lstm_intro">1997</a>)</span>) use. In the broadest sense, LSTM networks augment the traditional RNN architecture by adding a series of ‘gates’ that control which parts of the hidden state are remembered and used from time-step to time-step.</p>
<div class="figure" style="text-align: center"><span id="fig:lstmdiagram"></span>
<img src="figures/lstm_cell.png" alt="The internal structure of an LSTM hidden unit. The gates are all fed by the input at time t, the whole hidden unit at time (t - 1), and the hidden units internal state from time (t - 1). The gates themselves are linear combinations of the input that are then transformed by sigmoid activation functions." width="60%" />
<p class="caption">
Figure 3.4: The internal structure of an LSTM hidden unit. The gates are all fed by the input at time t, the whole hidden unit at time (t - 1), and the hidden units internal state from time (t - 1). The gates themselves are linear combinations of the input that are then transformed by sigmoid activation functions.
</p>
</div>
<p>It is important to note that an LSTM network is really just an RNN that has had the neurons a hidden layer replaced by a more complicated cell that controls its inputs and outputs along with having a secondary internal recurrence to an internal state. See figure <a href="architectures.html#fig:lstmdiagram">3.4</a> for the details of this internal cell.</p>
<p>These gates are themselves neurons that take linear combinations of the input at time <span class="math inline">\(t\)</span>, the hidden state at <span class="math inline">\((t - 1)\)</span> and the LSTM cell’s internal state at <span class="math inline">\((t - 1)\)</span> and produce an output that is squashed between zero (closed) and one (open) by a sigmoid activation function. The input gate will control what information makes it through to the unit internal state, the forget gate decides which information from the internal state gets recycled for the next time point, and the output gate decides what of the internal state is important to the next layer.</p>
<p>While LSTMs are conceptually more complicated, their improvements in performance over standard RNNs is drastic (<span class="citation">Hochreiter and Schmidhuber (<a href="#ref-lstm_intro">1997</a>)</span>). The network can now learn what features are important to its given outcome at a given time, while at the same time filtering out information that it finds unnecessary in context. To give intuition to this we can imagine a baseball game. Say the batter hits a foul ball, that foul has different implications on the outcome of the at bat depending on how many strikes the player has when it occurred. A traditional RNN would always choose to remember a foul the same, whereas an LSTM network could learn this contextual frame for importance.</p>
<p>The major issue with LSTMs is how many parameters there are to learn. The number of parameter’s associated with a neuron in a hidden layer jumps roughly three times over a standard RNN due to the weights necessary for the gates. Due to this, LSTMs require a very large amount of data to effectively train and not over-fit. In practice, these complications, combined with the recovery of some degrees of freedom through regularization, seem to be worth it and LSTMs are by far the most common RNN-based architecture used today. However, some other approaches using similar methodologies have proven successful while reducing the number of parameters needed.</p>
</div>
<div id="gated-recurrent-units" class="section level4">
<h4><span class="header-section-number">3.2.6.2</span> Gated recurrent units</h4>
<p>Introduced relatively recently (2014), Gated recurrent units (GRU)(<span class="citation">Cho et al. (<a href="#ref-gru_intro">2014</a>)</span>), like LSTM, use gates to help control information flow through time, but it omits the extra recurrence step of the internal step from the LSTM and also sets the update and forget gates to be complements of each other, thus accomplishing the task with a single gate. This is paired with a reset gate that controls how much of the hidden state from the previous time-point makes it into the hidden state’s input vector.</p>
<div class="figure" style="text-align: center"><span id="fig:grudiagram"></span>
<img src="figures/gru_cell.png" alt="The internal structure of an GRU hidden unit. Unlike the LSTM there are only two gates, with the update and forget get tasks being taken care of by a single gate that controls what proportion to remember and what to forget. In addition, a reset gate controls how much of the hidden state from the previous step to bring into the current." width="60%" />
<p class="caption">
Figure 3.5: The internal structure of an GRU hidden unit. Unlike the LSTM there are only two gates, with the update and forget get tasks being taken care of by a single gate that controls what proportion to remember and what to forget. In addition, a reset gate controls how much of the hidden state from the previous step to bring into the current.
</p>
</div>
<p>While the GRU has fewer parameters to train, and does appear to perform better in lower data scenarios (<span class="citation">Graves (<a href="#ref-graves_rnn">2012</a>)</span>), in many cases the increased expressiveness of the LSTM allows for better performance. A recent large-scale survey of RNN architectures (<span class="citation">Melis, Dyer, and Blunsom (<a href="#ref-rnn_survey">2017</a>)</span>) found that on all benchmarks LSTM networks outperformed GRUs when using a systematic hyper-parameter search.</p>
</div>
</div>
<div id="computational-hurdles" class="section level3">
<h3><span class="header-section-number">3.2.7</span> Computational Hurdles</h3>
<p>The complications caused by RNNs propagating values/errors over many time steps is also the cause of the biggest computational hurdle associated with RNNs. When forward propagating or back propagating, we need to do it all in one sequential set of calculations.</p>
<p>It is very common for neural network training to be multi-threaded using graphics processing units (GPUs). The reason this is effective is most of the calculations involved are simple and only depend on a few sequential layers, and thus it is very easy to run many training samples through the network in parallel to calculate their gradients and then aggregate those to a gradient descent step. However, with RNNs we must processes each sequence all the way though its (potentially very numerous) time steps. As a result, RNNs train much slower than other neural network architectures.</p>
<p>There are a few general approaches to solving this. The simplest is to just stop back propagation at a certain number of steps, regardless of if the sequence has been fully processed yet. This is known as truncated back propagation through time (<span class="citation">Williams and Peng (<a href="#ref-trunc_bptt">1990</a>)</span>) and does often substantially speed up training, but at the obvious cost of limiting the length of time the network is able to look back in.</p>
<p>Another technique is to modify the architecture of the RNN such that the hidden state receives input not from itself at the previous time point but the output at the previous time-point. This allows the model to be trained one time point at a time because the output from the next time step can be substituted with the observed value at that time step. While this approach massively speeds up training, as mentioned previously, it throws away some potentially valuable latent-space information that the model has learned. This approach has gained very little traction due to these limitations.</p>
<div class="figure" style="text-align: center"><span id="fig:outputrnn"></span>
<img src="figures/output_rnn.png" alt="An RNN where the recurrent connection is made between the previous timepoint's output and the hidden unit. This model is less expressive than the traditional hidden - hidden architecture but is much easier to train." width="45%" />
<p class="caption">
Figure 3.6: An RNN where the recurrent connection is made between the previous timepoint’s output and the hidden unit. This model is less expressive than the traditional hidden - hidden architecture but is much easier to train.
</p>
</div>
<p>As both the number of observations as well as the number of time points grows the RNN architecture becomes increasingly strained and researchers have begun looking at other potential ways of modeling sequential data that allow for dynamic learning of temporal dependencies while training in reasonable amounts of time. The most promising approach thus far has been using convolutional neural networks.</p>
</div>
</div>
<div id="convolutional-neural-networks" class="section level2">
<h2><span class="header-section-number">3.3</span> Convolutional Neural Networks</h2>
<p>While CNNs have garnered a great amount of attention in recent years for their successes in computer-vision they were originally introduced as a method for modeling time-series data (<span class="citation">Waibel et al. (<a href="#ref-hinton_cnn">1989</a>)</span>)[^Although at the time they were called time-delay neural networks.]. A convolutional neural network is one where a convolution (, or window that detects features such as edges,) is run over the spatially connected dimensions of the data in order create a map where a given feature is located.</p>
<div id="application-to-spatially-correlated-data" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Application to spatially correlated data</h3>
<p>In the context of image recognition this mapping means scanning a two-dimensional block of pixels over the range of the picture to detect important features in the data. In time-series data this window is one dimensional (e.g. a five minute window) and it scans exclusively along the sequence detecting some feature.</p>
</div>
<div id="feature-learning" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Feature Learning</h3>
<p>The power of neural networks comes in with the fact that these feature recognizers are learned. This is in contrast with traditional approaches where the features to be extracted had to be manually curated. Usually some number of convolution’s are initialized for a given model and are allowed to learn the patterns that help them best minimize the loss function. Interestingly, in the case of computer-vision, sometimes fascinating patterns are learned that mimic what one might expect, but are much more complicated than those manually created by humans (<span class="citation">Olah, Mordvintsev, and Schubert (<a href="#ref-cnn_vis">2017</a>)</span>).</p>
<p>Neuroscience research has shown that CNNs mimic how the first stages of the animal visual perception system works (<span class="citation">Kheradpisheh et al. (<a href="#ref-cnn_animals">2016</a>)</span>) and has in many cases surpassed human level performance on basic tasks.</p>
<div class="figure" style="text-align: center"><span id="fig:cnnexplain"></span>
<img src="figures/convolutional_explainer.jpeg" alt="A diagram of how a convolutions are applied to sequential data." width="100%" />
<p class="caption">
Figure 3.7: A diagram of how a convolutions are applied to sequential data.
</p>
</div>
</div>
<div id="weight-sharing-1" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Weight sharing</h3>
<p>Like RNNs, a CNN can be thought of in the context of a traditional feed forward neural network. A convolution operation is simply the sharing of weights between grouped blocks of the hidden layers. For instance, if we had two convolutions of length three that were reading in sequential data in the hidden layer neurons (1,2,3) would correspond to convolution one on the first time-point of the input, neurons (4,5,6) would correspond to convolution two on the first time point, neurons (7,8,9) to convolution one on time-point two, and so on. See <a href="architectures.html#fig:cnnexplain">3.7</a> for a visual treatment of this<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>.</p>
</div>
<div id="translation-invariance" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Translation invariance</h3>
<p>After the first convolutional layer has produced a map of the location of certain features on the input, subsequent layers can perform what is known as a ‘pooling’ operation to both reduce the dimensionality of the latent space and also to make the feature detection translation invariant.</p>
<p>A pooling layer acts similarly to a convolutional layer in that it scans sequentially over its input, however in the case of a pooling layer the input is the output of a convolutional layer. In addition, instead of applying some learned linear transformation to its inputs, it performs a predesignated pooling operation such as finding the maximum, or average of the input values for its window and then returning that singular value. By taking steps larger than one between each application of the filter (also known as the stride), this serves to create an indicator for if a feature was ‘seen’ in the region, but it does not care about where in that region it was seen.</p>
<p>This can be beneficial in many scenarios. For instance, if you were trying to identify the waveform of the word “dog”, one feature detector may be to find the pattern corresponding to the “d” phoneme, and one to the “au” phoneme. Depending on the speed of the speaker, the “d” phoneme might appear two tenths of a second before the “au” one or one tenth. By pooling we help the model getting confused in this case, because it just looks for the “au” phoneme to occur in a flexible window after the “d” one.</p>
<p>Translation invariance does also have its downsides too. Geoffrey Hinton (coincidentally the inventor of CNNs) has been very open about how he thinks <a href="www.youtube.com%2Fwatch%3Fv%3DrTawFwUvnLE&amp;usg=AOvVaw3CKtrHEhfBZkXdd7xgJ1PM">they are bad models</a>. His main complaint rests with the fact that pooling layers throw away large chunks of information in ways that animal vision systems do not. Recently he published a paper with co-authors Sara Sabour and Nicholas Frosst that introduces a new architecture called the “capsule network” (<span class="citation">Sabour, Frosst, and Hinton (<a href="#ref-capsnet">2017</a>)</span>) which acts similarly to a convolutional network with pooling, but it preserves information about the translation of the feature.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rnn_intro">
<p>Williams, Ronald J, and David Zipser. 1989. “A Learning Algorithm for Continually Running Fully Recurrent Neural Networks.” <em>Neural Computation</em> 1 (2). MIT Press: 270–80.</p>
</div>
<div id="ref-cnn_intro">
<p>LeCun, Yann, and others. 1989. “Generalization and Network Design Strategies.” <em>Connectionism in Perspective</em>. Zurich, Switzerland: Elsevier, 143–55.</p>
</div>
<div id="ref-rnn_captions">
<p>Mao, Junhua, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. 2014. “Deep Captioning with Multimodal Recurrent Neural Networks (M-Rnn).” <em>arXiv Preprint arXiv:1412.6632</em>.</p>
</div>
<div id="ref-google_translate">
<p>Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” <em>arXiv Preprint arXiv:1609.08144</em>.</p>
</div>
<div id="ref-rnn_regression">
<p>Cai, Xindi, Nian Zhang, Ganesh K Venayagamoorthy, and Donald C Wunsch. 2007. “Time Series Prediction with Recurrent Neural Networks Trained by a Hybrid Pso–EA Algorithm.” <em>Neurocomputing</em> 70 (13). Elsevier: 2342–53.</p>
</div>
<div id="ref-rnn_speach">
<p>Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. “Speech Recognition with Deep Recurrent Neural Networks.” In <em>Acoustics, Speech and Signal Processing (Icassp), 2013 Ieee International Conference on</em>, 6645–9. IEEE.</p>
</div>
<div id="ref-rnn_handwriting">
<p>Graves, Alex, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, and Santiago Fernández. 2008. “Unconstrained on-Line Handwriting Recognition with Recurrent Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 577–84.</p>
</div>
<div id="ref-word2vec">
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In <em>Advances in Neural Information Processing Systems</em>, 3111–9.</p>
</div>
<div id="ref-goodfellow_DL">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-vanishing_gradient">
<p>Hochreiter, Sepp, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, and others. 2001. “Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies.” A field guide to dynamical recurrent neural networks. IEEE Press.</p>
</div>
<div id="ref-bengio_gradient">
<p>Bengio, Yoshua, and Paolo Frasconi. 1994. “Credit Assignment Through Time: Alternatives to Backpropagation.” In <em>Advances in Neural Information Processing Systems</em>, 75–82.</p>
</div>
<div id="ref-graves_rnn">
<p>Graves, Alex. 2012. <em>Supervised Sequence Labelling with Recurrent Neural Networks</em>. Vol. 385. Springer.</p>
</div>
<div id="ref-bounded_gradient_rnn">
<p>Sussillo, David. 2014. “RANDOM Walks: TRAINING Very Deep Nonlin-Ear Feed-Forward Networks with Smart Ini.” <em>arXiv Preprint arXiv</em> 1412.</p>
</div>
<div id="ref-skip_connections">
<p>Lin, Tsungnan, Bill G Horne, Peter Tino, and C Lee Giles. 1998. “Learning Long-Term Dependencies Is Not as Difficult with Narx Recurrent Neural Networks.”</p>
</div>
<div id="ref-lstm_intro">
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). MIT Press: 1735–80.</p>
</div>
<div id="ref-gru_intro">
<p>Cho, Kyunghyun, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.” <em>arXiv Preprint arXiv:1409.1259</em>.</p>
</div>
<div id="ref-rnn_survey">
<p>Melis, Gábor, Chris Dyer, and Phil Blunsom. 2017. “On the State of the Art of Evaluation in Neural Language Models.” <em>arXiv.org</em>, July. <a href="http://arxiv.org/abs/1707.05589v2" class="uri">http://arxiv.org/abs/1707.05589v2</a>.</p>
</div>
<div id="ref-trunc_bptt">
<p>Williams, Ronald J, and Jing Peng. 1990. “An Efficient Gradient-Based Algorithm for on-Line Training of Recurrent Network Trajectories.” <em>Neural Computation</em> 2 (4). MIT Press: 490–501.</p>
</div>
<div id="ref-hinton_cnn">
<p>Waibel, Alex, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang. 1989. “Phoneme Recognition Using Time-Delay Neural Networks.” <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em> 37 (3). IEEE: 328–39.</p>
</div>
<div id="ref-cnn_vis">
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div id="ref-cnn_animals">
<p>Kheradpisheh, Saeed Reza, Masoud Ghodrati, Mohammad Ganjtabesh, and Timothée Masquelier. 2016. “Deep Networks Can Resemble Human Feed-Forward Vision in Invariant Object Recognition.” <em>Scientific Reports</em> 6. Nature Publishing Group: 32672.</p>
</div>
<div id="ref-capsnet">
<p>Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton. 2017. “Dynamic Routing Between Capsules.” In <em>Advances in Neural Information Processing Systems</em>, 3857–67.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>This is in contrast with hidden markov models, which force the state use to infer the next step to be of the possible classes.<a href="architectures.html#fnref6">↩</a></p></li>
<li id="fn7"><p>We rely on the model to learn a hidden state that stores information from the past to avoid the problems with Markov processes<a href="architectures.html#fnref7">↩</a></p></li>
<li id="fn8"><p><span class="citation">Bengio and Frasconi (<a href="#ref-bengio_gradient">1994</a>)</span> showed that after only 10-20 steps the probability of successfully learning a dependency was effectively zero.<a href="architectures.html#fnref8">↩</a></p></li>
<li id="fn9"><p>The actual location placement of the convolutional outputs for the next layer does not actually matter unless the next layer is itself a convolutional layer.<a href="architectures.html#fnref9">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neuralnetworks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-sequence_architectures.Rmd",
"text": "Edit"
},
"download": ["strayer_qualifying_exam.pdf", "strayer_qualifying_exam.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
