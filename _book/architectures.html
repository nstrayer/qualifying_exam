<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="neuralnetworks.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Report Layout</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#continuous-classification"><i class="fa fa-check"></i><b>2.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>2.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#activity-prediction"><i class="fa fa-check"></i><b>2.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#ehr-monitoring"><i class="fa fa-check"></i><b>2.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#hospital-automation"><i class="fa fa-check"></i><b>2.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#history-of-methods"><i class="fa fa-check"></i><b>2.3</b> History of methods</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#windowed-regression"><i class="fa fa-check"></i><b>2.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#transformation-methods"><i class="fa fa-check"></i><b>2.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="2.3.3" data-path="intro.html"><a href="intro.html#hidden-markov-models"><i class="fa fa-check"></i><b>2.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="2.3.4" data-path="intro.html"><a href="intro.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>2.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>3</b> Neural Networks</a><ul>
<li class="chapter" data-level="3.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>3.1</b> History</a><ul>
<li class="chapter" data-level="3.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>3.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="3.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>3.2</b> The Computation Graph</a></li>
<li class="chapter" data-level="3.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>3.3</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="3.3.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>3.3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.3.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>3.3.2</b> Forward Propagation</a></li>
<li class="chapter" data-level="3.3.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propigation"><i class="fa fa-check"></i><b>3.3.3</b> Back Propigation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>3.4</b> Training</a><ul>
<li class="chapter" data-level="3.4.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>3.4.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="3.4.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>3.4.2</b> Gradient Descent Modifications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>4</b> Further Architectures</a><ul>
<li class="chapter" data-level="4.0.1" data-path="architectures.html"><a href="architectures.html#mathematical-operations-1"><i class="fa fa-check"></i><b>4.0.1</b> Mathematical Operations</a></li>
<li class="chapter" data-level="4.0.2" data-path="architectures.html"><a href="architectures.html#mathematical-operations-2"><i class="fa fa-check"></i><b>4.0.2</b> Mathematical Operations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="architectures" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Further Architectures</h1>
<p>heyo</p>
<!-- *  Optimization -->
<!--        *  Back-Propigation -->
<!--        *  Stochastic Gradient Descent -->
<!--    *  Universal Approximation Theorem -->
<!-- *Convolutional Neural Networks* -->
<!--    *  Application to spatially correlated data -->
<!--    *  Successes in computer vision -->
<!--    *  Convolution as a kernel method -->
<!--    *  Weight sharing -->
<!--    *  Transformation invariance -->
<!-- *Recurrent Neural Networks* -->
<!--    *  Applications with sequential data -->
<!--    *  Successes in natural language processing -->
<!--        *  Abstracts Example -->
<!--    *  Cyclical Computation Graph -->
<!--    *  Weight sharing -->
<!--    *  Exploding and vanishing gradients -->
<!--    *  Modern Extensions -->
<!--        *  LSTM -->
<!--        *  GRU -->
<!--    *  Computational costs -->

<div id="mathematical-operations-1" class="section level3">
<h3><span class="header-section-number">4.0.1</span> Mathematical Operations</h3>
<p>The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input.</p>
<div id="forward-propagation-1" class="section level4">
<h4><span class="header-section-number">4.0.1.1</span> Forward Propagation</h4>
<p>Let a neural network with <span class="math inline">\(l\)</span> layers and <span class="math inline">\(k\)</span> dimensional input <span class="math inline">\(X\)</span> and <span class="math inline">\(m\)</span> dimensional output <span class="math inline">\(\hat{y}\)</span> attempting to predict the true target <span class="math inline">\(y\)</span>. Each layer is composed of <span class="math inline">\(s_i, i \in \{1, 2, ...,l\}\)</span> neurons and has respective non-linear activation function <span class="math inline">\(f_i\)</span>. Layer activation vectors <span class="math inline">\(\underline{a_i}\)</span> and post-activation function outputs <span class="math inline">\(\underline{o_i}\)</span>. The weights into neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(i\)</span> is given by <span class="math inline">\(w_{(i,j)}\)</span> which is a vector of size <span class="math inline">\(s_{i -1}\)</span>. We can thus view the weights into the <span class="math inline">\(i^{\text{th}}\)</span> layer as a matrix <span class="math inline">\(W_i\)</span> of of size <span class="math inline">\(s_{i -1} \times s_i\)</span>. Finally the network has a differntiable with respect to <span class="math inline">\(\hat{y}\)</span> loss function: <span class="math inline">\(L(\hat{y}, y)\)</span>.</p>
<p>Forward propigation then proceeds as follows.</p>
<ol style="list-style-type: decimal">
<li>Input <span class="math inline">\(X\)</span> <span class="math inline">\((1 \times k\)</span>) is multiplied by <span class="math inline">\(W_1\)</span> (size <span class="math inline">\((k\times s_i)\)</span> ) to acheive the <em>activation values</em> of the first hidden layer.
<ul>
<li><span class="math inline">\(X \cdot W_1 = \underline{a_1}\)</span></li>
</ul></li>
<li>The <span class="math inline">\((1 \times s_1)\)</span> activation vector of the first layer is then run element-wise though the first layer’s non-linear activation function to acheive the output of layer 1.
<ul>
<li><span class="math inline">\(\underline{o_1} = f_1(\underline{a}_1)\)</span></li>
</ul></li>
<li>This series of opperations is then repetaed through all the layers, (using the subsequent layers output vector as the input to the next layer,) until the final layer is reached.
<ul>
<li><span class="math inline">\(\underline{o_i} = f_i(\underline{o_{(i - 1)}} \cdot W_i)\)</span></li>
</ul></li>
<li>Finally, the loss is calculated from the output of our final layer.
<ul>
<li><span class="math inline">\(L_n = L(\underline{o_l}, y) = L(\hat{y}, y)\)</span></li>
</ul></li>
</ol>
<p>While not strictly neccesary for forward propigation, the intermediate layer activation and output vectors are kept stored so they can be used in the later calculation of the gradient via back propigation.</p>
</div>
<div id="loss-function" class="section level4">
<h4><span class="header-section-number">4.0.1.2</span> Loss Function</h4>
<p>Like all machine learning algorithms, neural networks have a loss function that helps them guage how well they fit their training data. For most neural network applications we want to choose a loss function that has easy to calculate derivatives for our models output <span class="math inline">\(\hat{y}\)</span>. This is because training is most commonly done via gradient descent and for a gradient to be calculable we must have a differentiable loss <span class="math inline">\(L(\hat{y}, y)\)</span>.</p>

</div>
</div>
<div id="mathematical-operations-2" class="section level3">
<h3><span class="header-section-number">4.0.2</span> Mathematical Operations</h3>
<p>The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input.</p>
<p><em>Forward Propagation</em></p>
<p>Let our neural network with <span class="math inline">\(l\)</span> layers: <span class="math inline">\(N\)</span> have an input <span class="math inline">\(X\)</span> with <span class="math inline">\(k\)</span> covariates of interest and a target output of <span class="math inline">\(y\)</span>.</p>

<div id="refs" class="references">
<div>
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div>
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 1097–1105.</p>
</div>
<div>
<p>McCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4). Springer: 115–33.</p>
</div>
<div>
<p>Minsky, Marvin, Seymour A Papert, and Léon Bottou. 1969. <em>Perceptrons: An Introduction to Computational Geometry</em>. MIT press.</p>
</div>
<div>
<p>Rosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6). American Psychological Association: 386.</p>
</div>
<div>
<p>Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” <em>Nature</em> 323 (6088). Nature Publishing Group: 533–36.</p>
</div>
<div>
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning.” <em>arXiv.org</em>, March. <a href="http://arxiv.org/abs/1703.03864v2" class="uri">http://arxiv.org/abs/1703.03864v2</a>.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neuralnetworks.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-architectures.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
