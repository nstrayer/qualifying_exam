[
["neuralnetworks.html", "Chapter 3 Neural Networks 3.1 History 3.2 Universal Approximation Theorem 3.3 The Computation Graph 3.4 Mathematical Operations 3.5 Training 3.6 Activation Functions 3.7 Loss functions", " Chapter 3 Neural Networks A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler function. We can think of each application of a different mathematical function as providing a new representation of the input. (Goodfellow, Bengio, and Courville (2016)) Neural networks (sometimes referred to as multilayer perceptrons) are actually very simple models. Traditional modern neural networks simply pass data forward through a “network” that at each layer, performs a linear (also referred to as affine) transformation of its inputs followed by a element wise non-linear transformation (also called an activation function). In doing this they can build up successively more complex representations of data and use those to make decisions about it. This can be roughly thought about in the analogy of recognizing a cat. First you see, ears, a nose, two eyes, four feet, and a fluffy tail; next, you recognize the ears, nose and eyes as a head, the tail and legs as a body; and lastly the head and body as a cat. In performing this ‘classification’ of a cat you first constructed small features and successively stacked them to figure out what you were looking at. While this is obviously a stretched definition of how neural networks work, it actually is very close to how a special variant called Convolutional Neural Networks work for computer vision techniques (Olah, Mordvintsev, and Schubert (2017)). 3.1 History While neural networks’ popularity has taken off in recent years they are in fact not very new techniques. The neuron or smallest unit of a neural network was first introduced in 1943 (McCulloch and Pitts (1943)). It was then another 15 years until the perceptron (now commonly called ‘neural network’) was introduced (Rosenblatt (1958)) that tied together groups of neurons to represent more complex relationships. Another ten years later, in a textbook (Minsky, Papert, and Bottou (1969)) it was shown that a simple single layer perceptron was incapable of solving certain classes of problems like the XOR problem. The authors argued that the only way for a perceptron to overcome this hurdle would be to be stacked together, which while appealing was not possible to be trained effectively at the time. Figure 3.1: Example of the XOR problem. Classes encoded by color are not linearly seperable. It wasn’t until 1986 that a technique for training these multi-layer perceptrons was introduced (Rumelhart, Hinton, and Williams (1986)). Finally all of the algorithmic pieces were in place for deep neural networks, but interest stagnated due to the computational intensiveness of training the networks, a lack of data, and the success of other competing machine learning algorithms. Interest in the field of deep learning has had a massive resurgence in the second decade of the 21st century. Driven by the ability for massive amounts of data to be captured and also innovations in neural network architectures. One commonly sited tipping point for the current “deep learning revolution” was the 2012 paper (Krizhevsky, Sutskever, and Hinton (2012)) in which a deep neural network architecture known as “convolutional neural networks” won the ImageNet prize and showed massive improvements over traditional methods. 3.1.1 Biological Inspirations The word ‘neural’ in the name is reference to the fact that these models derive inspiration from how the brain actually works. With the individual nodes in a hidden ‘layer’ frequently being called a ‘neuron’. While the broad concept’s may be similar between the way animal brains work and neural networks it is important to note that the similarities end approximately at the network-ness of both systems. There have however, been some more recent work on trying to more closely mimic the brain structure with architectures such as capsule networks (Sabour, Frosst, and Hinton (2017)). In addition, neuroscience experiments have demonstrated that at least part of our visual system does truly perform these hierarchical stacks of features when recognizing objects (Kheradpisheh et al. (2016)). 3.1.2 Geometric Interpretation Another way of thinking of how neural networks work is as a building up a series of successive transformations of the data-space that attempt to eventually let the data be linearly separable. In this interpretation each layer can be seen as a shift and rotation of the data (the linear transformation), followed by a warping of the new space (the activation function). In his excellent blog post: Neural Networks, Manifolds, and Topology, Chris Olah gives an excellent visual demonstration of this. 3.2 Universal Approximation Theorem One powerful theoretical result from neural networks is that they are universal approximators (Hornik, Stinchcombe, and White (1989)). A neural network with a single hidden layer and non-linear activations functions on that layer can represent any borel-measurable function. This result means that there are no theoretical limits on the capabilities of neural networks. Obviously, in real-world situations this is not the case. We can not have infinite width hidden layers, infinite parameters requires infinite data, and even more limiting are our inefficient learning methods. All constraints considered though, the universal approximation theorem does provide confidence that, as long as they are properly constructed and trained, neural networks are amazingly flexible models. 3.3 The Computation Graph While the building blocks of neural networks are simple, very often complete models are composed of many blocks and thus reasoning about them becomes difficult. A method of dealing with this complexity, along with also helping in the intuition of many other properties, is to represent the networks as a ‘computation graph.’ A computation graph is simply directed acyclic diagram that shows the flow of data through the model. Each neuron is usually represented as a circle with the weights both in and out of the neuron’s value represented as edges. Sometimes, when the models get even larger, the layers (or groups of neurons) will get lumped into a single object (as on the right of the figure). 3.4 Mathematical Operations There are two basic operations that are performed on neural networks: forward propagation (or generating predictions from inputs) and back propagation (or calculating the gradients on all the model’s parameters for training). Here we will give a brief overview of these operations. For a more thorough treatment of this problem see Goodfellow, Bengio, and Courville (2016). 3.4.1 Terminology When covering the basic mathematical operations of a neural network it helps to have a reference for some of the terms that get used. This list provides the most commonly used terms for the models we will be describing. Neuron: An individual node in the network. Has two values: activation, or the value of the linear function of all inputs, and the post-activation-function value, or a simple transformation of the activation by the activation function. Affine Transformation: A linear transformation of an input (either data input or a hidden layer’s output). Essentially a linear regression. Bias Term: A constant term added to the affine transformation for a given neuron. Also known as an ‘intercept term.’. Activation Function: A simple non-linear function that takes an input and ‘squashes’ it to some range. A activation functions is the sigmoid, which takes an unbounded real-valued input and returns a value between -1 and 1. Layer: A collection of neurons who’s inputs typically all come from the same set of inputs. Hidden Layer: A layer who’s input is the output of a previous layer and who’s output is another layer. E.g. Input layer -&gt; hidden layer -&gt; output layer. The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input. 3.4.2 Forward Propagation Let a neural network with \\(l\\) layers and \\(k\\) dimensional input \\(X\\) and \\(m\\) dimensional output \\(\\hat{y}\\) attempting to predict the true target \\(y\\). Each layer is composed of \\(s_i, i \\in \\{1, 2, ...,l\\}\\) neurons and has respective non-linear activation function \\(f_i\\). Layer activation vectors \\(\\underline{a_i}\\) and post-activation function outputs \\(\\underline{o_i}\\). The weights into neuron \\(j\\) in layer \\(i\\) is given by \\(w_{(i,j)}\\) which is a vector of size \\(s_{i -1}\\). We can thus view the weights into the \\(i^{\\text{th}}\\) layer as a matrix \\(W_i\\) of of size \\(s_{i -1} \\times s_i\\). Finally the network has a differentiable with respect to \\(\\hat{y}\\) loss function: \\(L(\\hat{y}, y)\\). Forward propagation then proceeds as follows. Input \\(X\\) \\((1 \\times k\\)) is multiplied by \\(W_1\\) (size \\((k\\times s_i)\\) ) to achieve the activation values of the first hidden layer. \\(X \\cdot W_1 = \\underline{a_1}\\) The \\((1 \\times s_1)\\) activation vector of the first layer is then run element-wise though the first layer’s non-linear activation function to achieve the output of layer 1. \\(\\underline{o_1} = f_1(\\underline{a}_1)\\) This series of operations is then repeated through all the layers, (using the subsequent layers output vector as the input to the next layer,) until the final layer is reached. \\(\\underline{o_i} = f_i(\\underline{o_{(i - 1)}} \\cdot W_i)\\) Finally, the loss is calculated from the output of our final layer. \\(L_n = L(\\underline{o_l}, y) = L(\\hat{y}, y)\\) While not strictly necessary for forward propagation, the intermediate layer activation and output vectors are kept stored so they can be used in the later calculation of the gradient via back propagation. 3.4.3 Back Propigation If we are just looking to gather predictions from our model we can stop at forward propagation. However, most likely we want to train our model first. The most common technique for training neural networks is using a technique called back propagation (Rumelhart, Hinton, and Williams (1986)). In this algorithm the chain rule is used to walk back through the layers of the model starting from the loss function to the input weights in order to calculate each weight’s gradient. This gradient is then descended using any number of gradient descent algorithms. (We will demonstrate the simplest steepest descent method.) 3.4.3.1 The Chain Rule Back propagation is really nothing more than a repeated application of the chain rule from calculus. Let \\(x\\) be a single dimensional real valued input that is mapped through first equation \\(f\\) and then \\(g\\), both of which map from a single dimensional real number to another single dimensional real number: \\(f(x) = z, g(z) = y\\) or \\(g(f(x)) = y\\). The chain rule states that we can calculate the derivative the outcome with respect to the input by a series of multiplications of the derivatives of the composing functions. \\[\\begin{equation} \\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx} \\tag{3.1} \\end{equation}\\] This single dimensional example could be thought of as a neural network composed of two layers, each with a single dimension. While this is interesting the value of this observation comes when the chain rule is applied to higher dimensional values. 3.4.3.2 Expanding to higher dimensions Now, let \\(\\mathbb{x} \\in \\mathbb{R}^m\\) and \\(\\mathbb{z} \\in \\mathbb{R}^n\\). The function \\(f\\) maps from \\(\\mathbb{R}^m \\to \\mathbb{R}^n\\) and \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\). Further, let \\(\\textbf{z} = f(\\textbf{x})\\) and \\(y = g(\\textbf{z})\\). The chain rule can then be expressed as: \\[\\begin{equation} \\frac{dy}{dx_i} = \\sum_{j}\\frac{dy}{dz_j}\\frac{dz_j}{dx_i} \\tag{3.2} \\end{equation}\\] Or that the derivative of \\(y\\) with respect to the \\(i^{\\text{th}}\\) element of \\(\\textbf{x}\\) is the sum of the series of products of the derivatives of result \\(\\textbf{z}\\) that sits between the two values in the function composition. Another way of thinking of this is, the derivative of the output of the function composition \\(f \\circ g\\) with respect to some element of the input is the sum of all of the derivatives of all of the paths leading from the input element to the output. Figure 3.2: How back propigation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph. 3.4.3.3 Applied to Neural Networks To apply this technique to neural networks we simply need to make sure all components of our network are differentiable and then walk back from the loss function first to the output layer, calculating the gradients of the output layer’s neurons with respect to the loss. Once we have calculated the gradients with respect to the weights of the output layer we only need use those calculated gradients to calculate the gradients of the preceding layer. We can then proceed layer by layer, walking back through the model filling out each neuron’s weight gradients until we reach the input. To calculate the gradient of the weights for hidden layer \\(i\\) we can recall that the hidden layers output can be represented as \\(\\textbf{a}_i = f_i(\\textbf{W}\\cdot\\textbf{a}_{(i -1)})\\) (we’re omitting the bias term here for simplicity). Thus to calculate the gradient’s on the weights we can set \\(\\textbf{g}^*_i = \\textbf{g}_{i+1} \\odot f&#39;(a_{i+1})\\) to be the gradient un-activated by our layer’s activation function’s derivative. Then to find the derivative with respect to each neuron’s weights within the layer we multiply this un-activated gradient by the transpose of the layer’s output vector: \\(\\textbf{g}_i = \\textbf{g}^*_i \\textbf{o}_i^t\\). The fact that the calculation of these gradients is so simple is fundamental to deep learning. If it were more complicated extremely large networks (such as those used in computer vision) with millions of parameters to tune would simply be computationally infeasible to calculate gradients for. The run time a simple product of the number of neurons in each layer and the number of layers in the whole model. 3.4.3.4 Other methods Non-gradient based techniques have been explored for neural networks but ultimately found extremely slow for large networks. For instance evolutionary strategies (Salimans et al. (2017)), a new technique proposed by a group at OpenAi uses random searches of the parameter space. However, since often these parameter spaces are millions of dimensions an extremely large number of random perturbations are needed to inform a good direction to move. However, these methods are used when the loss function does not have a gradient (such as reinforcement learning scenarios). 3.5 Training 3.5.1 Gradient Descent Once the gradient is computed optimization proceeds the same way any gradient-based optimization problem does. The simplest algorithm for doing so being the steepest descent algorithm. In this algorithm each weight for our network is updated by adding its gradient multiplied by a small scalar called a ‘learning rate’. The new updated weights are then used in another forward propagation, followed by another back propagation and weight update and so on until the loss finds a minimum or some other stopping criteria is defined (such as a given number steps) is satisfied. 3.5.2 Gradient Descent Modifications There are many scenarios in which traditional steepest descent is not an ideal for finding minimums. A classic example is when the loss function takes the form of a long trough. In this case steepest descent will spend most of its time bouncing around between the sides of the trough due to over stepping the low point, thus wasting many of its iterations undoing its previous work rather than progressing in the true direction of the minimum. Figure 3.3: Demonstration of steepest descents limitations on trough like gradient surfaces. Much of each step’s progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from Nicolas Bertagnolli’s blog One of the most common additions to simple gradient descent is the addition of momentum. Momentum makes each step a function of not only the current gradient but also a decaying average of previous generations. For a thorough overview of momentum based methods see Goh (2017). 3.6 Activation Functions There are many different possible activation functions. One of the simplest being the sigmoid function which simply squashes the neuron’s activation from all real numbers to between 0 and 1. A newer and extremely common choice is the rectified linear unit (Relu) function (Nair and Hinton (2010)). Definition 3.1 (sigmoid activation function) \\[f(x) = e(x)/(1 + e(x))\\] Definition 3.2 (Rectified Linear Unit activation function) \\[f(x) = \\text{max}(0,x)\\] Figure 3.4: Comparison of the relu and sigmoid activation functions. Another, popular loss function that is used on the output layer when predicting categories is the softmax function. Unlike the previously mentioned loss functions this one takes a vector \\(\\textbf{o}\\) of length \\(m\\) representing the output of each of the layer’s neurons. Definition 3.3 (Softmax activation function.) \\[f(\\textbf{o})_i = \\frac{\\text{exp}(o_i)}{\\sum_{j = 1}^m \\text{exp}(o_j)}\\] This function serves to turn the output of a layer into a multivariate probability vector. The \\(m\\) outputs of the softmax sum to 1. It is important to note that, while the values sum to one, it’s not a true probability output, but will help indicate which class is most likely and roughly by what magnitude over other potential class choices. 3.7 Loss functions Like any machine learning model, neural networks have a loss function (sometimes called a cost function), or a function that helps the model know how close it is to performing as desired. As previously mentioned, in the case of neural networks we almost always want our loss function to be differentiable with respect to our model’s output \\(\\hat{\\textbf{y}}\\) as the derivative is needed for calculating the gradient on which the model is trained. Almost always the loss function is derived from the model’s likelihood and thus we can view our model training as a form of maximum likelihood estimation. If we let \\(\\theta\\) represent the parameters (weights) of our model with input \\(\\textbf{x}\\) and output \\(\\textbf{y}\\). The negative log-likelihood of our model is minimized when the Kullback Leibler divergence between our estimated model and the truth is minimized. Thus we can view training as traversing the space of \\(\\theta\\) in an attempt to find values that return the lowest value to this function. Definition 3.4 (General maximum likelihood loss function) \\[J(\\theta) = - \\mathbb{E}_{x,y \\sim \\hat{p} \\text{ data}} \\log{{p_{\\text{model}}(\\textbf{y} |\\textbf{x} })}\\] Of course, the precise form of the maximum likelihood loss function will change depending on the model. Here we will briefly introduce the two classes of loss functions: regression and classification, and the most commonly used functions within those classes. 3.7.1 Regression loss functions When fitting a neural network who’s outcome is a single real numbered variable \\(\\hat{y}\\) the most common loss function used is the mean square error. Definition 3.5 (Mean squared error loss) \\[\\text{MSE}(\\theta) = -\\frac{1}{n} \\sum_{i = 1}^{n} (\\hat{y}^{(i)} - y^{(i)})^2\\] Where \\(y_i\\) represents the outcome of the \\(i^{\\text{th}}\\) observation of \\(n\\) training observations. Minimizing the mean squared error can be shown to be equivalent to maximizing the likelihood function (Goodfellow, Bengio, and Courville (2016) chapter 5.5) and is thus a maximum likelihood method. 3.7.2 Classification loss functions There are two separate instances of classification that need to be considered. The binary case, where we are classifying a yes or no answer for a single class, or a categorical case, where we have more than two possible classes to choose from and need to place our data into one of them. In the single outcome case the approach to assume the outcome is a Bernoulli distribution over outcome \\(y\\) conditioned on the input \\(x\\). The resultant loss function is commonly referred to as ‘binary cross entropy.’1 Definition 3.6 (Binary cross entropy loss) \\[L(\\theta) = -\\frac{1}{n}\\sum_{i = 1}^n\\left[ y_i\\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)\\right]\\] This can be generalized into the multivariate case by swapping a multinomial model as the conditional distribution and thus adding more terms to the internal summation representing each possible class and it’s assigned probability. Definition 3.7 (Categorical cross entropy loss) \\[L(\\theta) = -\\frac{1}{n}\\sum_{i = 1}^n\\sum_{j = 1}^k y_{i,j}\\log(\\hat{y}_{i,j})\\] Where \\(y_{i,j}\\) represents the \\(i^{\\text{th}}\\) observation’s value for the \\(k^{\\text{th}}\\) category (dummy encoded). 3.7.3 Other loss functions While it is possible to use other loss functions in neural networks it is commonly not advised as the maximum-likelihood methods perform just as well and usually results in a more robust fit to the data. As deep learning situations usually have a large number of training observations the Cramer-Rao lower bound property, that a maximum likelihood estimator has the lowest variance of any consistent estimator, is particularly relevant. For a more thorough overview of the common loss functions used in deep learning see Goodfellow, Bengio, and Courville (2016) chapter six. References "]
]
