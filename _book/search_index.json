[
["neuralnetworks.html", "Chapter 3 Neural Networks 3.1 History 3.2 The Computation Graph 3.3 Mathematical Operations 3.4 Training 3.5 Activation Functions 3.6 Loss functions", " Chapter 3 Neural Networks A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler function. We can think of each application of a different mathematical function as providing a new representation of the input. (Goodfellow, Bengio, and Courville (2016)) Neural networks (sometimes refered to as multilayer perceptrons) are actually very simple models. Traditional modern neural networks simply pass data forward through a “network” that at each layer, performs a linear (also refered to as affenine) transformation of its inputs followed by a element wise non-linear transformation (also called an activation function). In doing this they can build up succesively more complex representations of data and use those to make decisions about it. This can be roughly thought about in the analogy of recognizing a cat. First you see, ears, a nose, two eyes, four feet, and a fluffy tail; next, you recognize the ears, nose and eyes as a head, the tail and legs as a body; and lastly the head and body as a cat. In performing this ‘classification’ of a cat you first constructed small features and successively stacked them to figure out what you were looking at. While this is obviously a streatched definition of how neural networks work, it actually is very close to how a special variant called Convolutional Neural Networks work for computer vision techniques ((???)). 3.1 History While neural networks’ popularity has taken off in recent years they are in fact not very new techniques. The neuron or smallest unit of a neural network was first introduced in 1943 (McCulloch and Pitts (1943)). It was then another 15 years until the perceptron (now commonly called ‘neural network’) was introduced (Rosenblatt (1958)) that tied together groups of neurons to represent more complex relationships. Another ten years later, in a textbook (Minsky, Papert, and Bottou (1969)) it was shown that a simple single layer perceptron was incapable of solving certain classes of problems like the XOR problem. The authors argued that the only way for a perceptron to overcome this hurdle would be to be stacked together, which while appealing was not possible to be trained effectively at the time. Figure 3.1: Example of the XOR problem. Classes encoded by color are not linearly seperable. It wasn’t until 1986 that a technique for training these multi-layer perceptrons was introduced (Rumelhart, Hinton, and Williams (1986)). Finally all of the algorithmic peices were in place for deep neural networks, but interest stagnated due to the computational intensiveness of training the networks, a lack of data, and the success of other competing machine learning algorithms. Interest in the field of deep learning has had a massive resurgence in the second decade of the 21st century. Driven by the ability for massive amounts of data to be captured and also inovations in neural network architectures. One commonly sited tipping point for the current “deep learning revolution” was the 2012 paper (Krizhevsky, Sutskever, and Hinton (2012)) in which a deep neural network architecture known as “convolutional neural networks” won the imagenet prize and showed massive improvements over traditional methods. 3.1.1 Biological Inspirations The word ‘neural’ in the name is reference to the fact that these models derive inspiration from how the brain actually works. With the individual nodes in a hidden ‘layer’ frequently being called a ‘neuron’. While the broad concept’s may be similar between the way animal brains work and neural networks it is important to note that the similarities end approximately at the network-ness of both systems. There have however, been some more recent work on trying to more closely mimic the brain structure with architectures such as capsule networks ((???)). In addition, neuroscience experiments have demonstrated that at least part of our visual system does truly perform these hierarchical stackings of features when recognizing objects ((???)). 3.1.2 Geometric Interpretation Another way of thinking of how neural networks work is as a building up a series of successive transformations of the data-space that attempt to eventually let the data be linearly seperable. In this intepretation each layer can be seen as a shift and rotation of the data (the linear transformation), followed by a warping of the new space (the activation function). In his excelent blogpost: Neural Networks, Manifolds, and Topology, Chris Olah gives an excelent visual demonstration of this. 3.2 The Computation Graph While the building blocks of neural networks are simple, very often complete models are composed of many blocks and thus reasoning about them becomes difficult. A method of dealing with this complexity, along with also helping in the intuition of many other properties, is to represent the networks as a ‘computation graph.’ A computation graph is simply directed acyclic diagram that shows the flow of data through the model. Each neuron is usually represented as a circle with the weights both in and out of the neuron’s value represented as edges. Sometimes, when the models get even larger, the layers (or groups of neurons) will get lumped into a single object (as on the right of the figure). 3.3 Mathematical Operations There are two basic operations that are performed on neural networks: forward propigation (or generating predictions from inputs) and back propigation (or calculating the gradients on all the model’s parameters for training). Here we will give a brief overfiew of these operations. For a more thorough treatment of this problem see Goodfellow, Bengio, and Courville (2016). 3.3.1 Terminology When covering the basic mathematical operations of a neural network it helps to have a reference for some of the terms that get used. This list provides the most commonly used terms for the models we will be describing. Neuron: An individual node in the network. Has two values: activation, or the value of the linear function of all inputs, and the post-activation-function value, or a simple transformation of the activation by the activation function. Affine Transformation: A linear transformation of an input (either data input or a hidden layer’s output). Essentially a linear regression. Bias Term: A constant term added to the affine transformation for a given neuron. Also known as an ‘intercept term.’. Activation Function: A simple non-linear function that takes an input and ‘squashes’ it to some range. A activation functions is the sigmoid, which takes an unbounded real-valued input and returns a value between -1 and 1. Layer: A collection of neurons who’s inputs typically all come from the same set of inputs. Hidden Layer: A layer who’s input is the output of a previous layer and who’s output is another layer. E.g. Input layer -&gt; hidden layer -&gt; output layer. The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input. 3.3.2 Forward Propagation Let a neural network with \\(l\\) layers and \\(k\\) dimensional input \\(X\\) and \\(m\\) dimensional output \\(\\hat{y}\\) attempting to predict the true target \\(y\\). Each layer is composed of \\(s_i, i \\in \\{1, 2, ...,l\\}\\) neurons and has respective non-linear activation function \\(f_i\\). Layer activation vectors \\(\\underline{a_i}\\) and post-activation function outputs \\(\\underline{o_i}\\). The weights into neuron \\(j\\) in layer \\(i\\) is given by \\(w_{(i,j)}\\) which is a vector of size \\(s_{i -1}\\). We can thus view the weights into the \\(i^{\\text{th}}\\) layer as a matrix \\(W_i\\) of of size \\(s_{i -1} \\times s_i\\). Finally the network has a differntiable with respect to \\(\\hat{y}\\) loss function: \\(L(\\hat{y}, y)\\). Forward propigation then proceeds as follows. Input \\(X\\) \\((1 \\times k\\)) is multiplied by \\(W_1\\) (size \\((k\\times s_i)\\) ) to acheive the activation values of the first hidden layer. \\(X \\cdot W_1 = \\underline{a_1}\\) The \\((1 \\times s_1)\\) activation vector of the first layer is then run element-wise though the first layer’s non-linear activation function to acheive the output of layer 1. \\(\\underline{o_1} = f_1(\\underline{a}_1)\\) This series of opperations is then repeated through all the layers, (using the subsequent layers output vector as the input to the next layer,) until the final layer is reached. \\(\\underline{o_i} = f_i(\\underline{o_{(i - 1)}} \\cdot W_i)\\) Finally, the loss is calculated from the output of our final layer. \\(L_n = L(\\underline{o_l}, y) = L(\\hat{y}, y)\\) While not strictly neccesary for forward propigation, the intermediate layer activation and output vectors are kept stored so they can be used in the later calculation of the gradient via back propigation. 3.3.3 Back Propigation If we are just looking to gather predictions from our model we can stop at forward propigation. However, most likely we want to train our model first. The most common techinique for training neural networks is using a technique called back propigation (Rumelhart, Hinton, and Williams (1986)). In this algorithm the chain rule is used to walk back through the layers of the model starting from the loss function to the input weights in order to calculate each weight’s gradient. This gradient is then descended using any number of gradient descent algorithms. (We will demonstrate the simpliest steepest descent method.) 3.3.3.1 The Chain Rule Back propigation is really nothing more than a repeated application of the chain rule from calculus. Let \\(x\\) be a single dimensional real valued input that is mapped through first equation \\(f\\) and then \\(g\\), both of which map from a single dimesional real number to another single dimensional real number: \\(f(x) = z, g(z) = y\\) or \\(g(f(x)) = y\\). The chain rule states that we can calculate the derivative the outcome with respect to the input by a series of multiplications of the derivatives of the composing functions. \\[\\begin{equation} \\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx} \\tag{3.1} \\end{equation}\\] This single dimensional example could be thought of as a neural network composed of two layers, each with a single dimension. While this is interesting the value of this observation comes when the chain rule is applied to higher dimensional values. 3.3.3.2 Expanding to higher dimensions Now, let \\(\\mathbb{x} \\in \\mathbb{R}^m\\) and \\(\\mathbb{z} \\in \\mathbb{R}^n\\). The function \\(f\\) maps from \\(\\mathbb{R}^m \\to \\mathbb{R}^n\\) and \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\). Further, let \\(\\textbf{z} = f(\\textbf{x})\\) and \\(y = g(\\textbf{z})\\). The chain rule can then be expressed as: \\[\\begin{equation} \\frac{dy}{dx_i} = \\sum_{j}\\frac{dy}{dz_j}\\frac{dz_j}{dx_i} \\tag{3.2} \\end{equation}\\] Or that the derivative of \\(y\\) with respect to the \\(i^{\\text{th}}\\) element of \\(\\textbf{x}\\) is the sum of the series of products of the derivatives of result \\(\\textbf{z}\\) that sits between the two values in the function composition. Another way of thinking of this is, the derivative of the output of the function composition \\(f \\circ g\\) with repect to some element of the input is the sum of all of the derivatives of all of the paths leading from the input element to the output. Figure 3.2: How back propigation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph. 3.3.3.3 Applied to Neural Networks To apply this technique to neural networks we simply need to make sure all components of our network are differentiable and then walk back from the loss function first to the output layer, calculating the gradients of the output layer’s neurons with respect to the loss. Once we have calculated the gradients with respect to the weights of the output layer we only need use those calculated gradients to calculate the gradients of the preceeding layer. We can then proceed layer by layer, walking back through the model filling out each neuron’s weight gradients until we reach the input. To calculate the gradient of the weights for hidden layer \\(i\\) we can recall that the hidden layers output can be represented as \\(\\textbf{a}_i = f_i(\\textbf{W}\\cdot\\textbf{a}_{(i -1)})\\) (we’re ommiting the bias term here for simplicity). Thus to calculate the gradient’s on the weights we can set \\(\\textbf{g}^*_i = \\textbf{g}_{i+1} \\odot f&#39;(a_{i+1})\\) to be the gradient un-activated by our layer’s activation function’s derivative. Then to find the derivative with respect to each neuron’s weights within the layer we multiply this un-activated gradient by the transpose of the layer’s output vector: \\(\\textbf{g}_i = \\textbf{g}^*_i \\textbf{o}_i^t\\). The fact that the calculation of these gradients is so simple is fundemental to deep learning. If it were more complicated extremely large networks (such as those used in computer vision) with millions of parameters to tune would simply be computationally infesable to calculate gradients for. The runtime a simple product of the number of neurons in each layer and the number of layers in the whole model. 3.3.3.4 Other methods Non-gradient based techniques have been explored for neural networks but ultimately found extremely slow for large networks. For instance evolutionary strategies (Salimans et al. (2017)), a new techinque proposed by a group at OpenAi uses random searches of the parameter space. However, since often these parameter spaces are millions of dimensions an extremely large number of random perturbations are needed to inform a good direction to move. However, these methods are used when the loss function does not have a gradient (such as reinforcement learning scenarios). 3.4 Training 3.4.1 Gradient Descent Once the gradient is computed optimization proceeds the same way any gradient-based optimization problem does. The simplest algorithm for doing so being the steepest descent algorithm. In this algorithm each weight for our network is updated by adding its gradient multiplied by a small scalar called a ‘learning rate’. The new updated weights are then used in another forward propigation, followed by another back propigation and weight update and so on until the loss finds a minimum or some other stopping criteria is defined (such as a given number steps) is satisfied. 3.4.2 Gradient Descent Modifications There are many scenarios in which traditional steepest descent is not an ideal for finding minimums. A classic example is when the loss function takes the form of a long trough. In this case steepest descent will spend most of its time bouncing around between the sides of the trough due to over stepping the low point, thus wasting many of its iterations undoing its previous work rather than progressing in the true direction of the minimum. Figure 3.3: Demonstration of steepest descents limitations on trough like gradient surfaces. Much of each step’s progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from Nicolas Bertagnolli’s blog One of the most common additions to simple gradient descent is the addition of momentum. Momentum makes each step a function of not only the current gradient but also a decaying average of previous generations. For a thorough overview of momentum based methods see Goh (2017). 3.5 Activation Functions There are many different possible activation functions. One of the simplest being the sigmoid function \\(f(x) = e(x)/(1 + e(x))\\) which simply squashes the neuron’s activation to between 0 and 1. Newer and extremely common is the Relu function (Nair and Hinton (2010)). 3.6 Loss functions Regression loss functions Classification loss functions References "]
]
