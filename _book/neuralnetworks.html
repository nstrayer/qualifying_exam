<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="architectures.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Report Layout</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#continuous-classification"><i class="fa fa-check"></i><b>2.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>2.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#activity-prediction"><i class="fa fa-check"></i><b>2.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#ehr-monitoring"><i class="fa fa-check"></i><b>2.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#hospital-automation"><i class="fa fa-check"></i><b>2.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#history-of-methods"><i class="fa fa-check"></i><b>2.3</b> History of methods</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#windowed-regression"><i class="fa fa-check"></i><b>2.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#transformation-methods"><i class="fa fa-check"></i><b>2.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="2.3.3" data-path="intro.html"><a href="intro.html#hidden-markov-models"><i class="fa fa-check"></i><b>2.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="2.3.4" data-path="intro.html"><a href="intro.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>2.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>3</b> Neural Networks</a><ul>
<li class="chapter" data-level="3.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>3.1</b> History</a><ul>
<li class="chapter" data-level="3.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>3.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="3.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>3.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="3.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>3.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="3.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>3.4</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="3.4.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>3.4.1</b> Terminology</a></li>
<li class="chapter" data-level="3.4.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>3.4.2</b> Forward Propagation</a></li>
<li class="chapter" data-level="3.4.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propigation"><i class="fa fa-check"></i><b>3.4.3</b> Back Propigation</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>3.5</b> Training</a><ul>
<li class="chapter" data-level="3.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>3.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="3.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>3.5.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>3.6</b> Activation Functions</a></li>
<li class="chapter" data-level="3.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>3.7</b> Loss functions</a><ul>
<li class="chapter" data-level="3.7.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>3.7.1</b> Regression loss functions</a></li>
<li class="chapter" data-level="3.7.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>3.7.2</b> Classification loss functions</a></li>
<li class="chapter" data-level="3.7.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>3.7.3</b> Other loss functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>4</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>4.1</b> Terminology</a></li>
<li class="chapter" data-level="4.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>4.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="4.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>4.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="4.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>4.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="4.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>4.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="4.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>4.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="4.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>4.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="4.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>4.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="4.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>4.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>4.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="4.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>4.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="4.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>4.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>4.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="4.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>4.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>5</b> Oportunities for advancing field</a><ul>
<li class="chapter" data-level="5.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>5.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="5.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>5.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="5.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware-hardware"><i class="fa fa-check"></i><b>5.1.2</b> Runtime on mobile hardware hardware</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>5.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="5.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>5.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="5.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>5.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>5.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>5.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="5.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>5.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="placeholder.html"><a href="placeholder.html"><i class="fa fa-check"></i><b>6</b> Placeholder</a><ul>
<li class="chapter" data-level="6.0.1" data-path="placeholder.html"><a href="placeholder.html#mathematical-operations-1"><i class="fa fa-check"></i><b>6.0.1</b> Mathematical Operations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="placeholder-1.html"><a href="placeholder-1.html"><i class="fa fa-check"></i><b>7</b> Placeholder</a><ul>
<li class="chapter" data-level="7.0.1" data-path="placeholder-1.html"><a href="placeholder-1.html#mathematical-operations-2"><i class="fa fa-check"></i><b>7.0.1</b> Mathematical Operations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neuralnetworks" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Neural Networks</h1>
<blockquote>
<p>A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler function. We can think of each application of a different mathematical function as providing a new representation of the input. (<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span>)</p>
</blockquote>
<p>Neural networks (sometimes referred to as multilayer perceptrons) are actually very simple models. Traditional modern neural networks simply pass data forward through a “network” that at each layer, performs a linear (also referred to as affine) transformation of its inputs followed by a element wise non-linear transformation (also called an activation function). In doing this they can build up successively more complex representations of data and use those to make decisions about it.</p>
<p>This can be roughly thought about in the analogy of recognizing a cat. First you see, ears, a nose, two eyes, four feet, and a fluffy tail; next, you recognize the ears, nose and eyes as a head, the tail and legs as a body; and lastly the head and body as a cat. In performing this ‘classification’ of a cat you first constructed small features and successively stacked them to figure out what you were looking at. While this is obviously a stretched definition of how neural networks work, it actually is very close to how a special variant called Convolutional Neural Networks work for computer vision techniques (<span class="citation">Olah, Mordvintsev, and Schubert (<a href="#ref-cnn_vis">2017</a>)</span>).</p>
<div id="history" class="section level2">
<h2><span class="header-section-number">3.1</span> History</h2>
<p>While neural networks’ popularity has taken off in recent years they are in fact not very new techniques. The neuron or smallest unit of a neural network was first introduced in 1943 (<span class="citation">McCulloch and Pitts (<a href="#ref-mcculloch_neuron">1943</a>)</span>). It was then another 15 years until the perceptron (now commonly called ‘neural network’) was introduced (<span class="citation">Rosenblatt (<a href="#ref-rosenblatt_perceptron">1958</a>)</span>) that tied together groups of neurons to represent more complex relationships.</p>
<p>Another ten years later, in a textbook (<span class="citation">Minsky, Papert, and Bottou (<a href="#ref-minsky_perceptrons">1969</a>)</span>) it was shown that a simple single layer perceptron was incapable of solving certain classes of problems like the XOR problem. The authors argued that the only way for a perceptron to overcome this hurdle would be to be stacked together, which while appealing was not possible to be trained effectively at the time.</p>
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/nn06_rbfn_xor/html/nn06_rbfn_xor_2_newrb_01.png" alt="Example of the XOR problem. Classes encoded by color are not linearly seperable."  />
<p class="caption">
Figure 3.1: Example of the XOR problem. Classes encoded by color are not linearly seperable.
</p>
</div>
<p>It wasn’t until 1986 that a technique for training these multi-layer perceptrons was introduced (<span class="citation">Rumelhart, Hinton, and Williams (<a href="#ref-backprop_1986">1986</a>)</span>). Finally all of the algorithmic pieces were in place for deep neural networks, but interest stagnated due to the computational intensiveness of training the networks, a lack of data, and the success of other competing machine learning algorithms.</p>
<p>Interest in the field of deep learning has had a massive resurgence in the second decade of the 21st century. Driven by the ability for massive amounts of data to be captured and also innovations in neural network architectures. One commonly sited tipping point for the current “deep learning revolution” was the 2012 paper (<span class="citation">Krizhevsky, Sutskever, and Hinton (<a href="#ref-imagenet_2012">2012</a>)</span>) in which a deep neural network architecture known as “convolutional neural networks” won the ImageNet prize and showed massive improvements over traditional methods.</p>
<div id="biological-inspirations" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Biological Inspirations</h3>
<p>The word ‘neural’ in the name is reference to the fact that these models derive inspiration from how the brain actually works. With the individual nodes in a hidden ‘layer’ frequently being called a ‘neuron’. While the broad concept’s may be similar between the way animal brains work and neural networks it is important to note that the similarities end approximately at the network-ness of both systems. There have however, been some more recent work on trying to more closely mimic the brain structure with architectures such as capsule networks (<span class="citation">Sabour, Frosst, and Hinton (<a href="#ref-capsnet">2017</a>)</span>). In addition, neuroscience experiments have demonstrated that at least part of our visual system does truly perform these hierarchical stacks of features when recognizing objects (<span class="citation">Kheradpisheh et al. (<a href="#ref-cnn_animals">2016</a>)</span>).</p>
</div>
<div id="geometric-interpretation" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Geometric Interpretation</h3>
<p>Another way of thinking of how neural networks work is as a building up a series of successive transformations of the data-space that attempt to eventually let the data be linearly separable. In this interpretation each layer can be seen as a shift and rotation of the data (the linear transformation), followed by a warping of the new space (the activation function). In his excellent blog post: <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural Networks, Manifolds, and Topology</a>, Chris Olah gives an excellent visual demonstration of this.</p>
</div>
</div>
<div id="universal-approximation-theorem" class="section level2">
<h2><span class="header-section-number">3.2</span> Universal Approximation Theorem</h2>
<p>One powerful theoretical result from neural networks is that they are universal approximators (<span class="citation">Hornik, Stinchcombe, and White (<a href="#ref-universal_approximators">1989</a>)</span>). A neural network with a single hidden layer and non-linear activations functions on that layer can represent <em>any</em> borel-measurable function. This result means that there are no theoretical limits on the capabilities of neural networks. Obviously, in real-world situations this is not the case. We can not have infinite width hidden layers, infinite parameters requires infinite data, and even more limiting are our inefficient learning methods. All constraints considered though, the universal approximation theorem does provide confidence that, as long as they are properly constructed and trained, neural networks are amazingly flexible models.</p>
</div>
<div id="the-computation-graph" class="section level2">
<h2><span class="header-section-number">3.3</span> The Computation Graph</h2>
<p>While the building blocks of neural networks are simple, very often complete models are composed of many blocks and thus reasoning about them becomes difficult. A method of dealing with this complexity, along with also helping in the intuition of many other properties, is to represent the networks as a ‘computation graph.’</p>
<p>A computation graph is simply directed acyclic diagram that shows the flow of data through the model. Each neuron is usually represented as a circle with the weights both in and out of the neuron’s value represented as edges.</p>
<!-- [image:0E87AF40-C4A7-4EFD-8ED1-D04FAD772692-1368-0000019FCB3F31C6/image.png] -->
<!-- /Example of a simple computation graph. From @dl_book, figure 6.2./ -->
<p>Sometimes, when the models get even larger, the layers (or groups of neurons) will get lumped into a single object (as on the right of the figure).</p>
</div>
<div id="mathematical-operations" class="section level2">
<h2><span class="header-section-number">3.4</span> Mathematical Operations</h2>
<p>There are two basic operations that are performed on neural networks: forward propagation (or generating predictions from inputs) and back propagation (or calculating the gradients on all the model’s parameters for training). Here we will give a brief overview of these operations. For a more thorough treatment of this problem see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span>.</p>
<div id="terminology" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Terminology</h3>
<p>When covering the basic mathematical operations of a neural network it helps to have a reference for some of the terms that get used. This list provides the most commonly used terms for the models we will be describing.</p>
<p><strong>Neuron</strong>: An individual node in the network. Has two values: activation, or the value of the linear function of all inputs, and the post-activation-function value, or a simple transformation of the activation by the activation function.</p>
<p><strong>Affine Transformation</strong>: A linear transformation of an input (either data input or a hidden layer’s output). Essentially a linear regression.</p>
<p><strong>Bias Term</strong>: A constant term added to the affine transformation for a given neuron. Also known as an ‘intercept term.’.</p>
<p><strong>Activation Function</strong>: A simple non-linear function that takes an input and ‘squashes’ it to some range. A activation functions is the sigmoid, which takes an unbounded real-valued input and returns a value between -1 and 1.</p>
<p><strong>Layer</strong>: A collection of neurons who’s inputs typically all come from the same set of inputs.</p>
<p><strong>Hidden Layer</strong>: A layer who’s input is the output of a previous layer and who’s output is another layer. E.g. Input layer -&gt; hidden layer -&gt; output layer.</p>
<p>The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input.</p>
</div>
<div id="forward-propagation" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Forward Propagation</h3>
<p>Let a neural network with <span class="math inline">\(l\)</span> layers and <span class="math inline">\(k\)</span> dimensional input <span class="math inline">\(X\)</span> and <span class="math inline">\(m\)</span> dimensional output <span class="math inline">\(\hat{y}\)</span> attempting to predict the true target <span class="math inline">\(y\)</span>. Each layer is composed of <span class="math inline">\(s_i, i \in \{1, 2, ...,l\}\)</span> neurons and has respective non-linear activation function <span class="math inline">\(f_i\)</span>. Layer activation vectors <span class="math inline">\(\underline{a_i}\)</span> and post-activation function outputs <span class="math inline">\(\underline{o_i}\)</span>. The weights into neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(i\)</span> is given by <span class="math inline">\(w_{(i,j)}\)</span> which is a vector of size <span class="math inline">\(s_{i -1}\)</span>. We can thus view the weights into the <span class="math inline">\(i^{\text{th}}\)</span> layer as a matrix <span class="math inline">\(W_i\)</span> of of size <span class="math inline">\(s_{i -1} \times s_i\)</span>. Finally the network has a differentiable with respect to <span class="math inline">\(\hat{y}\)</span> loss function: <span class="math inline">\(L(\hat{y}, y)\)</span>.</p>
<p>Forward propagation then proceeds as follows.</p>
<ol style="list-style-type: decimal">
<li>Input <span class="math inline">\(X\)</span> <span class="math inline">\((1 \times k\)</span>) is multiplied by <span class="math inline">\(W_1\)</span> (size <span class="math inline">\((k\times s_i)\)</span> ) to achieve the <em>activation values</em> of the first hidden layer.
<ul>
<li><span class="math inline">\(X \cdot W_1 = \underline{a_1}\)</span></li>
</ul></li>
<li>The <span class="math inline">\((1 \times s_1)\)</span> activation vector of the first layer is then run element-wise though the first layer’s non-linear activation function to achieve the output of layer 1.
<ul>
<li><span class="math inline">\(\underline{o_1} = f_1(\underline{a}_1)\)</span></li>
</ul></li>
<li>This series of operations is then repeated through all the layers, (using the subsequent layers output vector as the input to the next layer,) until the final layer is reached.
<ul>
<li><span class="math inline">\(\underline{o_i} = f_i(\underline{o_{(i - 1)}} \cdot W_i)\)</span></li>
</ul></li>
<li>Finally, the loss is calculated from the output of our final layer.
<ul>
<li><span class="math inline">\(L_n = L(\underline{o_l}, y) = L(\hat{y}, y)\)</span></li>
</ul></li>
</ol>
<p>While not strictly necessary for forward propagation, the intermediate layer activation and output vectors are kept stored so they can be used in the later calculation of the gradient via back propagation.</p>
</div>
<div id="back-propigation" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Back Propigation</h3>
<p>If we are just looking to gather predictions from our model we can stop at forward propagation. However, most likely we want to train our model first. The most common technique for training neural networks is using a technique called back propagation (<span class="citation">Rumelhart, Hinton, and Williams (<a href="#ref-backprop_1986">1986</a>)</span>). In this algorithm the chain rule is used to walk back through the layers of the model starting from the loss function to the input weights in order to calculate each weight’s gradient. This gradient is then descended using any number of gradient descent algorithms. (We will demonstrate the simplest steepest descent method.)</p>
<div id="the-chain-rule" class="section level4">
<h4><span class="header-section-number">3.4.3.1</span> The Chain Rule</h4>
<p>Back propagation is really nothing more than a repeated application of the chain rule from calculus. Let <span class="math inline">\(x\)</span> be a single dimensional real valued input that is mapped through first equation <span class="math inline">\(f\)</span> and then <span class="math inline">\(g\)</span>, both of which map from a single dimensional real number to another single dimensional real number: <span class="math inline">\(f(x) = z, g(z) = y\)</span> or <span class="math inline">\(g(f(x)) = y\)</span>. The chain rule states that we can calculate the derivative the outcome with respect to the input by a series of multiplications of the derivatives of the composing functions.</p>
<span class="math display" id="eq:chainrule">\[\begin{equation} 
  \frac{dy}{dx} = \frac{dy}{dz}\frac{dz}{dx}
  \tag{3.1}
\end{equation}\]</span>
<p>This single dimensional example could be thought of as a neural network composed of two layers, each with a single dimension. While this is interesting the value of this observation comes when the chain rule is applied to higher dimensional values.</p>
</div>
<div id="expanding-to-higher-dimensions" class="section level4">
<h4><span class="header-section-number">3.4.3.2</span> Expanding to higher dimensions</h4>
<p>Now, let <span class="math inline">\(\mathbb{x} \in \mathbb{R}^m\)</span> and <span class="math inline">\(\mathbb{z} \in \mathbb{R}^n\)</span>. The function <span class="math inline">\(f\)</span> maps from <span class="math inline">\(\mathbb{R}^m \to \mathbb{R}^n\)</span> and <span class="math inline">\(g: \mathbb{R}^n \to \mathbb{R}\)</span>. Further, let <span class="math inline">\(\textbf{z} = f(\textbf{x})\)</span> and <span class="math inline">\(y = g(\textbf{z})\)</span>. The chain rule can then be expressed as:</p>
<span class="math display" id="eq:chainrulemv">\[\begin{equation} 
  \frac{dy}{dx_i} = \sum_{j}\frac{dy}{dz_j}\frac{dz_j}{dx_i}
  \tag{3.2}
\end{equation}\]</span>
<p>Or that the derivative of <span class="math inline">\(y\)</span> with respect to the <span class="math inline">\(i^{\text{th}}\)</span> element of <span class="math inline">\(\textbf{x}\)</span> is the sum of the series of products of the derivatives of result <span class="math inline">\(\textbf{z}\)</span> that sits between the two values in the function composition.</p>
<p>Another way of thinking of this is, the derivative of the output of the function composition <span class="math inline">\(f \circ g\)</span> with respect to some element of the input is the sum of all of the derivatives of all of the paths leading from the input element to the output.</p>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="figures/backprop.png" alt="How back propigation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph."  />
<p class="caption">
Figure 3.2: How back propigation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph.
</p>
</div>
</div>
<div id="applied-to-neural-networks" class="section level4">
<h4><span class="header-section-number">3.4.3.3</span> Applied to Neural Networks</h4>
<p>To apply this technique to neural networks we simply need to make sure all components of our network are differentiable and then walk back from the loss function first to the output layer, calculating the gradients of the output layer’s neurons with respect to the loss. Once we have calculated the gradients with respect to the weights of the output layer we only need use those calculated gradients to calculate the gradients of the preceding layer. We can then proceed layer by layer, walking back through the model filling out each neuron’s weight gradients until we reach the input.</p>
<p>To calculate the gradient of the weights for hidden layer <span class="math inline">\(i\)</span> we can recall that the hidden layers output can be represented as <span class="math inline">\(\textbf{a}_i = f_i(\textbf{W}\cdot\textbf{a}_{(i -1)})\)</span> (we’re omitting the bias term here for simplicity). Thus to calculate the gradient’s on the weights we can set <span class="math inline">\(\textbf{g}^*_i = \textbf{g}_{i+1} \odot f&#39;(a_{i+1})\)</span> to be the gradient un-activated by our layer’s activation function’s derivative. Then to find the derivative with respect to each neuron’s weights within the layer we multiply this un-activated gradient by the transpose of the layer’s output vector: <span class="math inline">\(\textbf{g}_i = \textbf{g}^*_i \textbf{o}_i^t\)</span>.</p>
<p>The fact that the calculation of these gradients is so simple is fundamental to deep learning. If it were more complicated extremely large networks (such as those used in computer vision) with millions of parameters to tune would simply be computationally infeasible to calculate gradients for. The run time a simple product of the number of neurons in each layer and the number of layers in the whole model.</p>
</div>
<div id="other-methods" class="section level4">
<h4><span class="header-section-number">3.4.3.4</span> Other methods</h4>
<p>Non-gradient based techniques have been explored for neural networks but ultimately found extremely slow for large networks. For instance evolutionary strategies (<span class="citation">Salimans et al. (<a href="#ref-evostrat">2017</a>)</span>), a new technique proposed by a group at OpenAi uses random searches of the parameter space. However, since often these parameter spaces are millions of dimensions an extremely large number of random perturbations are needed to inform a good direction to move. However, these methods are used when the loss function does not have a gradient (such as reinforcement learning scenarios).</p>
</div>
</div>
</div>
<div id="training" class="section level2">
<h2><span class="header-section-number">3.5</span> Training</h2>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Gradient Descent</h3>
<p>Once the gradient is computed optimization proceeds the same way any gradient-based optimization problem does. The simplest algorithm for doing so being the steepest descent algorithm. In this algorithm each weight for our network is updated by adding its gradient multiplied by a small scalar called a ‘learning rate’. The new updated weights are then used in another forward propagation, followed by another back propagation and weight update and so on until the loss finds a minimum or some other stopping criteria is defined (such as a given number steps) is satisfied.</p>
</div>
<div id="gradient-descent-modifications" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Gradient Descent Modifications</h3>
<p>There are many scenarios in which traditional steepest descent is not an ideal for finding minimums. A classic example is when the loss function takes the form of a long trough. In this case steepest descent will spend most of its time bouncing around between the sides of the trough due to over stepping the low point, thus wasting many of its iterations undoing its previous work rather than progressing in the true direction of the minimum.</p>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="http://www.nbertagnolli.com/assets/descent_methods/GDWBLS.png" alt="Demonstration of steepest descents limitations on trough like gradient surfaces. Much of each step's progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from [Nicolas Bertagnolli's blog](http://www.nbertagnolli.com/)" width="300" height="300" />
<p class="caption">
Figure 3.3: Demonstration of steepest descents limitations on trough like gradient surfaces. Much of each step’s progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from <a href="http://www.nbertagnolli.com/">Nicolas Bertagnolli’s blog</a>
</p>
</div>
<p>One of the most common additions to simple gradient descent is the addition of momentum. Momentum makes each step a function of not only the current gradient but also a decaying average of previous generations. For a thorough overview of momentum based methods see <span class="citation">Goh (<a href="#ref-momentum">2017</a>)</span>.</p>
</div>
</div>
<div id="activation-functions" class="section level2">
<h2><span class="header-section-number">3.6</span> Activation Functions</h2>
<p>There are many different possible activation functions. One of the simplest being the sigmoid function which simply squashes the neuron’s activation from all real numbers to between 0 and 1. A newer and extremely common choice is the rectified linear unit (Relu) function (<span class="citation">Nair and Hinton (<a href="#ref-relu_paper">2010</a>)</span>).</p>

<div class="definition">
<span id="def:sigmoid" class="definition"><strong>Definition 3.1  (sigmoid activation function)  </strong></span><span class="math display">\[f(x) = e(x)/(1 + e(x))\]</span>
</div>


<div class="definition">
<span id="def:relu" class="definition"><strong>Definition 3.2  (Rectified Linear Unit activation function)  </strong></span><span class="math display">\[f(x) = \text{max}(0,x)\]</span>
</div>

<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" alt="Comparison of the relu and sigmoid activation functions." width="672" />
<p class="caption">
Figure 3.4: Comparison of the relu and sigmoid activation functions.
</p>
</div>
<p>Another, popular loss function that is used on the output layer when predicting categories is the softmax function. Unlike the previously mentioned loss functions this one takes a vector <span class="math inline">\(\textbf{o}\)</span> of length <span class="math inline">\(m\)</span> representing the output of each of the layer’s neurons.</p>

<div class="definition">
<span id="def:softmax" class="definition"><strong>Definition 3.3  (Softmax activation function.)  </strong></span><span class="math display">\[f(\textbf{o})_i = \frac{\text{exp}(o_i)}{\sum_{j = 1}^m \text{exp}(o_j)}\]</span>
</div>

<p>This function serves to turn the output of a layer into a multivariate probability vector. The <span class="math inline">\(m\)</span> outputs of the softmax sum to 1. It is important to note that, while the values sum to one, it’s not a true probability output, but will help indicate which class is most likely and roughly by what magnitude over other potential class choices.</p>
</div>
<div id="loss-functions" class="section level2">
<h2><span class="header-section-number">3.7</span> Loss functions</h2>
<p>Like any machine learning model, neural networks have a loss function (sometimes called a cost function), or a function that helps the model know how close it is to performing as desired. As previously mentioned, in the case of neural networks we almost always want our loss function to be differentiable with respect to our model’s output <span class="math inline">\(\hat{\textbf{y}}\)</span> as the derivative is needed for calculating the gradient on which the model is trained.</p>
<p>Almost always the loss function is derived from the model’s likelihood and thus we can view our model training as a form of maximum likelihood estimation. If we let <span class="math inline">\(\theta\)</span> represent the parameters (weights) of our model with input <span class="math inline">\(\textbf{x}\)</span> and output <span class="math inline">\(\textbf{y}\)</span>. The negative log-likelihood of our model is minimized when the Kullback Leibler divergence between our estimated model and the truth is minimized. Thus we can view training as traversing the space of <span class="math inline">\(\theta\)</span> in an attempt to find values that return the lowest value to this function.</p>

<div class="definition">
<span id="def:negloglik" class="definition"><strong>Definition 3.4  (General maximum likelihood loss function)  </strong></span><span class="math display">\[J(\theta) = - \mathbb{E}_{x,y \sim \hat{p} \text{ data}} \log{{p_{\text{model}}(\textbf{y} |\textbf{x} })}\]</span>
</div>

<p>Of course, the precise form of the maximum likelihood loss function will change depending on the model. Here we will briefly introduce the two classes of loss functions: regression and classification, and the most commonly used functions within those classes.</p>
<div id="regression-loss-functions" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Regression loss functions</h3>
<p>When fitting a neural network who’s outcome is a single real numbered variable <span class="math inline">\(\hat{y}\)</span> the most common loss function used is the mean square error.</p>

<div class="definition">
<p><span id="def:mse" class="definition"><strong>Definition 3.5  (Mean squared error loss)  </strong></span><span class="math display">\[\text{MSE}(\theta) = -\frac{1}{n} \sum_{i = 1}^{n} (\hat{y}^{(i)} - y^{(i)})^2\]</span></p>
Where <span class="math inline">\(y_i\)</span> represents the outcome of the <span class="math inline">\(i^{\text{th}}\)</span> observation of <span class="math inline">\(n\)</span> training observations.
</div>

<p>Minimizing the mean squared error can be shown to be equivalent to maximizing the likelihood function (<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter 5.5) and is thus a maximum likelihood method.</p>
</div>
<div id="classification-loss-functions" class="section level3">
<h3><span class="header-section-number">3.7.2</span> Classification loss functions</h3>
<p>There are two separate instances of classification that need to be considered. The binary case, where we are classifying a yes or no answer for a single class, or a categorical case, where we have more than two possible classes to choose from and need to place our data into one of them.</p>
<p>In the single outcome case the approach to assume the outcome is a Bernoulli distribution over outcome <span class="math inline">\(y\)</span> conditioned on the input <span class="math inline">\(x\)</span>. The resultant loss function is commonly referred to as ‘binary cross entropy.’<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>

<div class="definition">
<span id="def:crossentropy" class="definition"><strong>Definition 3.6  (Binary cross entropy loss)  </strong></span><span class="math display">\[L(\theta) = -\frac{1}{n}\sum_{i = 1}^n\left[ y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)\right]\]</span>
</div>

<p>This can be generalized into the multivariate case by swapping a multinomial model as the conditional distribution and thus adding more terms to the internal summation representing each possible class and it’s assigned probability.</p>

<div class="definition">
<p><span id="def:catcrossentropy" class="definition"><strong>Definition 3.7  (Categorical cross entropy loss)  </strong></span><span class="math display">\[L(\theta) = -\frac{1}{n}\sum_{i = 1}^n\sum_{j = 1}^k y_{i,j}\log(\hat{y}_{i,j})\]</span></p>
Where <span class="math inline">\(y_{i,j}\)</span> represents the <span class="math inline">\(i^{\text{th}}\)</span> observation’s value for the <span class="math inline">\(k^{\text{th}}\)</span> category (dummy encoded).
</div>

</div>
<div id="other-loss-functions" class="section level3">
<h3><span class="header-section-number">3.7.3</span> Other loss functions</h3>
<p>While it is possible to use other loss functions in neural networks it is commonly not advised as the maximum-likelihood methods perform just as well and usually results in a more robust fit to the data. As deep learning situations usually have a large number of training observations the Cramer-Rao lower bound property, that a maximum likelihood estimator has the lowest variance of any consistent estimator, is particularly relevant. For a more thorough overview of the common loss functions used in deep learning see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter six.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-goodfellow_DL">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-cnn_vis">
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div id="ref-mcculloch_neuron">
<p>McCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4). Springer: 115–33.</p>
</div>
<div id="ref-rosenblatt_perceptron">
<p>Rosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6). American Psychological Association: 386.</p>
</div>
<div id="ref-minsky_perceptrons">
<p>Minsky, Marvin, Seymour A Papert, and Léon Bottou. 1969. <em>Perceptrons: An Introduction to Computational Geometry</em>. MIT press.</p>
</div>
<div id="ref-backprop_1986">
<p>Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” <em>Nature</em> 323 (6088). Nature Publishing Group: 533–36.</p>
</div>
<div id="ref-imagenet_2012">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 1097–1105.</p>
</div>
<div id="ref-capsnet">
<p>Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton. 2017. “Dynamic Routing Between Capsules.” In <em>Advances in Neural Information Processing Systems</em>, 3857–67.</p>
</div>
<div id="ref-cnn_animals">
<p>Kheradpisheh, Saeed Reza, Masoud Ghodrati, Mohammad Ganjtabesh, and Timothée Masquelier. 2016. “Deep Networks Can Resemble Human Feed-Forward Vision in Invariant Object Recognition.” <em>Scientific Reports</em> 6. Nature Publishing Group: 32672.</p>
</div>
<div id="ref-universal_approximators">
<p>Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” <em>Neural Networks</em> 2 (5). Elsevier: 359–66.</p>
</div>
<div id="ref-evostrat">
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning.” <em>arXiv.org</em>, March. <a href="http://arxiv.org/abs/1703.03864v2" class="uri">http://arxiv.org/abs/1703.03864v2</a>.</p>
</div>
<div id="ref-momentum">
<p>Goh, Gabriel. 2017. “Why Momentum Really Works.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00006">10.23915/distill.00006</a>.</p>
</div>
<div id="ref-relu_paper">
<p>Nair, Vinod, and Geoffrey E Hinton. 2010. “Rectified Linear Units Improve Restricted Boltzmann Machines.” In <em>Proceedings of the 27th International Conference on Machine Learning (Icml-10)</em>, 807–14.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>It is interesting to note that while we typically only use the term ‘cross entropy’ for categorical outcomes, any time we are minimizing KL or maximizing the likelihood we are minimizing the cross entropy. So technically we could also call the mean squared error the ‘continuous cross entropy loss.’<a href="neuralnetworks.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="architectures.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-deep_learning.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
