<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="architectures.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Continuous Classification using Deep Neural Networks</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#continuous-classification"><i class="fa fa-check"></i><b>1.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>1.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#activity-prediction"><i class="fa fa-check"></i><b>1.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#ehr-monitoring"><i class="fa fa-check"></i><b>1.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#hospital-automation"><i class="fa fa-check"></i><b>1.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#history-of-methods"><i class="fa fa-check"></i><b>1.3</b> History of methods</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#windowed-regression"><i class="fa fa-check"></i><b>1.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#transformation-methods"><i class="fa fa-check"></i><b>1.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#hidden-markov-models"><i class="fa fa-check"></i><b>1.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>1.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>2</b> Neural Networks</a><ul>
<li class="chapter" data-level="2.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>2.1</b> History</a><ul>
<li class="chapter" data-level="2.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>2.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="2.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>2.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="2.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>2.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="2.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>2.4</b> Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>2.5</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>2.5.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="2.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propagation"><i class="fa fa-check"></i><b>2.5.2</b> Back Propagation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>2.6</b> Training</a><ul>
<li class="chapter" data-level="2.6.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>2.6.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.6.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>2.6.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>2.7</b> Activation Functions</a></li>
<li class="chapter" data-level="2.8" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>2.8</b> Loss functions</a><ul>
<li class="chapter" data-level="2.8.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#likelihoods"><i class="fa fa-check"></i><b>2.8.1</b> Likelihoods</a></li>
<li class="chapter" data-level="2.8.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>2.8.2</b> Regression loss functions</a></li>
<li class="chapter" data-level="2.8.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>2.8.3</b> Classification loss functions</a></li>
<li class="chapter" data-level="2.8.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>2.8.4</b> Other loss functions</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="neuralnetworks.html"><a href="neuralnetworks.html#avoiding-overfitting"><i class="fa fa-check"></i><b>2.9</b> Avoiding Overfitting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#parameter-penalization"><i class="fa fa-check"></i><b>2.9.1</b> Parameter penalization</a></li>
<li class="chapter" data-level="2.9.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#dropout"><i class="fa fa-check"></i><b>2.9.2</b> Dropout</a></li>
<li class="chapter" data-level="2.9.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#early-stopping"><i class="fa fa-check"></i><b>2.9.3</b> Early Stopping</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>3</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="3.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>3.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="3.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>3.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="3.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>3.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="3.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>3.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="3.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>3.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="3.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>3.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="3.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>3.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="3.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>3.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="3.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>3.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>3.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="3.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>3.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>4</b> Opportunities for advancing field</a><ul>
<li class="chapter" data-level="4.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>4.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="4.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>4.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="4.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware"><i class="fa fa-check"></i><b>4.1.2</b> Runtime on mobile hardware</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>4.2</b> Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>4.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="4.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>4.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="4.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>4.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>4.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>4.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="4.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>4.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neuralnetworks" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Neural Networks</h1>
<blockquote>
<p>A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler function. We can think of each application of a different mathematical function as providing a new representation of the input. (<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span>)</p>
</blockquote>
<p>Neural networks (sometimes referred to as multilayer perceptrons) are at their core very simple models. Traditional modern neural networks simply pass data forward through a “network” that at each layer, performs a linear (also referred to as affine) transformation of its inputs followed by a element-wise non-linear transformation (also called an activation function). In doing this they can build up successively more complex representations of data and use those to make decisions about it.</p>
<p>This can be thought about in the analogy of recognizing a cat. First you see ears, a nose, two eyes, four feet, and a fluffy tail; next, you recognize the ears, nose and eyes as a head, the tail and legs as a body; and lastly the head and body as a cat. In performing this ‘classification’ of a cat you first constructed small features and successively stacked them to figure out what you were looking at. While this is obviously a stretched definition of how neural networks work, it actually is very close to how a special variant called Convolutional Neural Networks work for computer vision techniques (<span class="citation">Olah, Mordvintsev, and Schubert (<a href="#ref-cnn_vis">2017</a>)</span>).</p>
<div id="history" class="section level2">
<h2><span class="header-section-number">2.1</span> History</h2>
<p>While neural networks’ popularity has taken off in recent years they are not a new technique. The neuron or smallest unit of a neural network was first introduced in 1943 (<span class="citation">McCulloch and Pitts (<a href="#ref-mcculloch_neuron">1943</a>)</span>). It was then another 15 years until the perceptron (now commonly called ‘neural network’) was introduced (<span class="citation">Rosenblatt (<a href="#ref-rosenblatt_perceptron">1958</a>)</span>) that tied together groups of neurons to represent more complex relationships.</p>
<p>Another ten years later, in a textbook (<span class="citation">Minsky, Papert, and Bottou (<a href="#ref-minsky_perceptrons">1969</a>)</span>) it was shown that a simple single layer perceptron was incapable of solving certain classes of problems like the “And Or” (XOR) problem (<a href="neuralnetworks.html#fig:xorproble">2.1</a>). This is due to the separating hyperplane found by a single layer perceptron being a linear combination of the input values. This forces the classes to themselves be linearly separable, which the XOR problem is not. The authors argued that the only way for a perceptron to overcome this hurdle would be to be stacked together, which, while appealing, was not possible to be trained effectively at the time.</p>
<div class="figure" style="text-align: center"><span id="fig:xorproble"></span>
<img src="figures/xor_problem.png" alt="Example of the XOR problem. Classes encoded by color are not linearly seperable." width="50%" />
<p class="caption">
Figure 2.1: Example of the XOR problem. Classes encoded by color are not linearly seperable.
</p>
</div>
<p>It wasn’t until 1986 that a realistic technique for training these multi-layer perceptrons was introduced (<span class="citation">Rumelhart, Hinton, and Williams (<a href="#ref-backprop_1986">1986</a>)</span>). Finally all of the algorithmic pieces were in place for deep neural networks, but interest stagnated due to the computational expense of training the networks, a lack of data, and the success of other competing machine learning algorithms.</p>
<p>Interest in the field of deep learning has had a massive resurgence in the second decade of the 21st century. Driven by growing stores of data and innovations in neural network architectures. One commonly cited tipping point for the current “deep learning revolution” was the 2012 paper (<span class="citation">Krizhevsky, Sutskever, and Hinton (<a href="#ref-imagenet_2012">2012</a>)</span>) in which a deep convolutional neural network won the ImageNet prize and showed massive improvements over traditional methods.</p>
<div id="biological-inspirations" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Biological Inspirations</h3>
<p>The word ‘neural’ in the ‘neural network’ is reference to the fact that these models derive inspiration from how the brain works. With the individual nodes in a hidden ‘layer’ frequently being called a ‘neuron’. While the broad concepts may be similar between the way animal brains and neural networks work, the similarities are minimal. With animal brains relying not only on the connections and firing of the neurons but also the chemical concentrations and interactions of the two not yet fully understood.</p>
<p>There has however, been some more recent work on trying to more closely mimic the brain structure with architectures such as capsule networks (<span class="citation">Sabour, Frosst, and Hinton (<a href="#ref-capsnet">2017</a>)</span>). These networks introduce another level of architectural hierarchy to the neural network system: the capsule which is a collection of neurons within a given layer. These capsules attempt to recognize the existence of features of the input data, while also modeling the feature’s transformation and position. This deals with the problem of traditional neural networks failing to recognize when input features are transformed (e.g. an upside down mug is not recognized by the same feature detectors as a right side up mug). Neuroscience experiments have demonstrated that at least part of our visual system does truly perform these hierarchical stacks of features when recognizing objects (<span class="citation">Kheradpisheh et al. (<a href="#ref-cnn_animals">2016</a>)</span>).</p>
</div>
<div id="geometric-interpretation" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Geometric Interpretation</h3>
<p>Another way of thinking of how neural networks work is as a building up a series of successive transformations of the data-space that attempt to eventually let the data be linearly separable ((<a href="neuralnetworks.html#fig:geometricinterp">2.2</a>). In this interpretation each layer can be seen as a shift and rotation of the data (the linear transformation), followed by a warping of the new space (the activation function). In his excellent blog post: <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural Networks, Manifolds, and Topology</a>, Chris Olah gives an excellent visual demonstration of this.</p>
<div class="figure" style="text-align: center"><span id="fig:geometricinterp"></span>
<img src="figures/warp_space.png" alt="Example of how a neural network can, through a series of affine transformations followed by non-linear squashings, turn a linearly inseperable proble into a linearly seperable one. Image courtesy of [Chris Olah's blog](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"  />
<p class="caption">
Figure 2.2: Example of how a neural network can, through a series of affine transformations followed by non-linear squashings, turn a linearly inseperable proble into a linearly seperable one. Image courtesy of <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Chris Olah’s blog</a>
</p>
</div>
</div>
</div>
<div id="universal-approximation-theorem" class="section level2">
<h2><span class="header-section-number">2.2</span> Universal Approximation Theorem</h2>
<p>One powerful theoretical result from neural networks is that they are universal approximators (<span class="citation">Hornik, Stinchcombe, and White (<a href="#ref-universal_approximators">1989</a>)</span>). A neural network with a single hidden layer and non-linear activations functions on that layer can represent <em>any</em> borel-measurable function. This result means that there are no theoretical limits on the capabilities of neural networks. Obviously, in real-world situations this is not the case. We can not have infinite width hidden layers, infinite parameters requires infinite data, and even more limiting are our inefficient learning methods. All constraints considered though, the universal approximation theorem does provide confidence that, as long as they are properly constructed and trained, neural networks are amazingly flexible models.</p>
</div>
<div id="the-computation-graph" class="section level2">
<h2><span class="header-section-number">2.3</span> The Computation Graph</h2>
<p>While the building blocks of neural networks are simple, often complete models are composed of hundreds to even thousands of neurons and millions of connections and representing them in mathematical notation becomes exceedingly difficult. A method of dealing with this complexity, along with also helping in the intuition of many other properties, is to represent the networks as a ‘computation graph.’</p>
<p>A computation graph is simply directed acyclic diagram that shows the flow of data through the model. Each neuron is usually represented as a circle with the weights both in and out of the neuron’s value represented as edges.</p>
<div class="figure" style="text-align: center"><span id="fig:compgraph"></span>
<img src="figures/computation_graph.png" alt="A neural network computation graph at two levels of detail: the neuron left (left) and the layer level (right)."  />
<p class="caption">
Figure 2.3: A neural network computation graph at two levels of detail: the neuron left (left) and the layer level (right).
</p>
</div>
<p>Sometimes, when the models get even larger, the layers (or groups of neurons) will get lumped into a single node in the graph (as on the right of figure <a href="neuralnetworks.html#fig:compgraph">2.3</a>.)</p>
</div>
<div id="terminology" class="section level2">
<h2><span class="header-section-number">2.4</span> Terminology</h2>
<p>When covering the basic mathematical operations of a neural network it helps to have a reference for some of the terms that get used. This list provides the most commonly used terms for the models we will be describing.</p>
<p><strong>Neuron</strong>: An individual node in the network. Has two values: activation, or the value of the linear function of all inputs, and the post-activation-function value, or a simple transformation of the activation by the activation function.</p>
<p><strong>Affine Transformation</strong>: A linear transformation of an input (either data input or a hidden layer’s output). Essentially a linear regression.</p>
<p><strong>Bias Term</strong>: A constant term added to the affine transformation for a given neuron. Also known as an ‘intercept term.’. For notational simplicity in most of our formulas we will omit this.</p>
<p><strong>Activation Function</strong>: A non-linear function that takes an input and ‘squashes’ it to some range. A common activation function is the sigmoid, which takes an unbounded real-valued input and returns a value between -1 and 1.</p>
<p><strong>Layer</strong>: A collection of neurons who’s inputs typically share the same inputs (either another layer’s output or the data).</p>
<p><strong>Hidden Layer</strong>: A layer who’s input is the output of a previous layer and who’s output is another layer. E.g. Input layer -&gt; hidden layer -&gt; output layer.</p>
</div>
<div id="mathematical-operations" class="section level2">
<h2><span class="header-section-number">2.5</span> Mathematical Operations</h2>
<p>The basic operations that one does on a neural network really fall into two categories. Forward propagation, or the passing of data into and through subsequent layers of the model to arrive at an output, and back-propagation, or the calculation of the gradient of each parameter in the model by stepping back through the model from the loss function to the input. For a more thorough treatment of these steps see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter six.</p>
<div id="forward-propagation" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Forward Propagation</h3>
<p>Let a neural network with <span class="math inline">\(l\)</span> layers and <span class="math inline">\(k\)</span> dimensional input <span class="math inline">\(X\)</span> and <span class="math inline">\(m\)</span> dimensional output <span class="math inline">\(\hat{y}\)</span> attempting to predict the true target <span class="math inline">\(y\)</span>. Each layer is composed of <span class="math inline">\(s_i, i \in \{1, 2, ...,l\}\)</span> neurons and has respective non-linear activation function <span class="math inline">\(f_i\)</span>. The output of a layer after affine transformation is represented as a vector of length <span class="math inline">\(s_i\)</span>: <span class="math inline">\(\underline{a_i}\)</span> and the layer output vector post-activation function outputs: <span class="math inline">\(\underline{o_i}\)</span>. The weights representing the affine transition from one layer <span class="math inline">\(i\)</span> to layer <span class="math inline">\(j\)</span> are a matrix <span class="math inline">\(W_i\)</span> of of size <span class="math inline">\(s_{i} \times s_j\)</span>. Finally the network has a differentiable with respect to <span class="math inline">\(\hat{y}\)</span> loss function: <span class="math inline">\(L(\hat{y}, y)\)</span>. In the case of categorical outcomes <span class="math inline">\(\hat{y}\)</span> corresponds to the probability of the class of interest. This is some times referred to as <span class="math inline">\(\hat{p}\)</span>, or <span class="math inline">\(\hat{p}_i\)</span> in the multi-class situation, where <span class="math inline">\(i\)</span> corresponds to one of <span class="math inline">\(k\)</span> output classes. For simplicity of notation we will use <span class="math inline">\(\hat{y}\)</span> to represent both the continuous and categorical cases here.</p>
<p>Forward propagation then proceeds as follows.</p>
<ol style="list-style-type: decimal">
<li>Input <span class="math inline">\(X\)</span> <span class="math inline">\((1 \times k\)</span>) is multiplied by <span class="math inline">\(W_1\)</span> (size <span class="math inline">\((k\times s_i)\)</span> ) to achieve the <em>activation values</em> of the first hidden layer.
<ul>
<li><span class="math inline">\(X \cdot W_1 = \underline{a_1}\)</span></li>
</ul></li>
<li>The <span class="math inline">\((1 \times s_1)\)</span> activation vector of the first layer is then run element-wise though the first layer’s non-linear activation function to achieve the output of layer 1.
<ul>
<li><span class="math inline">\(\underline{o_1} = f_1(\underline{a_1})\)</span></li>
</ul></li>
<li>This series of operations is then repeated through all the layers, (using the subsequent layers output vector as the input to the next layer,) until the final layer is reached.
<ul>
<li><span class="math inline">\(\underline{o_i} = f_i(\underline{o_{(i - 1)}} \cdot W_i)\)</span></li>
</ul></li>
<li>Finally, the loss is calculated from the output of our final layer.
<ul>
<li><span class="math inline">\(L_n = L(\underline{o_l}, y) = L(\hat{y}, y)\)</span></li>
</ul></li>
</ol>
<p>While not strictly necessary for forward propagation, the intermediate layer activations and output vectors are kept stored so they can be used in the later calculation of the gradient via back propagation.</p>
</div>
<div id="back-propagation" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Back Propagation</h3>
<p>If we are just looking to gather predictions from our model we can stop at forward propagation. However, most likely we want to train our model first. The most common technique for training neural networks is using a technique called back propagation (<span class="citation">Rumelhart, Hinton, and Williams (<a href="#ref-backprop_1986">1986</a>)</span>). In this algorithm the chain rule is used to walk back through the layers of the model starting from the loss function to the input weights in order to calculate each weight’s gradient with respect to the average loss over training samples (or expected loss)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. This gradient is then descended using any number of gradient descent algorithms.</p>
<div id="the-chain-rule" class="section level4">
<h4><span class="header-section-number">2.5.2.1</span> The Chain Rule</h4>
<p>Back propagation is nothing more than a repeated application of the chain rule from calculus. Let <span class="math inline">\(x\)</span> be a single dimensional real valued input that is mapped through first equation <span class="math inline">\(f\)</span> and then <span class="math inline">\(g\)</span>, both of which map from a single dimensional real number to another single dimensional real number: <span class="math inline">\(f(x) = z, g(z) = y\)</span> or <span class="math inline">\(g(f(x)) = y\)</span>. The chain rule states that we can calculate the derivative of the outcome with respect to the input by a series of multiplications of the derivatives of the composing functions.</p>
<span class="math display" id="eq:chainrule">\[\begin{equation} 
  \frac{dy}{dx} = \frac{dy}{dz}\frac{dz}{dx}
  \tag{2.1}
\end{equation}\]</span>
<p>This single dimensional example could be thought of as a neural network composed of two layers, each with a single dimension. The single dimensional case is illuminating, but the value of the chain rule comes when it is applied to higher dimensional values.</p>
</div>
<div id="expanding-to-higher-dimensions" class="section level4">
<h4><span class="header-section-number">2.5.2.2</span> Expanding to higher dimensions</h4>
<p>Now, let <span class="math inline">\(\mathbb{x} \in \mathbb{R}^m\)</span> and <span class="math inline">\(\mathbb{z} \in \mathbb{R}^n\)</span>. The function <span class="math inline">\(f\)</span> maps from <span class="math inline">\(\mathbb{R}^m \to \mathbb{R}^n\)</span> and <span class="math inline">\(g: \mathbb{R}^n \to \mathbb{R}\)</span>. Further, let <span class="math inline">\(\textbf{z} = f(\textbf{x})\)</span> and <span class="math inline">\(y = g(\textbf{z})\)</span>. The chain rule can then be expressed as:</p>
<span class="math display" id="eq:chainrulemv">\[\begin{equation} 
  \frac{dy}{dx_i} = \sum_{j}\frac{dy}{dz_j}\frac{dz_j}{dx_i}
  \tag{2.2}
\end{equation}\]</span>
<p>Or that the derivative of <span class="math inline">\(y\)</span> with respect to the <span class="math inline">\(i^{\text{th}}\)</span> element of <span class="math inline">\(\textbf{x}\)</span> is the sum of the series of products of the derivatives of result <span class="math inline">\(\textbf{z}\)</span> that sits between the two values in the function composition.</p>
<p>Another way of thinking of this is, the derivative of the output of the function composition <span class="math inline">\(f \circ g\)</span> with respect to some element of the input is the sum of all of the derivatives of all of the paths leading from the input element to the output.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="figures/backprop.png" alt="How back Propagation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph (blue connections)."  />
<p class="caption">
Figure 2.4: How back Propagation steps back from the output to the input. To calculate the gradient with respect to the loss of the orange neuron the we need to aggregate the gradients of all connected points further along in the computation graph (blue connections).
</p>
</div>
</div>
<div id="applied-to-neural-networks" class="section level4">
<h4><span class="header-section-number">2.5.2.3</span> Applied to Neural Networks</h4>
<p>To apply this technique to neural networks we need to make sure all components of our network are differentiable and then walk back from the loss function to the last (or output) layer, calculating the gradients of the output layer’s neurons with respect to the loss. Once we have calculated the gradients with respect to the weights of the output layer we only need use those calculated gradients to calculate the gradients of the preceding layer. We can then proceed layer by layer, walking back through the model filling out each neuron’s weight gradients until we reach the input.</p>
<p>To calculate the gradient of the weights for hidden layer <span class="math inline">\(i\)</span> we can recall that the hidden layers output can be represented as <span class="math inline">\(\textbf{a}_i = f_i(\textbf{W}\cdot\textbf{a}_{(i -1)})\)</span> (we’re omitting the bias term here for simplicity). Thus to calculate the gradient’s on the weights we can set <span class="math inline">\(\textbf{g}^*_i = \textbf{g}_{i+1} \odot f&#39;(a_{i+1})\)</span> to be the gradient un-activated by our layer’s activation function’s derivative. Then to find the derivative with respect to each neuron’s weights within the layer we multiply this un-activated gradient by the transpose of the layer’s output vector: <span class="math inline">\(\textbf{g}_i = \textbf{g}^*_i \textbf{o}_i^t\)</span>.</p>
<p>The fact that the calculation of these gradients is so simple is fundamental to deep learning. By ensuring that the flow of error is acyclic and all transforming functions are divisible, a deep neural network can be trained with extremely simple mathematical operations. If it were more complicated, extremely large networks (such as those used in computer vision) with millions of parameters to tune would simply be computationally infeasible to calculate gradients for. Conveniently, the run time of the back propagation is a simple product of the number of neurons in each layer and the number of layers in the whole model.</p>
</div>
<div id="other-methods" class="section level4">
<h4><span class="header-section-number">2.5.2.4</span> Other methods</h4>
<p>Non-gradient based techniques have been explored for neural networks but have ultimately proven to slow for any realistic sized network. For instance evolutionary strategies (<span class="citation">Salimans et al. (<a href="#ref-evostrat">2017</a>)</span>), a new technique proposed by a group at OpenAi uses random searches of the parameter space with evolutionary algorithms to tune the model’s parameters. However, since often these parameter spaces are millions of dimensions, an extremely large number of random perturbations are needed to inform a good direction to move.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
</div>
</div>
</div>
<div id="training" class="section level2">
<h2><span class="header-section-number">2.6</span> Training</h2>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Gradient Descent</h3>
<p>Once the gradient is computed, optimization proceeds the same way any gradient-based optimization problem does. The simplest algorithm for doing so being the steepest descent algorithm. In this algorithm, each weight for our network is updated by adding its gradient multiplied by a small scalar called a ‘learning rate’ (<span class="math inline">\(\alpha\)</span>).</p>
<p><span class="math display">\[\theta^{(j)} = \theta^{(j - 1)} + \alpha\Delta_{\theta}\]</span></p>
<p>The new updated weights are then used in another forward propagation, followed by another back propagation and weight update. This processes is repeated until the loss finds a minimum or some other stopping criteria is defined (such as a given number steps) is satisfied.</p>
</div>
<div id="gradient-descent-modifications" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Gradient Descent Modifications</h3>
<p>There are many scenarios in which traditional steepest descent is not an ideal for finding minimums of the loss function. A classic example is when the loss function takes the form of a long trough (<a href="neuralnetworks.html#fig:badloss">2.5</a>. In this case steepest descent will spend most of its time bouncing around between the sides of the trough due to over stepping the low point, thus wasting many of its iterations undoing its previous work rather than progressing in the true direction of the minimum.</p>
<div class="figure" style="text-align: center"><span id="fig:badloss"></span>
<img src="figures/needs_momentum.png" alt="Demonstration of steepest descent's limitations on trough like gradient surfaces. Much of each step's progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from @momentum" width="100%" />
<p class="caption">
Figure 2.5: Demonstration of steepest descent’s limitations on trough like gradient surfaces. Much of each step’s progress is wasted on bouncing back and forth between the two walls of the gradient rather than descending directly towards the minimum. Image from <span class="citation">Goh (<a href="#ref-momentum">2017</a>)</span>
</p>
</div>
<p>One of the most common additions to plain gradient descent is the addition of momentum. Momentum makes each step a function of not only the current gradient but also a decaying average of previous generations. For a thorough overview of momentum based methods see <span class="citation">Goh (<a href="#ref-momentum">2017</a>)</span>.</p>
</div>
</div>
<div id="activation-functions" class="section level2">
<h2><span class="header-section-number">2.7</span> Activation Functions</h2>
<p>There are many different possible activation functions. The only conditions that an activation function needs to satisfy is that it takes an unbounded real input and returns some non-linear transformation of that input that is differentiable with respect to the input.</p>
<p>One of the simplest functions used is the sigmoid which simply squashes the neuron’s activation from all real numbers to between 0 and 1. A newer and extremely common choice is the rectified linear unit (Relu) function (<span class="citation">Nair and Hinton (<a href="#ref-relu_paper">2010</a>)</span>).</p>

<div class="definition">
<span id="def:sigmoid" class="definition"><strong>Definition 2.1  (sigmoid activation function)  </strong></span><span class="math display">\[f(x) = e(x)/(1 + e(x))\]</span>
</div>


<div class="definition">
<span id="def:relu" class="definition"><strong>Definition 2.2  (Rectified Linear Unit activation function)  </strong></span><span class="math display">\[f(x) = \text{max}(0,x)\]</span>
</div>

<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="strayer_qualifying_exam_files/figure-html/unnamed-chunk-2-1.png" alt="Comparison of the relu and sigmoid activation functions." width="85%" />
<p class="caption">
Figure 2.6: Comparison of the relu and sigmoid activation functions.
</p>
</div>
<p>Another, popular loss function that is used on the output layer when predicting categories is the softmax function. Unlike the previously mentioned loss functions this one takes a vector <span class="math inline">\(\textbf{o}\)</span> of length <span class="math inline">\(m\)</span> representing the output of each of the layer’s neurons.</p>

<div class="definition">
<span id="def:softmax" class="definition"><strong>Definition 2.3  (Softmax activation function.)  </strong></span><span class="math display">\[f(\textbf{o})_i = \frac{\text{exp}(o_i)}{\sum_{j = 1}^m \text{exp}(o_j)}\]</span>
</div>

<p>This function serves to turn the output of a layer into a multivariate probability vector. The <span class="math inline">\(m\)</span> outputs of the softmax sum to one.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
</div>
<div id="loss-functions" class="section level2">
<h2><span class="header-section-number">2.8</span> Loss functions</h2>
<p>Like any machine learning model, neural networks have a loss function (sometimes called a cost function), or a function that helps the model know how close it is to performing as desired. As previously mentioned, in the case of neural networks we almost always want our loss function to be differentiable with respect to our model’s output <span class="math inline">\(\hat{\textbf{y}}\)</span> as the derivative is needed for calculating the gradient on which the model is trained.</p>
<div id="likelihoods" class="section level3">
<h3><span class="header-section-number">2.8.1</span> Likelihoods</h3>
<p>Almost always the loss function is derived from a likelihood function. A likelihood function is chosen based upon a parametric model that is assumed to represent the distribution of outcomes for a model with parameters (weights) <span class="math inline">\(\mathbb{\theta}\)</span>, after conditioning on the input data.</p>

<div class="definition">
<span id="def:likelihood" class="definition"><strong>Definition 2.4  (Likelihood function)  </strong></span><span class="math display">\[P(\mathbb{\theta}, \mathbb{y}|\mathbb{x}) = P_{\text{model}}(\mathbb{y} | \mathbb{x})\]</span>
</div>

<p>For example, with a continuous outcome model it is commonly assumed that the outcome <span class="math inline">\(y\)</span> corresponding to input <span class="math inline">\(\mathbb{x}\)</span> will be normally distributed. Due to this assumption, the loss function is derived from the likelihood equation of the normal model.</p>
<p>Optimization then becomes a task of manipulating the parameters of the network <span class="math inline">\(\mathbb{\theta}\)</span> such that the predicted conditional distribution of the output is as close as possible to the observed conditional distribution. The ‘distance’ from the conditional distribution is a quantity referred to as the Kullback-Leibler divergence.</p>

<div class="definition">
<span id="def:kld" class="definition"><strong>Definition 2.5  (Expected Kulback-Leibler Divergence)  </strong></span><span class="math display">\[\text{KLD(model, observed)} = \sum_{i \in \text{observations}}P_{\text{observed}}(\mathbb{y_i} | \mathbb{x_i}) \cdot \log{\frac{P_{\text{observed}}(\mathbb{y_i} | \mathbb{x_i})}{P_{\text{model}}(\mathbb{y_i} | \mathbb{x_i})}}\]</span>
</div>


<div class="definition">
<span id="def:negloglik" class="definition"><strong>Definition 2.6  (General maximum likelihood loss function)  </strong></span><span class="math display">\[J(\theta) = - \mathbb{E}_{x,y \sim \hat{p} \text{ data}} \log{{p_{\text{model}}(\textbf{y} |\textbf{x} })}\]</span>
</div>


<div class="definition">
<span id="def:estnegloglik" class="definition"><strong>Definition 2.7  (Estimated maximum likelihood loss function)  </strong></span><span class="math display">\[\hat{J}(\theta) = {- \sum}_{i \in \text{observations}} \log{{p_{\text{model}}(\textbf{y} |\textbf{x} })}\]</span>
</div>

<p>The precise form of the maximum likelihood loss function will change depending on the model. Next we will briefly introduce the two classes of loss functions: regression and classification, and the most commonly used functions within those classes.</p>
</div>
<div id="regression-loss-functions" class="section level3">
<h3><span class="header-section-number">2.8.2</span> Regression loss functions</h3>
<p>When fitting a neural network who’s outcome is a single real numbered variable <span class="math inline">\(\hat{y}\)</span> the most common loss function used is the mean square error.</p>

<div class="definition">
<p><span id="def:mse" class="definition"><strong>Definition 2.8  (Expected mean squared error loss)  </strong></span><span class="math display">\[\text{MSE}(\theta) = -\frac{1}{n} \sum_{i = 1}^{n} (\hat{y}^{(i)} - y^{(i)})^2\]</span></p>
Where <span class="math inline">\(y_i\)</span> represents the outcome of the <span class="math inline">\(i^{\text{th}}\)</span> observation of <span class="math inline">\(n\)</span> training observations.
</div>

<p>This assumes that the output, when conditioned on the input, takes the form of a normal distribution. Minimizing the mean squared error is equivalent to maximizing the conditional normal likelihood function (<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter 5.5) and is thus a maximum likelihood method.</p>
</div>
<div id="classification-loss-functions" class="section level3">
<h3><span class="header-section-number">2.8.3</span> Classification loss functions</h3>
<p>There are two separate instances of classification that need to be considered. The binary case, where we are classifying a yes or no answer for a single class, or a categorical case, where we have more than two possible classes to choose from and need to place our data into one of them.</p>
<p>In the single outcome case the approach is to assume the outcome is a Bernoulli distribution over outcome <span class="math inline">\(y\)</span> conditioned on the input <span class="math inline">\(x\)</span>. The resultant loss function is commonly referred to as ‘binary cross entropy.’<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>

<div class="definition">
<span id="def:crossentropy" class="definition"><strong>Definition 2.9  (Expected binary cross entropy loss)  </strong></span><span class="math display">\[L(\theta) = -\frac{1}{n}\sum_{i = 1}^n\left[ y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)\right]\]</span>
</div>

<p>This can be generalized into the multivariate case by swapping a multinomial model as the conditional distribution and thus adding more terms to the internal summation representing each possible class and its assigned probability.</p>

<div class="definition">
<p><span id="def:catcrossentropy" class="definition"><strong>Definition 2.10  (Expected categorical cross entropy loss)  </strong></span><span class="math display">\[L(\theta) = -\frac{1}{n}\sum_{i = 1}^n\sum_{j = 1}^k y_{i,j}\log(\hat{y}_{i,j})\]</span></p>
Where <span class="math inline">\(y_{i,j}\)</span> represents the <span class="math inline">\(i^{\text{th}}\)</span> observation’s value for the <span class="math inline">\(k^{\text{th}}\)</span> category (dummy encoded).
</div>

</div>
<div id="other-loss-functions" class="section level3">
<h3><span class="header-section-number">2.8.4</span> Other loss functions</h3>
<p>While it is possible to use other loss functions in neural networks, it is commonly not advised as the maximum-likelihood methods perform just as well and usually results in a more robust fit to the data. As deep learning situations usually have a large number of training observations the Cramer-Rao lower bound property, that a maximum likelihood estimator has the lowest variance of any consistent estimator, is particularly relevant. For a more thorough overview of the common loss functions used in deep learning see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow_DL">2016</a>)</span> chapter six.</p>
</div>
</div>
<div id="avoiding-overfitting" class="section level2">
<h2><span class="header-section-number">2.9</span> Avoiding Overfitting</h2>
<p>Neural networks have a large number of parameters or weights that need to be tuned in order to minimize their loss. Whenever there are this many knobs to tune a large concern is overfitting to the training data, leaving the model unable to generalize.</p>
<p>Neural networks have many ways of dealing with overfitting, some common to almost all machine learning methods, and some specific to neural networks.</p>
<div id="parameter-penalization" class="section level3">
<h3><span class="header-section-number">2.9.1</span> Parameter penalization</h3>
<p>Perhaps the most common way of dealing with overfitting is by penalizing the parameters. In this case the model’s loss function adds a term that ‘penalizes’ large parameters. By doing this the model tries to find a balance between fitting the data well and also keeping its weights small. There are two primary penalization methods, <span class="math inline">\(L1\)</span> penalization, where the sum of the absolute value of the parameters is added to the loss, and <span class="math inline">\(L2\)</span> penalization, where the sum of the squares of the weights are added to the loss.</p>

<div class="definition">
<span id="def:l1penalization" class="definition"><strong>Definition 2.11  (L1 penalization)  </strong></span>For a model with a <span class="math inline">\(k\)</span>-dimensional parameter space and a penalization amount, <span class="math inline">\(\lambda\)</span>: <span class="math display">\[L_{L1}(\theta) = \hat{L}(\theta) + \lambda \sum_{j = 1}^{k}|\theta_j|\]</span>
</div>


<div class="definition">
<span id="def:l2penalization" class="definition"><strong>Definition 2.12  (L2 penalization)  </strong></span><span class="math display">\[L_{L2}(\theta) = \hat{L}(\theta) + \lambda \sum_{j = 1}^{k}\theta_j^2\]</span>
</div>

<p><span class="math inline">\(L1\)</span> penalization tends to shrink parameter weights to zero, where as <span class="math inline">\(L2\)</span> shrinks them asymptotically to zero.</p>
<p>The parameter of <span class="math inline">\(\lambda\)</span> corresponds to the severity of penalization for large weights. It is often tuned by comparing the training loss of the model with the validation or test loss and attempting to find a value of <span class="math inline">\(\lambda\)</span> that shrinks the losses to each other (but not over as to avoid under-fitting).</p>
</div>
<div id="dropout" class="section level3">
<h3><span class="header-section-number">2.9.2</span> Dropout</h3>
<p>Another method of avoiding overfitting that is unique to neural networks is the technique known as dropout (<span class="citation">Srivastava et al. (<a href="#ref-dropout">2014</a>)</span>). While training, the activation amounts of given neurons are randomly set to zero (or dropped) with probability <span class="math inline">\(P_{\text{drop}}\)</span>. This prevents the model from overfitting the data by never allowing it to tune weights such that they simply memorize the training data, as if it does its predictions will be very poor when random neurons are dropped. When the model is then used in a testing scenario no neurons are dropped but the weights are scaled back by <span class="math inline">\(1 - P_{\text{drop}}\)</span> to account for the fact that more neurons are contributing to the answer than were when the model was trained.</p>
<p>Dropout has been shown to be a very effective, if counter-intuitive, method for avoiding overfitting and is used extremely commonly.</p>
</div>
<div id="early-stopping" class="section level3">
<h3><span class="header-section-number">2.9.3</span> Early Stopping</h3>
<p>Another common method of avoiding overfitting is stopping the gradient-descent processes early. A rule is used to decide when the optimization process is terminated: commonly when the validation loss does not decrease for a given number of steps.</p>
<p>Early stopping is often combined with a technique known as gradient-clipping, whereby the steps of the gradient descent are not allowed to exceed a given size. This serves as a regulation method because it sets a bounds on how far the parameters can move: (# steps) <span class="math inline">\(\cdot\)</span> (max step size).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-goodfellow_DL">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-cnn_vis">
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div id="ref-mcculloch_neuron">
<p>McCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4). Springer: 115–33.</p>
</div>
<div id="ref-rosenblatt_perceptron">
<p>Rosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6). American Psychological Association: 386.</p>
</div>
<div id="ref-minsky_perceptrons">
<p>Minsky, Marvin, Seymour A Papert, and Léon Bottou. 1969. <em>Perceptrons: An Introduction to Computational Geometry</em>. MIT press.</p>
</div>
<div id="ref-backprop_1986">
<p>Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” <em>Nature</em> 323 (6088). Nature Publishing Group: 533–36.</p>
</div>
<div id="ref-imagenet_2012">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 1097–1105.</p>
</div>
<div id="ref-capsnet">
<p>Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton. 2017. “Dynamic Routing Between Capsules.” In <em>Advances in Neural Information Processing Systems</em>, 3857–67.</p>
</div>
<div id="ref-cnn_animals">
<p>Kheradpisheh, Saeed Reza, Masoud Ghodrati, Mohammad Ganjtabesh, and Timothée Masquelier. 2016. “Deep Networks Can Resemble Human Feed-Forward Vision in Invariant Object Recognition.” <em>Scientific Reports</em> 6. Nature Publishing Group: 32672.</p>
</div>
<div id="ref-universal_approximators">
<p>Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” <em>Neural Networks</em> 2 (5). Elsevier: 359–66.</p>
</div>
<div id="ref-evostrat">
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning.” <em>arXiv.org</em>, March. <a href="http://arxiv.org/abs/1703.03864v2" class="uri">http://arxiv.org/abs/1703.03864v2</a>.</p>
</div>
<div id="ref-momentum">
<p>Goh, Gabriel. 2017. “Why Momentum Really Works.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00006">10.23915/distill.00006</a>.</p>
</div>
<div id="ref-relu_paper">
<p>Nair, Vinod, and Geoffrey E Hinton. 2010. “Rectified Linear Units Improve Restricted Boltzmann Machines.” In <em>Proceedings of the 27th International Conference on Machine Learning (Icml-10)</em>, 807–14.</p>
</div>
<div id="ref-dropout">
<p>Srivastava, Nitish, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Journal of Machine Learning Research</em> 15 (1): 1929–58.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Commonly a modification of gradient descent called stochastic gradient descent is used, which relies on using a subset of the data to calculate the expected lost at each step for efficiency.<a href="neuralnetworks.html#fnref1">↩</a></p></li>
<li id="fn2"><p>These do find use when methods are used when the loss function does not have a gradient. Such as reinforcement learning scenarios.<a href="neuralnetworks.html#fnref2">↩</a></p></li>
<li id="fn3"><p>It is important to note that, while the values sum to one, it’s not a true probability output, but will help indicate which class is most likely and roughly by what magnitude over other potential class choices.<a href="neuralnetworks.html#fnref3">↩</a></p></li>
<li id="fn4"><p>It is interesting to note that while we typically only use the term ‘cross entropy’ for categorical outcomes, any time we are minimizing KL or maximizing the likelihood we are minimizing the cross entropy. So technically we could also call the mean squared error the ‘continuous cross entropy loss.’<a href="neuralnetworks.html#fnref4">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="architectures.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-deep_learning.Rmd",
"text": "Edit"
},
"download": ["strayer_qualifying_exam.pdf", "strayer_qualifying_exam.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
