<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="architectures.html">
<link rel="next" href="placeholder.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Report Layout</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#continuous-classification"><i class="fa fa-check"></i><b>2.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>2.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#activity-prediction"><i class="fa fa-check"></i><b>2.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#ehr-monitoring"><i class="fa fa-check"></i><b>2.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#hospital-automation"><i class="fa fa-check"></i><b>2.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#history-of-methods"><i class="fa fa-check"></i><b>2.3</b> History of methods</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#windowed-regression"><i class="fa fa-check"></i><b>2.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#transformation-methods"><i class="fa fa-check"></i><b>2.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="2.3.3" data-path="intro.html"><a href="intro.html#hidden-markov-models"><i class="fa fa-check"></i><b>2.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="2.3.4" data-path="intro.html"><a href="intro.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>2.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>3</b> Neural Networks</a><ul>
<li class="chapter" data-level="3.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>3.1</b> History</a><ul>
<li class="chapter" data-level="3.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>3.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="3.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>3.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="3.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>3.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="3.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>3.4</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="3.4.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>3.4.1</b> Terminology</a></li>
<li class="chapter" data-level="3.4.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>3.4.2</b> Forward Propagation</a></li>
<li class="chapter" data-level="3.4.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propigation"><i class="fa fa-check"></i><b>3.4.3</b> Back Propigation</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>3.5</b> Training</a><ul>
<li class="chapter" data-level="3.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>3.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="3.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>3.5.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>3.6</b> Activation Functions</a></li>
<li class="chapter" data-level="3.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>3.7</b> Loss functions</a><ul>
<li class="chapter" data-level="3.7.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>3.7.1</b> Regression loss functions</a></li>
<li class="chapter" data-level="3.7.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>3.7.2</b> Classification loss functions</a></li>
<li class="chapter" data-level="3.7.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>3.7.3</b> Other loss functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>4</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>4.1</b> Terminology</a></li>
<li class="chapter" data-level="4.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>4.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="4.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>4.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="4.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>4.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="4.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>4.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="4.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>4.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="4.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>4.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="4.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>4.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="4.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>4.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>4.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="4.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>4.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="4.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>4.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>4.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="4.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>4.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>5</b> Oportunities for advancing field</a><ul>
<li class="chapter" data-level="5.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>5.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="5.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>5.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="5.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware-hardware"><i class="fa fa-check"></i><b>5.1.2</b> Runtime on mobile hardware hardware</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>5.2</b> Inference</a><ul>
<li class="chapter" data-level="5.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>5.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="5.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>5.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="5.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>5.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>5.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>5.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="5.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>5.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="placeholder.html"><a href="placeholder.html"><i class="fa fa-check"></i><b>6</b> Placeholder</a><ul>
<li class="chapter" data-level="6.0.1" data-path="placeholder.html"><a href="placeholder.html#mathematical-operations-1"><i class="fa fa-check"></i><b>6.0.1</b> Mathematical Operations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="placeholder-1.html"><a href="placeholder-1.html"><i class="fa fa-check"></i><b>7</b> Placeholder</a><ul>
<li class="chapter" data-level="7.0.1" data-path="placeholder-1.html"><a href="placeholder-1.html#mathematical-operations-2"><i class="fa fa-check"></i><b>7.0.1</b> Mathematical Operations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="future" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Oportunities for advancing field</h1>
<p>While up to now has been an overview of the techniques that have been successfully and commonly implemented in sequential classification problems, this chapter will be devoted to new efforts to solving problems associated with the aforementioned methods. In addition to being a survey of the current state of the art it will also identify potential avenues for new research that could enhance our ability to work with sequential data subjected to various constraints.</p>
<div id="what-to-do-with-all-those-parameters" class="section level2">
<h2><span class="header-section-number">5.1</span> What to do with all those parameters</h2>
<p>A large issue with not just sequence-based deep learning, but the field as a whole is how large models are. For instance: VGGnet (<span class="citation">Simonyan and Zisserman (<a href="#ref-vggnet">2014</a>)</span>), the second place winner of the 2014 ImageNet competition and an extremely common model to use for image recognition tasks has 138 million parameters to tune. Going by the rule of thumb from Frank Harrell book “Regression Modeling Strategies”(<span class="citation">Harrell Jr (<a href="#ref-rms">2015</a>)</span>) of 10-20 observations per parameter in our model this is an issue, especially given the size of the data-set that the VGGnet model was trained on was <em>only</em> one million images.</p>
<p>Even simple models have large amounts of parameters to tune. A model with just a single hidden layer taking a ten dimensional input and a hidden layer of ten neurons performing binary classification would have (with bias parameters included) <span class="math inline">\((10*11) + (10*11) + (2*11) = 242\)</span> parameters to tune.</p>
<div id="theory-backed-methods-for-choosing-model-architectures" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Theory backed methods for choosing model architectures</h3>
<p>How then, did the VGGnet model achieve an accuracy of 93% on the test set? The answer lies in the fact that data is not shared over parameters in the same way it is in regression models. This stems from the fact that each layer’s parameters are using as their input the output from the previous layer, and thus are reusing data in some sense. While it does not appear that this means we only need to count the parameters in the first layer, it does mean that deep learning models need to be thought of differently than traditional regression based models in terms of parameter complexity.</p>
<p>As of yet, no solid theoretical explanation of exactly what is happening with the data to parameter learning in deep neural networks, and thus there are no concrete guidelines to model construction. Difficulties in ascertaining these guidelines seems in part due to the non-convex optimization routines used for the models.</p>
<p>The combination of the lack of theory and the computational time needed to train deep neural networks all but ruling out more traditional grid-search techniques for tuning layer numbers/size imply that there exists the potential for very impact research in this area.</p>
</div>
<div id="runtime-on-mobile-hardware-hardware" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Runtime on mobile hardware hardware</h3>
<p>Another impact of having extremely large models is they take a long time to not only train, but to be used to predict as well. If deep learning models are to be brought to mobile devices such as smart phones or watches the models need to be scaled down in terms of their size and run time complexity substantially. Efforts towards this have been successful with models such as SqueezeNet (<span class="citation">Iandola et al. (<a href="#ref-squeezenet">2016</a>)</span>) drastically reducing the number of parameters while still maintaining a good level of accuracy in ImageNet prediction performance. In addition, certain forms of penalization such as <span class="math inline">\(L_0\)</span> penalization can be applied to ‘thin out’ a network by forcing certain weights to drop to zero and then throwing them out (<span class="citation">Louizos, Welling, and Kingma (<a href="#ref-sparsenets">2017</a>)</span>), all while performing a single run of stochastic gradient descent, eliminating the need to do costly re-training of the network after dropping weights.</p>
<p>Great opportunity lies the development of objective and rigorous methods of eliminating unnecessary parameters in models. Approaches such as regularization are promising as are the evaluation of the models response to techniques such as ‘dropout’ where by neurons are randomly dropped during training in an effort to build robust prediction pathways. An analysis of the models robustness to certain neuron’s being dropped may indicate the potential for sparsity inducing techniques.</p>
</div>
</div>
<div id="inference" class="section level2">
<h2><span class="header-section-number">5.2</span> Inference</h2>
<p>A great source of confusion for many statisticians when reading literature in the deep learning world is that in many cases the same words have different meanings. “Inference” is a good example of this. To a statistician inference means the dissection of the inner workings of the model: what parameters are used to make predictions and how confident are we in those parameters. In deep learning, inference simply refers to the use of a model for prediction. There are a few things that stand in the way of traditional inference in deep learning models.</p>
<div id="peering-into-the-black-box" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Peering into the black box</h3>
<p>Often it is common to hear people refer to deep neural networks as “black box predictors.” This meaning simply that data goes in and the model performs some uninterpreted transformations to that data and returns its prediction. While in giant models such as VGGnet this may be the case, it is actually quite possible to see what is going on within a neural network, just the quantity of information to understand is too high to fully comprehend it in its raw form.</p>
<p>I believe that the desire for traditional inference in the statistical sense is a limiting goal. Having a parameter estimate and its uncertainty works well when models are single-stage linear combinations of the data, but neural networks, and arguably the world, does not work like that. Traditional inference has relied on making (hopefully) non-impactful simplifications on the true nature of the system being modeled in order for it to fit the framework of linear models.</p>
<p>With deep learning we have a model theoretically capable of modeling any function of data and I think we should take advantage of that. If the model objectively performs well, we should perform the simplification on the explanation side, rather than the model side. How exactly this is done is not a solved issue (and may never be), but some early examples include the work of visualizing the constitutional layers in computer vision models (<span class="citation">Olah, Mordvintsev, and Schubert (<a href="#ref-cnn_vis">2017</a>)</span>): investigating the features learned by neural networks can provide insight into the problem greatly.</p>
</div>
<div id="generative-adversarial-networks" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Generative Adversarial Networks</h3>
<p>One approach to simultaneously attempting to train a better model, but also understand the workings of a model is a class of deep learning models called “generative adversarial networks” (or GANs)(<span class="citation">Goodfellow et al. (<a href="#ref-gans">2014</a>)</span>). GANs train two separate networks in tandem: a generator and a discriminator. The job is for the generator to construct fake examples of some training set and the discriminator’s job is to decide if the example is a real observation or a generated one. These models have shown remarkable results in terms of image generation such as those recently presented by NVIDIA <a href="future.html#fig:ganexample">5.1</a> (<span class="citation">Karras et al. (<a href="#ref-progressive_gans">2017</a>)</span>).</p>
<div class="figure"><span id="fig:ganexample"></span>
<img src="figures/gan_example.png" alt="The output of a generative adversarial network trained on a database of celebrity faces. Both faces seen are entirely generated by the model and show that it learned to a very precise degree, what constitutes a 'face.'" height="320" />
<p class="caption">
Figure 5.1: The output of a generative adversarial network trained on a database of celebrity faces. Both faces seen are entirely generated by the model and show that it learned to a very precise degree, what constitutes a ‘face.’
</p>
</div>
<p>One potentially valuable for inference property of sample generated by GANs is they show what the model is ‘seeing’ when it chooses to classify something as a given class. For instance, an over-fit model may classify a house as a house because it sees a lot of the color blue in the sky. If that was the case a GAN would simply return a blue canvas when asked to generate a house.</p>
<p>Recently, a team at ETH Zurich used GANs on time series data taken from hospital records (<span class="citation">Esteban, Hyland, and Rätsch (<a href="#ref-medical_gans">2017</a>)</span>) and found that GANs could be used successfully on these data to generate realistic looking medical data, suggesting that the model was learning underlying patterns well<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
</div>
<div id="causality-problems" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Causality problems</h3>
<p>While much of deep learning is not currently focused on uncovering causal pathways, given the ability of these models to generalize so well, it is worth exploring the issue more. One area of concern with the models mentioned is the temporarily of data. For instance, in constitutional methods for sequence classification, often the convolutions are allowed to explore not only back in time, but also forward in time to classify at a given instance. The same goes with a class of RNNs that we didn’t discuss but have proved successful: the bi-directional RNN. In this case the RNN’s hidden state path travels not only forward in time, but also backwards.</p>
<p>These models that can see both backward and forward in time often perform better than their omni-directional counterparts. For instance, in speech the “au” phoneme may indicate an ‘e’ or an ‘a’ in a word, but it only becomes clear after the end of the word is heard which value it is. However, the flow of causality is forward in time, so these models explicitly violate this.</p>
<p>Potentially fitting a model that has the ability to see both forward and reverse temporal dependencies and then investigating the dependencies that were discovered by both directions could provide some insight into this. For instance, if the backwards in time component of the RNN found that the administration of some drug was a strong signal that high blood-pressure would later occur, but not the reverse direction the relationship could warrant further experimental exploration of causality potential.</p>
<p>It would be necessary to make sure that the patterns discovered were not due to residuals from the reverse-time predictors, but this could be done by forcing the model to ‘forget’ those patterns and seeing if our forward-time trends remain.</p>
</div>
</div>
<div id="small-or-sparse-data" class="section level2">
<h2><span class="header-section-number">5.3</span> Small or sparse data</h2>
<p>Deep learning has come to be almost synonymous with ‘big data.’ Most of the groundbreaking work tends to come out of large companies with massive data-sets and near-infinite compute power to fit their models. This has left the area of deep learning corresponding to small data relatively unexplored. We have already seen that deep learning models seem to use their parameters more efficiently than traditional statistical methods, but that is clearly not without limit. The following are brief survey of a few techniques for dealing with small or sparse (meaning a large portion missing labels) data.</p>
<div id="bayesian-deep-learning" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Bayesian deep learning</h3>
<p>As we have seen, a neural network is essentially a series of stacked linear models with static non-linear transformations applied. Much like we can fit a regression model in a Bayesian context, we can fit a deep neural network with Bayesian techniques. To do so, each tuneable parameter is simply provided a prior (usually a normal centered at zero) and the posterior model is determined the same as any other Bayesian model. Usually variational inference techniques are used instead of sampling techniques such as Hamiltonian Monte Carlo due to the size of the models (<span class="citation">Blundell et al. (<a href="#ref-bayesnets">2015</a>)</span>).</p>
<p>Bayesian neural networks have been shown to perform more efficiently on small data-sets than traditional models (<span class="citation">Srivastava et al. (<a href="#ref-dropout">2014</a>)</span>). In addition, some generative models such as autoencoders (see section below) have shown subjectively better results from Bayesian implementations than standard implementations (<span class="citation">Kingma and Welling (<a href="#ref-variational_autoencoders">2013</a>)</span>).</p>
</div>
<div id="semi-supervised-methods" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Semi-supervised methods</h3>
<p>In many circumstances the data may not be small as a whole, but the number of observations that have labels for what you want to predict is. An example of this is activity tagging data. For each day twenty four hours of time-series data are gathered on the subject, but often when asked to tag the data they only tag specific instances of activities, leaving much of the day blank. In addition it is often infeasible to ask them to label all of their data.</p>
<p>Another example comes from EHR based studies. Many times these studies rely on using physicians to perform chart reviews in order to construct their training and test sets. This is a costly and time consuming procedure.</p>
<div class="figure"><span id="fig:semisupervised"></span>
<img src="figures/semi-supervised.png" alt="Visual example of how adding unlabeled data can provide valuable information about the shape of the data valuable for classification. Image taken from [Wikipedia](https://en.wikipedia.org/wiki/Semi-supervised_learning)" height="320" />
<p class="caption">
Figure 5.2: Visual example of how adding unlabeled data can provide valuable information about the shape of the data valuable for classification. Image taken from <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">Wikipedia</a>
</p>
</div>
<p>There are various approaches to approaching this issue. A very promising one is the use of an initial unsupervised learning task on the data, followed by a supervised learning step using features learned by the unsupervised step.</p>
<p>For instance, say we wished to classify sentiment of a corpus of text, but only had labels of sentiment for a small subsection of the text. First an unsupervised model would be fit to all of the data, for instance, training the model to predict the next word in a sequence. This unsupervised model would learn to map the text at a given instance to some latent-space that most likely holds information about sentiment as well as the next word. What is then done is the final layer of the model that maps that latent-space to the next word is removed and replaced with a new layer that fits the form of our desired classification (in this case a binary outcome of “happy” or not). The model is then trained on the labeled data with the weights of the lower-layers either frozen at their values from the unsupervised step or simply initialized at them.</p>
<p>This approach of unsupervised pre-training has been shown to yield great improvements in the performance of sequence models (<span class="citation">Zhu (<a href="#ref-semi_supervised">2005</a>)</span>).</p>
<p>Other methods of performing semi-supervised learning include training the model on available labels, then using the trained model to classify the unlabeled data and then retraining the model treating those labels as the true values. Surprisingly this method does yield improvements over not using any unlabeled data(<span class="citation">Zhu (<a href="#ref-semi_supervised">2005</a>)</span>).</p>
<p>Exploration of the operating characteristics of semi-supervised learning could be a valuable contribution to areas of research such as electronic health records. A pseudo power calculation could be performed at the outset of a modeling effort. This would help the researchers optimize time and money by indicating how many labeled examples needed to be collected. In addition, efforts to extend the performance benefits of semi-supervised learning could allow models to be fit to domains where they were previously not able to be due to difficulties in gathering labels for data.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-vggnet">
<p>Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” <em>arXiv Preprint arXiv:1409.1556</em>.</p>
</div>
<div id="ref-rms">
<p>Harrell Jr, Frank E. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer.</p>
</div>
<div id="ref-squeezenet">
<p>Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters And&lt; 0.5 Mb Model Size.” <em>arXiv Preprint arXiv:1602.07360</em>.</p>
</div>
<div id="ref-sparsenets">
<p>Louizos, Christos, Max Welling, and Diederik P Kingma. 2017. “Learning Sparse Neural Networks through L<span class="math inline">\(_{0}\)</span> Regularization.” <em>arXiv.org</em>, December. <a href="http://arxiv.org/abs/1712.01312v1" class="uri">http://arxiv.org/abs/1712.01312v1</a>.</p>
</div>
<div id="ref-cnn_vis">
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div id="ref-gans">
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p>
</div>
<div id="ref-progressive_gans">
<p>Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. “Progressive Growing of Gans for Improved Quality, Stability, and Variation.” <em>arXiv Preprint arXiv:1710.10196</em>.</p>
</div>
<div id="ref-medical_gans">
<p>Esteban, Cristóbal, Stephanie L Hyland, and Gunnar Rätsch. 2017. “Real-Valued (Medical) Time Series Generation with Recurrent Conditional Gans.” <em>arXiv Preprint arXiv:1706.02633</em>.</p>
</div>
<div id="ref-bayesnets">
<p>Blundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” <em>arXiv Preprint arXiv:1505.05424</em>.</p>
</div>
<div id="ref-dropout">
<p>Srivastava, Nitish, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Journal of Machine Learning Research</em> 15 (1): 1929–58.</p>
</div>
<div id="ref-variational_autoencoders">
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” <em>arXiv Preprint arXiv:1312.6114</em>.</p>
</div>
<div id="ref-semi_supervised">
<p>Zhu, Xiaojin. 2005. “Semi-Supervised Learning Literature Survey.” Citeseer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>This opens a fascinating ethical conundrum in that, theoretically if over-fit, the model could serve to simple memorize patient data and would be a serious privacy threat. How do we decide when the model is interpreting general trends and when it’s working on the individual level? Are there cases for both?<a href="future.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="architectures.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="placeholder.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-future_directions.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
