<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="architectures.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Continuous Classification using Deep Neural Networks</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#continuous-classification"><i class="fa fa-check"></i><b>1.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>1.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#activity-prediction"><i class="fa fa-check"></i><b>1.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#ehr-monitoring"><i class="fa fa-check"></i><b>1.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#hospital-automation"><i class="fa fa-check"></i><b>1.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#history-of-methods"><i class="fa fa-check"></i><b>1.3</b> History of methods</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#windowed-regression"><i class="fa fa-check"></i><b>1.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#transformation-methods"><i class="fa fa-check"></i><b>1.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#hidden-markov-models"><i class="fa fa-check"></i><b>1.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>1.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>2</b> Neural Networks</a><ul>
<li class="chapter" data-level="2.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>2.1</b> History</a><ul>
<li class="chapter" data-level="2.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>2.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="2.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>2.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="2.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>2.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="2.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>2.4</b> Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>2.5</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>2.5.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="2.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propagation"><i class="fa fa-check"></i><b>2.5.2</b> Back Propagation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>2.6</b> Training</a><ul>
<li class="chapter" data-level="2.6.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>2.6.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.6.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>2.6.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>2.7</b> Activation Functions</a></li>
<li class="chapter" data-level="2.8" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>2.8</b> Loss functions</a><ul>
<li class="chapter" data-level="2.8.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>2.8.1</b> Regression loss functions</a></li>
<li class="chapter" data-level="2.8.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>2.8.2</b> Classification loss functions</a></li>
<li class="chapter" data-level="2.8.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>2.8.3</b> Other loss functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>3</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="3.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>3.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="3.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>3.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="3.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>3.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="3.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>3.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="3.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>3.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="3.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>3.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="3.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>3.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="3.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>3.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="3.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>3.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>3.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="3.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>3.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>4</b> Opportunities for advancing field</a><ul>
<li class="chapter" data-level="4.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>4.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="4.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>4.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="4.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware"><i class="fa fa-check"></i><b>4.1.2</b> Runtime on mobile hardware</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>4.2</b> Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>4.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="4.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>4.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="4.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>4.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>4.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>4.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="4.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>4.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="future" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Opportunities for advancing field</h1>
<p>Up to now has been an overview of the techniques in deep learning that have been successfully and commonly implemented in sequential classification problems. This chapter will be devoted to new efforts of solving problems associated with the aforementioned methods. In addition to being a survey of the current state of the art it will also identify potential avenues for new research that could enhance our ability to work with sequential data subjected to various constraints.</p>
<div id="what-to-do-with-all-those-parameters" class="section level2">
<h2><span class="header-section-number">4.1</span> What to do with all those parameters</h2>
<p>An issue with not just sequence-based deep learning, but the field as a whole, is how large models are. For instance: VGGnet (<span class="citation">Simonyan and Zisserman (<a href="#ref-vggnet">2014</a>)</span>), the second place winner of the 2014 ImageNet competition and an extremely common model to use for image recognition tasks, has 138 million parameters to tune. Going by the rule of thumb from Frank Harrell’s book “Regression Modeling Strategies” (<span class="citation">Harrell Jr (<a href="#ref-rms">2015</a>)</span>) of 10-20 observations per parameter in our model, this is an issue, especially given the size of the data-set that the VGGnet model was trained on was <em>only</em> one million images<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>Even simple neural nets have a large number of parameters. A model with just a single hidden layer taking a ten dimensional input and a hidden layer of ten neurons performing binary classification would have (with bias parameters included) <span class="math inline">\((10*11) + (10*11) + (2*11) = 242\)</span> parameters to tune.</p>
<div id="theory-backed-methods-for-choosing-model-architectures" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Theory backed methods for choosing model architectures</h3>
<p>How then, did the VGGnet model achieve an accuracy of 93% on the test set? The answer lies in the fact that data is not shared over parameters in the same way it is in regression models. This stems from the fact that each layer’s parameters are using as their input the output from the previous layer, and thus data are being reused. While it does not appear that this means we only need to count the parameters in the first layer, it does mean that deep learning models need to be thought of differently than traditional regression based models in terms of parameter complexity<span class="citation">(^It also points to the advantages of deep neural nets over wide ones, a topic considered more deeply in Goodfellow, Bengio, and Courville <a href="#ref-goodfellow_DL">2016</a>, chap. 13.)</span>.</p>
<p>As of yet, there is no solid theoretical explanation of exactly what the data to parameter relationships are in deep neural networks, and thus there are no concrete guidelines to model construction. Difficulties in ascertaining these guidelines seems in part due to the non-convex optimization routines used for the models.</p>
<p>The combination of the lack of theory and the computational time needed to perform traditional grid-search techniques for tuning layer numbers/size suggest the potential for very impactful research in this area.</p>
</div>
<div id="runtime-on-mobile-hardware" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Runtime on mobile hardware</h3>
<p>Another impact of having extremely large models is they take a long time to not only train, but to be used to predict as well. If deep learning models are to be brought to mobile devices such as smart phones or watches the models need to be scaled down in terms of their size and run time complexity substantially. Efforts towards this have been successful with models such as SqueezeNet (<span class="citation">Iandola et al. (<a href="#ref-squeezenet">2016</a>)</span>) drastically reducing the number of parameters compared to traditional convolutional networks, while still maintaining a good level of accuracy in ImageNet prediction performance. In addition, certain forms of penalization such as <span class="math inline">\(L_0\)</span> penalization can be applied to ‘thin out’ a network by forcing certain weights to drop to zero and then throwing them out (<span class="citation">Louizos, Welling, and Kingma (<a href="#ref-sparsenets">2017</a>)</span>), all while performing a single run of stochastic gradient descent, eliminating the need to do costly re-training of the network after dropping weights.</p>
<p>Great opportunity lies the development of objective and rigorous methods of eliminating unnecessary parameters in models. Approaches such as regularization are promising as are the evaluation of model response to techniques such as dropout (<span class="citation">Srivastava et al. (<a href="#ref-dropout">2014</a>)</span>) where by neurons are randomly dropped during forward propigation in training in an effort to build robust prediction pathways. An analysis of a model’s response to certain neuron’s being dropped may indicate the potential for sparsity inducing techniques.</p>
</div>
</div>
<div id="inference" class="section level2">
<h2><span class="header-section-number">4.2</span> Inference</h2>
<p>A great source of confusion for many statisticians when reading literature in the deep learning world is that in many cases the same words have different meanings. “Inference” is a good example of this. To a statistician inference means the dissection of the inner workings of the model: what parameters are used to make predictions and how confident are we in those parameters. In deep learning, inference typically refers to the use of a model for prediction. There are a few things that stand in the way of traditional inference in deep learning models.</p>
<div id="peering-into-the-black-box" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Peering into the black box</h3>
<p>Often it is common to hear people refer to deep neural networks as “black box predictors.” This meaning simply that data goes in and the model performs some uninterpretable transformations to that data and returns its prediction. While in giant models such as VGGnet may make this seem like the case, it is actually quite possible to see what is going on within a neural network, just the quantity of information to understand is too high to fully comprehend it in its raw form.</p>
<p>The desire for traditional inference in the statistical sense is a limiting goal. Having a parameter estimate and its uncertainty works well when models are single-stage linear combinations of the data, but neural networks, and arguably the world, does not usually work in linear combinations. Traditional inference has relied on making (hopefully) non-impactful simplifications of the true nature of the system being modeled in order for it to fit the framework of linear models.</p>
<p>With deep learning we have a system theoretically capable of modeling any function of data and we should take advantage of that. If the model objectively performs well, we should perform the simplification on the explanation side, rather than the model side. How exactly this is done is not a solved issue (and may never be), but some early examples include the work of visualizing the intermediate layers in computer vision models (<span class="citation">Olah, Mordvintsev, and Schubert (<a href="#ref-cnn_vis">2017</a>)</span>): investigating the features learned by neural networks can provide great insight into the way it parsing the signals in the data.</p>
</div>
<div id="generative-adversarial-networks" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Generative Adversarial Networks</h3>
<p>One approach that allows simultaneously attempting to train a better model, but also understanding the workings of the model is a class of deep learning models called “generative adversarial networks” (or GANs)(<span class="citation">Goodfellow et al. (<a href="#ref-gans">2014</a>)</span>). GANs train two separate neural networks in tandem: a generator and a discriminator. The job is for the generator to construct fake examples of some training set and the discriminator’s job is to decide if the example is a real observation or a generated one. These models have shown remarkable results in terms of image generation such as those recently presented by NVIDIA <a href="future.html#fig:ganexample">4.1</a> (<span class="citation">Karras et al. (<a href="#ref-progressive_gans">2017</a>)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ganexample"></span>
<img src="figures/gan_example.png" alt="The output of a generative adversarial network trained on a database of celebrity faces. Both faces seen are entirely generated by the model and show that it learned to a very precise degree, what constitutes a 'face.'" width="80%" />
<p class="caption">
Figure 4.1: The output of a generative adversarial network trained on a database of celebrity faces. Both faces seen are entirely generated by the model and show that it learned to a very precise degree, what constitutes a ‘face.’
</p>
</div>
<p>The output produced by the generator model of GANs effectively show what the descriminator model is ‘seeing’ when it chooses to classify something as a given class. For instance, an over-fit model may classify a house as a house because it sees a lot of the color blue in the sky. If that was the case a GAN would simply return a blue canvas when asked to generate a house<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>Recently, a team at ETH Zurich used GANs on time series data taken from hospital records (<span class="citation">Esteban, Hyland, and Rätsch (<a href="#ref-medical_gans">2017</a>)</span>) and found that GANs could be used successfully on these data to generate realistic looking medical data, suggesting that the model was learning underlying patterns well<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
</div>
<div id="causality-problems" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Causality problems</h3>
<p>While much of deep learning is not currently focused on uncovering causal pathways<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>, given the ability of these models to generalize so well, it is worth exploring the issue more. One area of concern with the models mentioned is the temporal order of data. For instance, in convolutional methods for sequence classification, often the convolutions are allowed to explore not only back in time, but also forward in time to classify at a given instance. The same goes with a class of RNNs that we didn’t discuss but have proved successful: the bi-directional RNN. In this case the RNN’s hidden state path travels not only forward in time, but also backwards.</p>
<p>These models that can see both backward and forward in time often perform better than their omni-directional counterparts. For instance, in speech the “au” phoneme may indicate an ‘e’ or an ‘a’ in a word, but it only becomes clear after the end of the word is heard which value it is. However, the flow of causality is forward in time, so these models explicitly violate this.</p>
<p>Potentially fitting a model that has the ability to see both forward and reverse temporal dependencies and then investigating the dependencies that were discovered by both directions could provide some insight into this. For instance, if the backwards in time component of the RNN found that the administration of some drug was a strong signal that high blood-pressure would later occur, but not the reverse direction the relationship could warrant further experimental exploration of causality potential. It would be necessary to make sure that the patterns discovered were not due to residuals from the reverse-time predictors, but this could be done by forcing the model to ‘forget’ those patterns and seeing if our forward-time trends remain.</p>
</div>
</div>
<div id="small-or-sparse-data" class="section level2">
<h2><span class="header-section-number">4.3</span> Small or sparse data</h2>
<p>Deep learning has come to be almost synonymous with ‘big data.’ Most of the groundbreaking work tends to come out of large companies with massive data-sets and near-infinite compute power to fit their models. This has left the area of deep learning corresponding to small data relatively unexplored. We have already seen that deep learning models seem to use their parameters more efficiently than traditional statistical methods, but that is clearly not without limit. The following are a brief survey of a few techniques for dealing with small or sparse (meaning a large portion missing labels) data.</p>
<div id="bayesian-deep-learning" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Bayesian deep learning</h3>
<p>As we have seen, a neural network is essentially a series of stacked linear models with static non-linear transformations applied. Much like we can fit a regression model in a Bayesian context, we can fit a deep neural network with Bayesian techniques. To do so, each tuneable parameter is simply provided a prior (usually a normal centered at zero) distribution and the posterior distribution is determined the same as any other Bayesian model. Usually variational inference techniques are used instead of sampling techniques such as Hamiltonian Monte Carlo due to the size of the models (<span class="citation">Blundell et al. (<a href="#ref-bayesnets">2015</a>)</span>).</p>
<p>Bayesian neural networks have been shown to perform more efficiently on small data-sets than traditional models (<span class="citation">Srivastava et al. (<a href="#ref-dropout">2014</a>)</span>). In addition, some generative models such as autoencoders (see section below) have shown subjectively better results from Bayesian implementations than standard implementations (<span class="citation">Kingma and Welling (<a href="#ref-variational_autoencoders">2013</a>)</span>).</p>
</div>
<div id="semi-supervised-methods" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Semi-supervised methods</h3>
<p>In many circumstances the data may not be small as a whole, but the number of observations that have labels for the desired prediction are. An example of this is activity tagging data. For each day twenty four hours of time-series data are gathered on the subject, but often when asked to tag the data they only tag specific instances of activities, leaving much of the day blank. In addition it is often infeasible to ask them to label every day of their data.</p>
<p>Another example comes from EHR based studies. Many times these studies rely on using physicians to perform chart reviews in order to construct their training and test sets. This is a costly and time consuming procedure.</p>
<div class="figure" style="text-align: center"><span id="fig:semisupervised"></span>
<img src="figures/semi-supervised.png" alt="Visual example of how adding unlabeled data can provide valuable information about the shape of the data valuable for classification. Image taken from Wikipedia." width="40%" />
<p class="caption">
Figure 4.2: Visual example of how adding unlabeled data can provide valuable information about the shape of the data valuable for classification. Image taken from Wikipedia.
</p>
</div>
<p>There are various approaches to dealing with this sparse data issue. A very promising avenue is the preliminary fitting of an unsupervised model on the data, followed by a supervised learning model using features learned by the unsupervised model</p>
<p>Say we wished to classify sentiment of a corpus of text, but only had labels of sentiment for a small subsection of the text. First an unsupervised model would be fit to all of the data. For instance a model trained to predict the next word in a sentence. This unsupervised model would learn to map the text at a given time-point to some latent-space that holds information about the next word and most likely sentiment as well. Tthe final layer of the word prediction model that maps that latent-space to the next word is removed and replaced with a new layer that fits the form of our desired classification (in this case a binary outcome of “happy” or not). This new model is then trained on the labeled data with the weights of the lower-layers either frozen at their values from the unsupervised step or simply initialized at them.</p>
<p>This approach of unsupervised pre-training has been shown to yield great improvements in the performance of sequence models (<span class="citation">Zhu (<a href="#ref-semi_supervised">2005</a>)</span>).</p>
<p>Other methods of performing semi-supervised learning include training the model on available labels, then using the trained model to classify the unlabeled data and then retraining the model treating those labels as the true values. Surprisingly this method does almost always yield improvements over not using any unlabeled data(<span class="citation">Zhu (<a href="#ref-semi_supervised">2005</a>)</span>).</p>
<p>Exploration of the operating characteristics of semi-supervised learning scenarios could be a valuable contribution to areas of research such as electronic health records. An example of potential impact: a pseudo power calculation could be performed at the outset of a modeling effort. This would help the researchers optimize time and money by informing how many labeled examples needed to be collected. In addition, efforts to extend the performance benefits of semi-supervised learning could allow models to be fit to domains where they were previously not able to be due to difficulties in gathering labels for data.</p>

<div id="refs" class="references">
<div>
<p>Blundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” <em>arXiv Preprint arXiv:1505.05424</em>.</p>
</div>
<div>
<p>Esteban, Cristóbal, Stephanie L Hyland, and Gunnar Rätsch. 2017. “Real-Valued (Medical) Time Series Generation with Recurrent Conditional Gans.” <em>arXiv Preprint arXiv:1706.02633</em>.</p>
</div>
<div>
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div>
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p>
</div>
<div>
<p>Harrell Jr, Frank E. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer.</p>
</div>
<div>
<p>Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters And&lt; 0.5 Mb Model Size.” <em>arXiv Preprint arXiv:1602.07360</em>.</p>
</div>
<div>
<p>Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. “Progressive Growing of Gans for Improved Quality, Stability, and Variation.” <em>arXiv Preprint arXiv:1710.10196</em>.</p>
</div>
<div>
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” <em>arXiv Preprint arXiv:1312.6114</em>.</p>
</div>
<div>
<p>Louizos, Christos, Max Welling, and Diederik P Kingma. 2017. “Learning Sparse Neural Networks through L<span class="math inline">\(_{0}\)</span> Regularization.” <em>arXiv.org</em>, December. <a href="http://arxiv.org/abs/1712.01312v1" class="uri">http://arxiv.org/abs/1712.01312v1</a>.</p>
</div>
<div>
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div>
<p>Rush, Alexander M, Sumit Chopra, and Jason Weston. 2015. “A Neural Attention Model for Abstractive Sentence Summarization.” <em>arXiv Preprint arXiv:1509.00685</em>.</p>
</div>
<div>
<p>Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” <em>arXiv Preprint arXiv:1409.1556</em>.</p>
</div>
<div>
<p>Srivastava, Nitish, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Journal of Machine Learning Research</em> 15 (1): 1929–58.</p>
</div>
<div>
<p>Zhu, Xiaojin. 2005. “Semi-Supervised Learning Literature Survey.” Citeseer.</p>
</div>
</div>
</div>
</div>
</div>









<h3>References</h3>
<div id="refs" class="references">
<div id="ref-vggnet">
<p>Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” <em>arXiv Preprint arXiv:1409.1556</em>.</p>
</div>
<div id="ref-rms">
<p>Harrell Jr, Frank E. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer.</p>
</div>
<div id="ref-goodfellow_DL">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-squeezenet">
<p>Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters And&lt; 0.5 Mb Model Size.” <em>arXiv Preprint arXiv:1602.07360</em>.</p>
</div>
<div id="ref-sparsenets">
<p>Louizos, Christos, Max Welling, and Diederik P Kingma. 2017. “Learning Sparse Neural Networks through L<span class="math inline">\(_{0}\)</span> Regularization.” <em>arXiv.org</em>, December. <a href="http://arxiv.org/abs/1712.01312v1" class="uri">http://arxiv.org/abs/1712.01312v1</a>.</p>
</div>
<div id="ref-dropout">
<p>Srivastava, Nitish, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Journal of Machine Learning Research</em> 15 (1): 1929–58.</p>
</div>
<div id="ref-cnn_vis">
<p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” <em>Distill</em>. doi:<a href="https://doi.org/10.23915/distill.00007">10.23915/distill.00007</a>.</p>
</div>
<div id="ref-gans">
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p>
</div>
<div id="ref-progressive_gans">
<p>Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. “Progressive Growing of Gans for Improved Quality, Stability, and Variation.” <em>arXiv Preprint arXiv:1710.10196</em>.</p>
</div>
<div id="ref-medical_gans">
<p>Esteban, Cristóbal, Stephanie L Hyland, and Gunnar Rätsch. 2017. “Real-Valued (Medical) Time Series Generation with Recurrent Conditional Gans.” <em>arXiv Preprint arXiv:1706.02633</em>.</p>
</div>
<div id="ref-bayesnets">
<p>Blundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” <em>arXiv Preprint arXiv:1505.05424</em>.</p>
</div>
<div id="ref-variational_autoencoders">
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” <em>arXiv Preprint arXiv:1312.6114</em>.</p>
</div>
<div id="ref-semi_supervised">
<p>Zhu, Xiaojin. 2005. “Semi-Supervised Learning Literature Survey.” Citeseer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>This comparison is not exactly fair as images are composed of many pixels as well.<a href="future.html#fnref1">↩</a></p></li>
<li id="fn2"><p>Another similar approach is neural networks with ‘attention’ mechanisms (<span class="citation">Rush, Chopra, and Weston (<a href="#ref-attention">2015</a>)</span>). These mechanisms can be used to explore what exactly in the data is contributing heavily to a given classification.<a href="future.html#fnref2">↩</a></p></li>
<li id="fn3"><p>This opens a fascinating ethical conundrum in that, theoretically if over-fit, the model could serve to simple memorize patient data and would be a serious privacy threat. How do we decide when the model is interpreting general trends and when it’s working on the individual level? Are there cases for both?<a href="future.html#fnref3">↩</a></p></li>
<li id="fn4"><p>It is being explored however, particularly in the Bayesian deep learning communities.<a href="future.html#fnref4">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="architectures.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-future_directions.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
