<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Continuous Classification using Deep Neural Networks</title>
  <meta name="description" content="Continuous Classification using Deep Neural Networks">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Continuous Classification using Deep Neural Networks" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Continuous Classification using Deep Neural Networks" />
  
  
  

<meta name="author" content="Nick Strayer">


<meta name="date" content="2017-12-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  

<link rel="next" href="neuralnetworks.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Continuous Classification using Deep Neural Networks</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#continuous-classification"><i class="fa fa-check"></i><b>1.1</b> Continuous Classification</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#potential-applications-of-continuous-classification-models"><i class="fa fa-check"></i><b>1.2</b> Potential applications of continuous classification models</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#activity-prediction"><i class="fa fa-check"></i><b>1.2.1</b> Activity Prediction</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#ehr-monitoring"><i class="fa fa-check"></i><b>1.2.2</b> EHR monitoring</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#hospital-automation"><i class="fa fa-check"></i><b>1.2.3</b> Hospital Automation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#history-of-methods"><i class="fa fa-check"></i><b>1.3</b> History of methods</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#windowed-regression"><i class="fa fa-check"></i><b>1.3.1</b> Windowed regression</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#transformation-methods"><i class="fa fa-check"></i><b>1.3.2</b> Transformation methods</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#hidden-markov-models"><i class="fa fa-check"></i><b>1.3.3</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#advantages-of-deep-learning-methods"><i class="fa fa-check"></i><b>1.3.4</b> Advantages of deep learning methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neuralnetworks.html"><a href="neuralnetworks.html"><i class="fa fa-check"></i><b>2</b> Neural Networks</a><ul>
<li class="chapter" data-level="2.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#history"><i class="fa fa-check"></i><b>2.1</b> History</a><ul>
<li class="chapter" data-level="2.1.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#biological-inspirations"><i class="fa fa-check"></i><b>2.1.1</b> Biological Inspirations</a></li>
<li class="chapter" data-level="2.1.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.1.2</b> Geometric Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>2.2</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="2.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#the-computation-graph"><i class="fa fa-check"></i><b>2.3</b> The Computation Graph</a></li>
<li class="chapter" data-level="2.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#terminology"><i class="fa fa-check"></i><b>2.4</b> Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="neuralnetworks.html"><a href="neuralnetworks.html#mathematical-operations"><i class="fa fa-check"></i><b>2.5</b> Mathematical Operations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#forward-propagation"><i class="fa fa-check"></i><b>2.5.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="2.5.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#back-propagation"><i class="fa fa-check"></i><b>2.5.2</b> Back Propagation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="neuralnetworks.html"><a href="neuralnetworks.html#training"><i class="fa fa-check"></i><b>2.6</b> Training</a><ul>
<li class="chapter" data-level="2.6.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent"><i class="fa fa-check"></i><b>2.6.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.6.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#gradient-descent-modifications"><i class="fa fa-check"></i><b>2.6.2</b> Gradient Descent Modifications</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="neuralnetworks.html"><a href="neuralnetworks.html#activation-functions"><i class="fa fa-check"></i><b>2.7</b> Activation Functions</a></li>
<li class="chapter" data-level="2.8" data-path="neuralnetworks.html"><a href="neuralnetworks.html#loss-functions"><i class="fa fa-check"></i><b>2.8</b> Loss functions</a><ul>
<li class="chapter" data-level="2.8.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#likelihoods"><i class="fa fa-check"></i><b>2.8.1</b> Likelihoods</a></li>
<li class="chapter" data-level="2.8.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#regression-loss-functions"><i class="fa fa-check"></i><b>2.8.2</b> Regression loss functions</a></li>
<li class="chapter" data-level="2.8.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#classification-loss-functions"><i class="fa fa-check"></i><b>2.8.3</b> Classification loss functions</a></li>
<li class="chapter" data-level="2.8.4" data-path="neuralnetworks.html"><a href="neuralnetworks.html#other-loss-functions"><i class="fa fa-check"></i><b>2.8.4</b> Other loss functions</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="neuralnetworks.html"><a href="neuralnetworks.html#avoiding-overfitting"><i class="fa fa-check"></i><b>2.9</b> Avoiding Overfitting</a><ul>
<li class="chapter" data-level="2.9.1" data-path="neuralnetworks.html"><a href="neuralnetworks.html#parameter-penalization"><i class="fa fa-check"></i><b>2.9.1</b> Parameter penalization</a></li>
<li class="chapter" data-level="2.9.2" data-path="neuralnetworks.html"><a href="neuralnetworks.html#dropout"><i class="fa fa-check"></i><b>2.9.2</b> Dropout</a></li>
<li class="chapter" data-level="2.9.3" data-path="neuralnetworks.html"><a href="neuralnetworks.html#early-stopping"><i class="fa fa-check"></i><b>2.9.3</b> Early Stopping</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="architectures.html"><a href="architectures.html"><i class="fa fa-check"></i><b>3</b> Architectures For Sequence Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="architectures.html"><a href="architectures.html#terminology-1"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="architectures.html"><a href="architectures.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a><ul>
<li class="chapter" data-level="3.2.1" data-path="architectures.html"><a href="architectures.html#applications-to-sequential-data"><i class="fa fa-check"></i><b>3.2.1</b> Applications to sequential data</a></li>
<li class="chapter" data-level="3.2.2" data-path="architectures.html"><a href="architectures.html#successes-in-natural-language-processing"><i class="fa fa-check"></i><b>3.2.2</b> Successes in natural language processing</a></li>
<li class="chapter" data-level="3.2.3" data-path="architectures.html"><a href="architectures.html#cyclical-computation-graph"><i class="fa fa-check"></i><b>3.2.3</b> Cyclical Computation Graph</a></li>
<li class="chapter" data-level="3.2.4" data-path="architectures.html"><a href="architectures.html#weight-sharing"><i class="fa fa-check"></i><b>3.2.4</b> Weight sharing</a></li>
<li class="chapter" data-level="3.2.5" data-path="architectures.html"><a href="architectures.html#problems-with-exploding-and-vanishing-gradients"><i class="fa fa-check"></i><b>3.2.5</b> Problems with exploding and vanishing gradients</a></li>
<li class="chapter" data-level="3.2.6" data-path="architectures.html"><a href="architectures.html#modern-extensions"><i class="fa fa-check"></i><b>3.2.6</b> Modern Extensions</a></li>
<li class="chapter" data-level="3.2.7" data-path="architectures.html"><a href="architectures.html#computational-hurdles"><i class="fa fa-check"></i><b>3.2.7</b> Computational Hurdles</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="architectures.html"><a href="architectures.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="3.3.1" data-path="architectures.html"><a href="architectures.html#application-to-spatially-correlated-data"><i class="fa fa-check"></i><b>3.3.1</b> Application to spatially correlated data</a></li>
<li class="chapter" data-level="3.3.2" data-path="architectures.html"><a href="architectures.html#feature-learning"><i class="fa fa-check"></i><b>3.3.2</b> Feature Learning</a></li>
<li class="chapter" data-level="3.3.3" data-path="architectures.html"><a href="architectures.html#weight-sharing-1"><i class="fa fa-check"></i><b>3.3.3</b> Weight sharing</a></li>
<li class="chapter" data-level="3.3.4" data-path="architectures.html"><a href="architectures.html#translation-invariance"><i class="fa fa-check"></i><b>3.3.4</b> Translation invariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>4</b> Opportunities for advancing field</a><ul>
<li class="chapter" data-level="4.1" data-path="future.html"><a href="future.html#what-to-do-with-all-those-parameters"><i class="fa fa-check"></i><b>4.1</b> What to do with all those parameters</a><ul>
<li class="chapter" data-level="4.1.1" data-path="future.html"><a href="future.html#theory-backed-methods-for-choosing-model-architectures"><i class="fa fa-check"></i><b>4.1.1</b> Theory backed methods for choosing model architectures</a></li>
<li class="chapter" data-level="4.1.2" data-path="future.html"><a href="future.html#runtime-on-mobile-hardware"><i class="fa fa-check"></i><b>4.1.2</b> Runtime on mobile hardware</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="future.html"><a href="future.html#inference"><i class="fa fa-check"></i><b>4.2</b> Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="future.html"><a href="future.html#peering-into-the-black-box"><i class="fa fa-check"></i><b>4.2.1</b> Peering into the black box</a></li>
<li class="chapter" data-level="4.2.2" data-path="future.html"><a href="future.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>4.2.2</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="4.2.3" data-path="future.html"><a href="future.html#causality-problems"><i class="fa fa-check"></i><b>4.2.3</b> Causality problems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="future.html"><a href="future.html#small-or-sparse-data"><i class="fa fa-check"></i><b>4.3</b> Small or sparse data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="future.html"><a href="future.html#bayesian-deep-learning"><i class="fa fa-check"></i><b>4.3.1</b> Bayesian deep learning</a></li>
<li class="chapter" data-level="4.3.2" data-path="future.html"><a href="future.html#semi-supervised-methods"><i class="fa fa-check"></i><b>4.3.2</b> Semi-supervised methods</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Continuous Classification using Deep Neural Networks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Continuous Classification using Deep Neural Networks</h1>
<h4 class="author"><em>Nick Strayer</em></h4>
<h4 class="date"><em>2017-12-17</em></h4>
</div>
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<div id="continuous-classification" class="section level2">
<h2><span class="header-section-number">1.1</span> Continuous Classification</h2>
<p>Imagine you are watching a movie. A friend walks in late and asks “what did I miss?” You tell them the main character has just escaped from a nasty predicament and has defeated the antagonist. What you have done is classification on a sequence. The sequence in this case is the frames of the movie and your classification was what was occurring in the movie at that moment. You <em>could</em> have given the same answer if you just saw a single frame, but most likely your assessment of the state of the movie depended on events you saw before and the context in which they placed the most recent frame.</p>
<p>Continuous classification in the context of statistics and machine learning is training models to observe data over time, like you watched the movie, and classify the status of the generating system at any given point. Sometimes seeing the most recent data is all that is needed, but more interesting and challenging problems need the algorithm to be able to make decisions about a current time while leveraging context from previous history to do so.</p>
<p>This report is a brief run through past attempts at continuous classification and a deeper exploration of the current state of the art methods.</p>
</div>
<div id="potential-applications-of-continuous-classification-models" class="section level2">
<h2><span class="header-section-number">1.2</span> Potential applications of continuous classification models</h2>
<p>The following are just a few examples of biomedical applications made possible with effective continuous classification models.</p>
<div id="activity-prediction" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Activity Prediction</h3>
<p>With the advent of wearable devices such as fitbits and apple watches, the amount of high temporal resolution data we have streaming from individuals is exploding and showing no sign of letting up.</p>
<p>Continuous classification models could use these data to classify the state of the wearer at any moment. A simple example of this is detecting different exercise types (e.g. running vs. swimming); which is implemented (by unpublished methods) internally at companies such as fitbit.</p>
<p>More advanced, and potentially impactful, applications include extending the predictions to more subtle but medically relevant states such as dehydration or sleep apnea (<span class="citation">Jose M. Sanchez et al. (<a href="#ref-wearable_atrial">2017</a>)</span>). Preliminary work in these areas using deep learning has shown surprising success with data as limited as heart-rate and motion indication being enough to predict sleep apnea and various cardiovascular risk states with a c-statistic of 0.94: comparable to invasive gold standards.</p>
</div>
<div id="ehr-monitoring" class="section level3">
<h3><span class="header-section-number">1.2.2</span> EHR monitoring</h3>
<p>With more and more information on patients being accrued in government and hospital databases we have a clearer than ever picture of a patient’s health over long periods of time. Unfortunately, due to a combination of overwhelming quantities and noise levels in the data, our ability to make use of these data has not kept up with their quantity.</p>
<p>Sequential models can help ease the burden on health practitioners in making use of these data. For instance, a model could be trained on a patient’s records to predict the likelihood of cardiovascular events. This model could then alert a doctor of potential risk in order to facilitate timely interventions. This could be especially helpful in large clinical settings where personal doctor-patient relationships may not be common. For a review of the performance of deep learning models in electronic health record contexts, see <span class="citation">Shickel et al. (<a href="#ref-deep_ehr">2017</a>)</span>.</p>
</div>
<div id="hospital-automation" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Hospital Automation</h3>
<p>Patient monitoring systems in hospitals have a complex web of sensors constantly reading data from patients. These systems are programmed to alert staff if different thresholds are crossed indicating the occurrence (, or likely occurrence,) of an event such as a heart attack. Continuous classification methods could assist these systems by automatically learning complex patterns related to events of interest and producing a continuous probability of an event in contrast to the traditional thresholding practice.</p>
<p>By not requiring explicit checks programmed into sensors (temperature above <span class="math inline">\(x\)</span> degrees), and easily combining the signals from multiple sensors, continuous classification could both provide more personalized and sensitive detection for outcomes of interest and help hospitals more efficiently allocate resources and potentially save lives.</p>
</div>
</div>
<div id="history-of-methods" class="section level2">
<h2><span class="header-section-number">1.3</span> History of methods</h2>
<p>While sources of data well suited to it have recently greatly expanded, interest in performing continuous classification is not a new topic. Many methods have been proposed for the task to varying degrees of success. Below is a brief review of some of the more successful methods and their advantages and limitations.</p>
<div id="windowed-regression" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Windowed regression</h3>
<p>Perhaps the most intuitive approach to the problem of incorporating context from previous time points into your prediction is to use a windowed approach. Broadly, in these approaches a window of some width (in previous observation numbers or time length) is sequentially run over the series. The data obtained from the window may have some form of summary applied to it. This could be a mean, median, or any other function which is then used to predict with.</p>
<p>By summarizing the multiple data-points into a single (or few) values noise can be removed, but at the cost of potentially throwing away useful information captured by the interval (such as trajectory.)</p>
<div class="figure" style="text-align: center"><span id="fig:dynamictimewarp"></span>
<img src="https://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/t/5751d9dd40261d920b308196/1464982022839/" alt="Comparison of dynamic time warping and traditional euclidean-based similarity between two time series sequences. Image courtousy of [SFL Scientific](https://sflscientific.com/data-science-blog/2016/6/3/dynamic-time-warping-time-series-analysis-ii)." width="30%" />
<p class="caption">
Figure 1.1: Comparison of dynamic time warping and traditional euclidean-based similarity between two time series sequences. Image courtousy of <a href="https://sflscientific.com/data-science-blog/2016/6/3/dynamic-time-warping-time-series-analysis-ii">SFL Scientific</a>.
</p>
</div>
<p>If the time dimension of the data are kept intact more advanced methods are available. One common example is dynamic time warping (<span class="citation">Berndt and Clifford (<a href="#ref-dtw">1994</a>)</span>, figure <a href="index.html#fig:dynamictimewarp">1.1</a>). In dynamic time warping the shape of a time series signals are matched by either squeezing or stretching the time dimension. This allows for the computation of similarity between signal patterns, rather than scale. Another common approach are kernel methods (see next section). Both of these methods allow more information to be retained in the sample but at the cost of setting a limit on how far back your model can learn dependencies in the data. For instance, if your window is one hour long but an activity lasts two hours your model will have a very hard time recognizing it.</p>
<p>One way to think of the window size in these approaches is as an infinitely strong prior on the interaction timeline (<span class="citation">Graves (<a href="#ref-graves_rnn">2012</a>)</span>). No matter how strong the data support informative signals from time points <span class="math inline">\(k\)</span> steps back, if our window is less than <span class="math inline">\(k\)</span> wide, our model will never reflect this.</p>
</div>
<div id="transformation-methods" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Transformation methods</h3>
<p>As mentioned before, when a window is scanned across the time dimension of data, one of the ways of extracting information is by performing some transformation on the data. Common examples include wavelet or Fourier transforms. These methods decompose complex correlated signals into many simple components. For instance, Fourier transforms applied to accelerometer data from an individuals wrist can be used to detect the frequencies associated with walking and running (<span class="citation">Ravi et al. (<a href="#ref-accelerometer_activity">2005</a>)</span>). These methods have also been used extensively in electrical systems and signal processing to help determine the state of the system.</p>
<div class="figure" style="text-align: center"><span id="fig:fouriertransform"></span>
<img src="figures/fourier_transform.png" alt="Example of transforming data from the data-domain to the frequency-domain for time series data. Image courtousy of [Allen D. Elster, MD FACR](http://mriquestions.com/index.html)." width="60%" />
<p class="caption">
Figure 1.2: Example of transforming data from the data-domain to the frequency-domain for time series data. Image courtousy of <a href="http://mriquestions.com/index.html">Allen D. Elster, MD FACR</a>.
</p>
</div>
<p>A few limitations are imposed by these methods. First, as previously mentioned, they are subject to the windowing constraints. Secondly, they rely on the data to be periodic or oscillatory in nature. For instance, accelerometer data oscillates back and forth as the individual swings their arms and electrical systems are inherently oscillatory. Data such as heart-rate or step counts produced by devices like apple watches and fitbits are a rather stable signal<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and thus transformation methods are unable to separate them into frequency domains at small time scales. In addition, these methods are unable to deal with non-numeric data which severally limits them in heterogeneous data domains such as EHR data.</p>
</div>
<div id="hidden-markov-models" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Hidden Markov Models</h3>
<p>In an attempt to deal with the fact that in most scenarios the classification of time point <span class="math inline">\(t\)</span> is dependent on that of previous time points, hidden Markov models (or HMMs) model data as a series of observations generated by a system transitioning between some unobserved (or latent) states. This is done by constructing a transition matrix that denotes the probability of transitioning from one state to another and conditioning it on whatever observed data you have. This can be thought of in mathematical form as the the probability of transitioning from one state <span class="math inline">\(s_a\)</span> to another <span class="math inline">\(s_b\)</span> given some input data at time t (<span class="math inline">\(x_t\)</span>).</p>
<p><span class="math display">\[P(s_a -&gt; s_b | x_t) = ...\]</span></p>
<p>This allows the model to learn time dependencies in the data. For instance, if a person is running now their next state is probably going to be walking rather than sitting or swimming.</p>
<p>HMMs were the state of the art models on continuous classification problems until very recently and are still very valuable for many problems. However, their greatest advantage is also their greatest disadvantage.</p>
<p>The Markov property (or the first ‘M’ in HMM) states that the next state of the system being modeled depends exclusively on the current state. This means that the model is ‘memory-less.’ For instance, returning to our running example, say an individual had been running in the previous time point, the model will most likely pick walking as their next state (ignoring any conditional data for simplicity) but what if before they were running they were swimming? This fact from multiple time-steps before would strongly hint that the next state would in fact be biking and not walking (they are running a triathlon.)</p>
<p>There are ways to fix this such as extending the model’s transition probabilities to multiple time-steps, however the number of parameters needed to estimate transition probabilities for <span class="math inline">\(m\)</span> previous time steps is (# of classes) to the <span class="math inline">\(k^{th}\)</span> power: <span class="math inline">\(m^k\)</span>, which rapidly becomes intractable. In addition, we have to a priori decide the number of time steps in the past that matter.</p>
</div>
<div id="advantages-of-deep-learning-methods" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Advantages of deep learning methods</h3>
<p>Before we dive into the mathematical underpinnings of deep learning methods we will go over how they solve many of the aforementioned issues from traditional methods.</p>
<div id="less-domain-knowledge-needed" class="section level4">
<h4><span class="header-section-number">1.3.4.1</span> Less Domain Knowledge Needed</h4>
<p>One of the ways that it helps to think about deep learning is as a computer program that programs itself given an objective and examples. In his popular blog post <a href="https://medium.com/@karpathy/software-2-0-a64152b37c35"><em>Software 2.0</em></a> Andrej Karapathy makes the argument that deep learning is powerful because it helps avoid the tedious process pf explicitly defining the context surrounding cases the computer is supposed to detect. One of the ways this is applicable to our problems is the ability for deep learning models to adapt to a wide range of problem/ data domains without much human-defined customization.</p>
<p>One demonstration of this advantage is less manual manipulation of data is needed before it can be used in models. If you had data from an accelerometer it could be fit with the same neural network as data from a more static heart-rate sensor would. The models are flexible enough to learn how to deal with different input patterns and characteristics without requiring the researcher to explicitly define a transformation. This independence from large amounts of human intervention has the potential to make performance assessments more accurate by automating most parts of the model fitting process (<span class="citation">Harrell Jr (<a href="#ref-rms">2015</a>)</span>).</p>
</div>
<div id="can-find-and-deal-with-arbitrary-time-dependencies" class="section level4">
<h4><span class="header-section-number">1.3.4.2</span> Can find and deal with arbitrary time dependencies</h4>
<p>Deep learning models are theoretically capable of learning time dependencies of infinite length and strength (<span class="citation">Hornik, Stinchcombe, and White (<a href="#ref-universal_approximators">1989</a>)</span>). While obviously it is impossible to supply a network with enough data to fit the number of parameters necessary to do so, the fact remains that deep learning methods are capable of handling long-term time dependencies. In addition to being able to model these dependencies they do so without any need for explicitly telling the model the length of the dependencies and also using substantially fewer parameters than an extended hidden Markov model (<span class="citation">Graves (<a href="#ref-graves_rnn">2012</a>)</span>).</p>
<p>For example, a recurrent neural network (RNN) can automatically learn that if a person swims and then runs, they will most likely be biking next, but it could also remember that a patient was given a flu vaccine three months prior and thus their symptoms most likely don’t indicate the flu but a cold. This flexibility to automatically learn arbitrary time dependency patterns is powerful in not only its ability to create accurate models, but potentially for exploration of causal patterns.</p>
</div>
<div id="multiple-architectures-for-solving-traditional-problems" class="section level4">
<h4><span class="header-section-number">1.3.4.3</span> Multiple architectures for solving traditional problems</h4>
<p>In a similar vein, one of the decisions that does need to be made with deep learning: which network architecture to use, conveniently is rather robust to the problem of continuous classification. For instance: convolutional neural networks that have achieved great success in computer vision were actually originally designed for time series data, and recent advances such as dilated convolutions (<span class="citation">Yu and Koltun (<a href="#ref-dilated_convolutions">2015</a>)</span>)(convolutions with gaps in their input dimension to allow for increasing context size as layers progress deeper) allow for them to search as far back in the time-series as needed to find valuable information for classification. Recurrent neural networks (which will be elaborated on in the following sections) are also fantastic for time-series data, as they explicitly model the autocorrelation found in the data via a recurrent cycle in their computation graph. This allows them to read data much like one reads a book, selectively remembering past events that have applicability to the current state.</p>
</div>
<div id="downsides" class="section level4">
<h4><span class="header-section-number">1.3.4.4</span> Downsides</h4>
<p>As a result of being so flexible deep learning models require a lot of data to properly tune all their parameters without over fitting. This results in not only more data being needed (with some exceptions such as Bayesian methods.)</p>
<p>In addition, neural network optimization can not be calculated with a closed form solution, thus requiring the use of iterative methods to traverse the objective (or cost/loss) function. The training of neural networks is a non-convex problem. This means that there is no guarantee that a found minima is a global minima and that for any global minima, there is no guarantee that it is unique.</p>
<div class="figure" style="text-align: center"><span id="fig:convexoptim"></span>
<img src="figures/convex_optimization.png" alt="Comparison of the objective surface of a convex and non-convex optimization problem." width="60%" />
<p class="caption">
Figure 1.3: Comparison of the objective surface of a convex and non-convex optimization problem.
</p>
</div>
<p>The fact that neural networks need a lot of data and require costly optimization procedures means they are very computationally intensive to train.</p>
<p>Another downside, although one shared by many other approaches described here, is that neural networks are not amenable to inference on specific factors contributing to their classifications. The nature of neural networks as a “black-box” method can decrease trust in the models and also can hide cases where the model has learned to classify on potential un-intentional characteristics of the training set (such as the instrument that took measurements instead of the measurements themselves).</p>
<p>These downsides and potential solutions are explored in the last chapter.</p>
<p>In the next chapter we will go over the basics of modern deep neural networks.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wearable_atrial">
<p>Jose M. Sanchez, MD, BS Brandon Ballinger, MD FHRS Jeffrey E. Olgin, MD MPH Mark J. Pletcher, PhD Eric Vittinghoff, BA Emily Lee, BA Shannon Fan, et al. 2017. “Detecting Atrial Fibrillation Using a Smart Watch - the mRhythm Study.” In <em>Heart Rhythm Scientific Sessions</em>. Heart Rhythm Society.</p>
</div>
<div id="ref-deep_ehr">
<p>Shickel, Benjamin, Patrick Tighe, Azra Bihorac, and Parisa Rashidi. 2017. “Deep Ehr: A Survey of Recent Advances on Deep Learning Techniques for Electronic Health Record (Ehr) Analysis.” <em>arXiv Preprint arXiv:1706.03446</em>.</p>
</div>
<div id="ref-dtw">
<p>Berndt, Donald J, and James Clifford. 1994. “Using Dynamic Time Warping to Find Patterns in Time Series.” In <em>KDD Workshop</em>, 10:359–70. 16. Seattle, WA.</p>
</div>
<div id="ref-graves_rnn">
<p>Graves, Alex. 2012. <em>Supervised Sequence Labelling with Recurrent Neural Networks</em>. Vol. 385. Springer.</p>
</div>
<div id="ref-accelerometer_activity">
<p>Ravi, Nishkam, Nikhil Dandekar, Preetham Mysore, and Michael L Littman. 2005. “Activity Recognition from Accelerometer Data.” In <em>Aaai</em>, 5:1541–6. 2005.</p>
</div>
<div id="ref-rms">
<p>Harrell Jr, Frank E. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer.</p>
</div>
<div id="ref-universal_approximators">
<p>Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” <em>Neural Networks</em> 2 (5). Elsevier: 359–66.</p>
</div>
<div id="ref-dilated_convolutions">
<p>Yu, Fisher, and Vladlen Koltun. 2015. “Multi-Scale Context Aggregation by Dilated Convolutions.” <em>arXiv Preprint arXiv:1511.07122</em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Although the raw data the sensors receive may not be.<a href="index.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="neuralnetworks.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/index.Rmd",
"text": "Edit"
},
"download": ["strayer_qualifying_exam.pdf", "strayer_qualifying_exam.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
