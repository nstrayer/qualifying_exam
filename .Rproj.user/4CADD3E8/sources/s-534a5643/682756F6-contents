# Further Architectures {#architectures}

In the previous chapter we described the general neural network architecture. This is usually called a dense-neural feed-forward networks. 'Dense' refers to the fact that all neurons of a given layer are connected to all nuerons of the successive layer. 'Feed-forward' refers to the fact that data flows into the network and straight to the output, traveling only forward through the layers. In this section we will expand upon this general model with different architectures: the recurrent neural network (RNN) (@rnn_intro) and the convolutional nueral net (CNN) (@cnn_intro). 

It is important to note that while these models are in fact sub-models of the dense feed-forward networks from the last chapter, just with restrictions placed on weights in the form of deleting connections (setting weight to 0) or sharing weights between multiple connections. 

These restrictions applied to standard neural networks allow the models to more efficiently model tasks related to sequential data by reducing the number of parameters that need to be fit or, in some cases, helping with the propigation of the gradients for efficient training. 


## Terminology 

Throughout this chapter we will refer to an input $\textbf{x}$ which is a single observation of $t$ time points. In addition, we have an outcome $\textbf{y}$, also of length $t$ that represents some state or property of the system generating $x$ at each time point. This could be the type of road a car is driving on, the sentiment of a speaker, or the next $x_i$ value (e.g. next word in a sentence). 


## Recurrent Neural Networks

One way to efficiently deal with the fact that sequential data is often highly correlated between observations is to fit a model to each time-point observation and then pass it information on what was happening prior. The model can then combine the previous information with the newly observed input to come up with a prediction. 

This can be accomplished in a nueral network by adding recurrent links between layers. Typically, this is done by passing the hidden layer (or layers) of the network the values of itself at the previous time point. I.e. $\textbf{h}_{t} = g(\textbf{x}_t, \textbf{h}_{t - 1})$. The idea behind this is that the hidden layer learns to encode some 'latent state' of the system that is informative for its output, and so letting the model know what that latent state was previously will help it update the latent state and provide an accurate output. 

Why not just pass the output at time $(t-1)$ to the hidden state at $t$ instead? While this is possible, and indeed works much better than not communicating information between time points at all, it suffers from the squashing of the latent state information to out outcome of interest. This results in a loss of information about what is happening in the system since the hidden or latent state to the outcome is not neccesarily a one-to-one function. In addition, there is conveneince in the fact that the hidden state is already of the same dimension, allowing for a simple element-wise addition of the components from the previous hidden state and the new input information. 

```{r,  out.height = 320, echo = FALSE, label = "cyclegraph", fig.cap = "A recurrent neural network with a single hidden layer. The hidden layer's values at time $t$ are passed to the hidden layer at time $(t + 1)$."}
knitr::include_graphics("figures/rnn_compact.png")
```


### Applications to sequential data 

RNN's are fundementally models for performing analysis on sequential data (although they have been applied to static inputs and used to generate sequential outputs (@rnn_captions)). Some of the major success stories in recurrent neural networks come in the realm of machine translation. For instance, Google's entire translation service is now powered by RNNs (@google_translate).

Other domains in which RNNs have been successfully applied is in time-series regression (@rnn_regression), speech recoginition (@rnn_speach), and handwriting recognition (@rnn_handwriting). 

```{r,  out.height = 320, echo = FALSE, fig.cap = "Example of an RNN's output for recognizing characters in handwritten characters. The model scans along one slice at a time of the data and the output is character likelihood. Still from [youtube video](https://www.youtube.com/watch?v=mLxsbWAYIpw) by Nikhil Buduma."}
knitr::include_graphics("figures/handwriting_rnn.png")
```

### Successes in natural language processing 

One of the areas that has seen great results from the application of RNNs is natural language processesing. Natural language processesing (or NLP) refers broadly to the modeling of textual data in order to infer things like sentiment, predict next words, or even generate entirely new sentences. 

Usage of neural networks for these tasks has greatly improved upon previous techniques that constrained were constrained by linear assumptions (e.g. word2vec (@word2vec)) or limited ability to look backwards in time. 

<!-- #### Abstracts Example -->

### Cyclical Computation Graph

A natural question that arrises from the cyclical computational graph shown in figure \@ref(fig:cyclegraph) is how the gradient can be calculated via back propigation. In fact, the cycle as represented is just a visual simplification of the true computational graph. The 'unrolled' graph can be throught of as a long chain of neural networks that share connections between sequential hidden layers. 

```{r,  out.height = 320, echo = FALSE, label = "unrolledgraph", fig.cap = "Unrolled view of the RNN shown in previous figure. Each timestep has two outputs, the timesteps predictions and its hidden state. The next time step subsequently has two inputs: the data at the timepoint and the previous timepoint's hidden state."}
knitr::include_graphics("figures/rnn_unrolled.png")
```


### Weight sharing



### Problems with exploding and vanishing gradients

### Modern Extensions

#### Long short term memory networks

#### Gated recurrent units

### Computational Hurdles
 

## Convolutional Neural Networks

### Application to spatially correlated data 

### Successes in computer vision

### Convolution as a kernel method

### Weight sharing

### Transformation invariance
